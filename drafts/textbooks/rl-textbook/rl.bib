@book{AIBook2006,
	address = {М.},
	author = {Рассел, Стюарт and Норвиг, Питер},
	edition = {2-е},
	file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Рассел, Норвиг/Рассел, Норвиг - 2006 - Искусственный интеллект современный подход.djvu:djvu},
	language = {russian},
	mendeley-groups = {Artifial Intelligence/Others AI},
	pages = {1408},
	publisher = {Издательский дом "Вильямс"},
	title = {{Искусственный интеллект: современный подход}},
	year = {2006}
}


@book{Osipov2011,
	address = {М.},
	author = {Осипов, Г. С.},
	keywords = {osipov},
	language = {russian},
	mendeley-groups = {Artifial Intelligence},
	mendeley-tags = {osipov},
	pages = {297},
	publisher = {ФИЗМАТЛИТ},
	title = {{Методы искусственного интеллекта}},
	year = {2011}
}

@book{Laird2012,
	author = {Laird, John E.},
	mendeley-groups = {Cogntive approach/Architectures},
	pages = {374},
	publisher = {MIT Press},
	title = {{The Soar Cognitive Architecture}},
	year = {2012}
}

@book{Osipov2018a,
	abstract = {В монографии рассмотрено возникновение и формирование картин мира субъекта деятельности. Рассматривается понятие знака в качестве основного элемента картины мира. Приведены психологические и нейрофизиологические основания знаковой структуры картин мира. Строится модель знака, рассматриваются структура знака, семейства отношений и операций на множестве знаков. Показывается, что различные семейства отношений на множестве знаков позволяют моделировать различные типы картин мира. Продемонстрированы возможности знакового подхода в моделировании некоторых когнитивных функций, таких как целеполагание и динамическое распределение ролей в коалициях субъектов деятельности. Книга предназначена специалистам в области искусственного интеллекта, психологии, лингвистики и всем интересующимся проблемами моделирования человеческого сознания. Может быть использована аспирантами и студентами старших курсов университетов.},
	address = {М.},
	author = {Осипов, Г. С. and Панов, А. И. and Чудова, Н. В. and Кузнецова, Ю. М.},
	file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Осипов et al/Осипов et al. - 2018 - Знаковая картина мира субъекта поведения.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Осипов et al/Осипов et al. - 2018 - Знаковая картина мира субъекта поведения.pdf:pdf},
	isbn = {978-5-9221-1781-4},
	keywords = {15-07-06214,17-17-00138,frccsc,mybook,osipov},
	language = {russian},
	mendeley-groups = {my{\_}publications},
	mendeley-tags = {15-07-06214,17-17-00138,frccsc,mybook,osipov},
	pages = {264},
	publisher = {Физматлит},
	title = {{Знаковая картина мира субъекта поведения}},
	url = {http://www.rfbr.ru/rffi/ru/books/o{\_}2052004 http://www.fmllib.ru/nauchnaya-literatura/znakovaya-kartina-mira-subekta-povedeniya/},
	year = {2018}
}

@inproceedings{Mnih2013,
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	archivePrefix = {arXiv},
	arxivId = {1312.5602},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	booktitle = {arXiv: 1312.5602},
	eprint = {1312.5602},
	file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/arXiv 1312.5602/Mnih et al/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:pdf},
	mendeley-groups = {Reinforcement,Neural networks/Deep learning},
	pages = {1--9},
	title = {{Playing Atari with Deep Reinforcement Learning}},
	url = {http://arxiv.org/abs/1312.5602},
	year = {2013}
}

@article{Oh2016,
	abstract = {In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.},
	archivePrefix = {arXiv},
	arxivId = {1605.09128},
	author = {Oh, Junhyuk and Chockalingam, Valliappa and Singh, Satinder and Lee, Honglak},
	doi = {10.1109/CVPR.2014.180},
	eprint = {1605.09128},
	file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Oh et al/Oh et al. - 2016 - Control of Memory, Active Perception, and Action in Minecraft.pdf:pdf},
	isbn = {9781510829008},
	issn = {10636919},
	mendeley-groups = {Reinforcement/Applications},
	title = {{Control of Memory, Active Perception, and Action in Minecraft}},
	url = {http://arxiv.org/abs/1605.09128},
	year = {2016}
}

@article{Silver2016,
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	doi = {10.1038/nature16961},
	file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Nature/Silver et al/Silver et al. - 2016 - Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
	isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
	issn = {0028-0836},
	journal = {Nature},
	mendeley-groups = {Neural networks/Deep learning},
	number = {7587},
	pages = {484--489},
	pmid = {26819042},
	publisher = {Nature Publishing Group},
	title = {{Mastering the game of Go with deep neural networks and tree search}},
	url = {http://dx.doi.org/10.1038/nature16961},
	volume = {529},
	year = {2016}
}

@article{Vinyals2017,
	abstract = {This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.},
	archivePrefix = {arXiv},
	arxivId = {1708.04782},
	author = {Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and K{\"{u}}ttler, Heinrich and Agapiou, John and Schrittwieser, Julian and Quan, John and Gaffney, Stephen and Petersen, Stig and Simonyan, Karen and Schaul, Tom and van Hasselt, Hado and Silver, David and Lillicrap, Timothy and Calderone, Kevin and Keet, Paul and Brunasso, Anthony and Lawrence, David and Ekermo, Anders and Repp, Jacob and Tsing, Rodney},
	doi = {https://deepmind.com/documents/110/sc2le.pdf},
	eprint = {1708.04782},
	file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Vinyals et al/Vinyals et al. - 2017 - StarCraft II A New Challenge for Reinforcement Learning.pdf:pdf},
	title = {{StarCraft II: A New Challenge for Reinforcement Learning}},
	url = {http://arxiv.org/abs/1708.04782},
	year = {2017}
}

@inproceedings{For2017,
	author = {For, Next and In, A I and Ratcliffe, Dino S and Devlin, Sam and Citi, Luca},
	booktitle = {What's Next For AI In Games},
	file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/What's Next For AI In Games/For et al/For et al. - 2017 - Clyde A deep reinforcement learning DOOM playing agent.pdf:pdf},
	isbn = {0000000277693},
	title = {{Clyde: A deep reinforcement learning DOOM playing agent}},
	year = {2017}
}

@book{Sutton2011,
	address = {М.},
	author = {Саттон, Р.С. and Барто, Э. Г.},
	edition = {2-е},
	file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Саттон, Барто/Саттон, Барто - 2011 - Обучение с подкреплением.pdf:pdf},
	language = {russian},
	mendeley-groups = {Reinforcement},
	pages = {399},
	publisher = {БИНОМ. Лаборатория знаний},
	title = {{Обучение с подкреплением}},
	year = {2011}
}

@book{Szepesvari2010,
	abstract = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective.What distin- guishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the al- gorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book,we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming.We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.},
	author = {Szepesv{\'{a}}ri, Csaba},
	booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	doi = {10.2200/S00268ED1V01Y201005AIM009},
	file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Synthesis Lectures on Artificial Intelligence and Machine Learning/Szepesv{\'{a}}ri/Szepesv{\'{a}}ri - 2010 - Algorithms for Reinforcement Learning.pdf:pdf},
	isbn = {9781608454921},
	issn = {1939-4608},
	mendeley-groups = {Topics,Topics/RL+textbook},
	number = {1},
	pages = {1--103},
	title = {{Algorithms for Reinforcement Learning}},
	url = {http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009},
	volume = {4},
	year = {2010}
}

@book{Stefanuk2004,
	address = {М.},
	author = {Стефанюк, В. Л.},
	file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Стефанюк/Стефанюк - 2004 - Локальная организация интеллектуальных систем.pdf:pdf},
	language = {russian},
	mendeley-groups = {Artifial Intelligence/Others AI},
	pages = {328},
	publisher = {ФИЗМАТЛИТ},
	title = {{Локальная организация интеллектуальных систем}},
	year = {2004}
}

@article{Waltz1965,
	author = {Waltz, M and Fu, K},
	doi = {10.1109/TAC.1965.1098193},
	file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Istroduction/Istroduction - 1965 - I )“ (2).pdf:pdf},
	issn = {0018-9286},
	journal = {Automatic Control, IEEE Transactions on},
	number = {4},
	pages = {390--398},
	title = {{A heuristic approach to reinforcement learning control systems}},
	url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1098193},
	volume = {AC-10},
	year = {1965}
}

