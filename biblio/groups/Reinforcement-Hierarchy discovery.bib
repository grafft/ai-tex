Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Cao2012a,
abstract = {We describe an approach to incorporating Bayesian priors in the MAXQ framework for hierarchical reinforcement learning (HRL). We define priors on the primitive environment model and on task pseudo-rewards. Since models for composite tasks can be complex, we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy. We show empirically that (i) our approach results in improved convergence over non-Bayesian baselines, (ii) using both task hierarchies and Bayesian priors is better than either alone, (iii) taking advantage of the task hierarchy reduces the computational cost of Bayesian reinforcement learning and (iv) in this framework, task pseudo-rewards can be learned instead of being manually specified, leading to hierarchically optimal rather than recursively optimal policies.},
author = {Cao, F and Ray, Soumya},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cao, Ray/2012/Bayesian hierarchical reinforcement learning.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {73--81},
title = {{Bayesian hierarchical reinforcement learning}},
url = {http://papers.nips.cc/paper/4752-bay},
year = {2012}
}
@article{Wang2012b,
abstract = {Reinforcement learning has been an important category of machine learning approaches exhibiting self-learning and online learning characteristics. Using reinforcement learning, an agent can learn its behaviors through trial-and-error interactions with a dynamic environment and finally come up with an optimal strategy. Reinforcement learning suffers the curse of dimensionality, though there has been significant progress to overcome this issue in recent years. MAXQ is one of the most common approaches for reinforcement learning. To function properly, MAXQ requires a decomposition of the agent's task into a task hierarchy. Previously, the decomposition can only be done manually. In this paper, we propose a mechanism for automatic subtask discovery. The mechanism applies clustering to automatically construct task hierarchy required by MAXQ, such that MAXQ can be fully automated. We present the design of our mechanism, and demonstrate its effectiveness through theoretical analysis and an extensive experimental evaluation. {\textcopyright} 2012 IEEE.},
author = {Wang, Hongbing and Li, Wenya and Zhou, Xuan},
doi = {10.1109/ICTAI.2012.165},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Wang, Li, Zhou/2012/Automatic discovery and transfer of MAXQ hierarchies in a complex system.pdf:pdf},
isbn = {9780769549156},
issn = {10823409},
journal = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
keywords = {Clustering,MAXQ,Reinforcement Learning,System of Systems},
pages = {1157--1162},
title = {{Automatic discovery and transfer of MAXQ hierarchies in a complex system}},
volume = {1},
year = {2012}
}
@inproceedings{Mehta2008b,
abstract = {We present an algorithm, HI-MAT (Hierar- chy Induction via Models And Trajectories), that discovers MAXQ task hierarchies by ap- plying dynamic Bayesian network models to a successful trajectory from a source rein- forcement learning task. HI-MAT discovers subtasks by analyzing the causal and tem- poral relationships among the actions in the trajectory. Under appropriate assumptions, HI-MAT induces hierarchies that are consis- tent with the observed trajectory and have compact value-function tables employing safe state abstractions. We demonstrate empir- ically that HI-MAT constructs compact hi- erarchies that are comparable to manually- engineered hierarchies and facilitate signifi- cant speedup in learning when transferred to a target task.},
address = {New York, New York, USA},
author = {Mehta, Neville and Ray, Soumya and Tadepalli, Prasad and Dietterich, Thomas},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390238},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Mehta et al/2008/Automatic discovery and transfer of MAXQ hierarchies.pdf:pdf},
isbn = {9781605582054},
pages = {648--655},
publisher = {ACM Press},
title = {{Automatic discovery and transfer of MAXQ hierarchies}},
url = {http://portal.acm.org/citation.cfm?id=1390238 http://portal.acm.org/citation.cfm?doid=1390156.1390238},
year = {2008}
}
@inproceedings{Davoodabadi2011a,
abstract = {In this paper the problem of automatically discovering subtasks and hierarchies in reinforcement learning is considered. We present a novel method that allows an agent to autonomously discover subgoals and create a hierarchy from actions. Our method identifies subgoals by partitioning local state transition graphs. Options constructed for reaching these subgoals are added to action choices and used for accelerating the Q-Learning algorithm. Experimental results show significant performance improvements, especially in the initial learning phase.},
author = {Davoodabadi, M and Beigy, H},
booktitle = {Proceedings of the 5th Indian International Conference on Artificial Intelligence, IICAI 2011},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Davoodabadi, Beigy/2011/A new method for discovering subgoals and constructing options in reinforcement learning.pdf:pdf},
isbn = {9780972741286},
keywords = {Artificial intelligence,Autonomously discovering subgoals,Community detection,Hierarchical reinforcement learning,Learning algorithms,Learning phase,Local state,Option,Performance improvements,Q-learning algorithms,Reinforcement learning,Subgoals,Subtasks},
pages = {441--450},
title = {{A new method for discovering subgoals and constructing options in reinforcement learning}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84872195092{\&}partnerID=40{\&}md5=9bf17b3f8f09d77ec8cd05285124028d},
year = {2011}
}
@article{Konidaris2012a,
abstract = {We present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to significantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that significantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-specific skills.},
author = {Konidaris, George and Scheidwasser, Ilya and Barto, Andrew G.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Konidaris, Scheidwasser, Barto/2012/Transfer in reinforcement learning via shared features.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {reinforcement learning,shaping,skills,transfer},
number = {1},
pages = {1333--1371},
title = {{Transfer in reinforcement learning via shared features}},
url = {http://dl.acm.org/citation.cfm?id=2503308.2343689{\%}5Cnhttp://dl.acm.org/ft{\_}gateway.cfm?id=2343689{\&}type=pdf},
volume = {13},
year = {2012}
}
@incollection{Castro2012a,
abstract = {Abstract. Temporally extended actions are usually effective in speeding up reinforcement learning. In this paper we present a mechanism for automatically constructing such actions, expressed as options [Sutton et al., 1999], in a finite Markov Decision Process (MDP). To do this, we compute a bisimulation metric [Ferns et al., 2004] between the states in a small MDP and the states in a large MDP, which we want to solve. The shape of this metric is then used to completely define a set of options for the large MDP. We demonstrate empirically that our approach is able to improve the speed of reinforcement learning, and is generally not sensitive to parameter tuning.},
author = {Castro, Pablo Samuel and Precup, Doina},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-29946-9_16},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Castro, Precup/2012/Automatic Construction of Temporally Extended Actions for MDPs Using Bisimulation Metrics.pdf:pdf},
isbn = {9783642299452},
issn = {03029743},
pages = {140--152},
title = {{Automatic Construction of Temporally Extended Actions for MDPs Using Bisimulation Metrics}},
url = {http://link.springer.com/10.1007/978-3-642-29946-9{\_}16},
volume = {7188 LNAI},
year = {2012}
}
@inproceedings{MacGlashan2010a,
abstract = {We present skill bootstrapping, a proposed new research direction for agent learning and planning that allows an agent to start with low-level primitive actions, and develop skills that can be used for higher-level planning. Skills are developed over the course of solving many different problems in a domain, using reinforcement learning techniques to complement the benefits and disadvantages of heuristic-search planning. We describe the overall architecture of the proposed approach, discuss how it relates to other work, and give motivating examples for why this approach would be successful.},
author = {MacGlashan, James and DesJArdins, Marie},
booktitle = {24th AAAI Conference on Artificial Intelligence},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/MacGlashan, DesJArdins/2010/Hierarchical Skill Learning for High-Level Planning.pdf:pdf;:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/MacGlashan, DesJArdins/2010/Hierarchical Skill Learning for High-Level Planning(2).pdf:pdf},
isbn = {9781577354666},
keywords = {abstraction,approximation,planning,reinforcement learning},
pages = {1988--1989},
title = {{Hierarchical Skill Learning for High-Level Planning}},
volume = {3},
year = {2010}
}
@article{Hayes2016b,
abstract = {Collaboration between humans and robots requires solutions to an array of challenging problems, including multi-agent planning, state estimation, and goal inference. There already exist feasible solutions for many of these challenges, but they depend upon having rich task models. In this work we detail a novel type of Hierarchical Task Network we call a Clique/Chain HTN (CC-HTN), alongside an algorithm for autonomously constructing them from topological properties derived from graphical task representations. As the presented method relies on the structure of the task itself, our work imposes no particular type of symbolic insight into motor primitives or environmental representation, making it applicable to a wide variety of use cases critical to human-robot interaction. We present evaluations within a multi-resolution goal inference task and a transfer learning application showing the utility of our approach.},
author = {Hayes, Bradley and Scassellati, Brian},
doi = {10.1109/ICRA.2016.7487760},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Hayes, Scassellati/2016/Autonomously constructing hierarchical task networks for planning and human-robot collaboration.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {5469--5476},
title = {{Autonomously constructing hierarchical task networks for planning and human-robot collaboration}},
volume = {2016-June},
year = {2016}
}
@article{Konidaris2016a,
abstract = {We describe a framework for building abstraction hierarchies whereby an agent alternates skill- and representation-acquisition phases to construct a sequence of increasingly abstract Markov decision processes. Our formulation builds on recent results showing that the appropriate abstract representation of a problem is specified by the agent's skills. We describe how such a hierarchy can be used for fast planning, and illustrate the construction of an appropriate hierarchy for the Taxi domain.},
archivePrefix = {arXiv},
arxivId = {1509.07582},
author = {Konidaris, George},
doi = {10.1016/j.surg.2015.03.052.Rate},
eprint = {1509.07582},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Konidaris/2016/Constructing abstraction hierarchies using a skill-symbol loop.pdf:pdf},
isbn = {6176366127},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning},
pages = {1648--1654},
pmid = {28579718},
title = {{Constructing abstraction hierarchies using a skill-symbol loop}},
volume = {2016-Janua},
year = {2016}
}
@article{Simsek2005a,
abstract = {We present a new subgoal-based method for automatically creating useful skills in reinforcement learning. Our method identifies subgoals by partitioning local state transition graphs—those that are constructed using only the most recent experiences of the agent. The local scope of our subgoal discovery method allows it to successfully identify the type of subgoals we seek—states that lie between two densely-connected regions of the state space—while producing an algorithm with low computational cost.},
author = {Şimşek, {\"{O}}zg{\"{u}}r and Wolfe, Alicia P. and Barto, Andrew G.},
doi = {10.1145/1102351.1102454},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Şimşek, Wolfe, Barto/2005/Identifying useful subgoals in reinforcement learning by local graph partitioning.pdf:pdf},
isbn = {1595931805},
journal = {Proceedings of the 22nd international conference on Machine learning  - ICML '05},
pages = {816--823},
title = {{Identifying useful subgoals in reinforcement learning by local graph partitioning}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102454},
year = {2005}
}
@inproceedings{Goel2017a,
author = {Goel, Karan and Mu, Tong and Brunskill, Emma},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Goel, Mu, Brunskill/2017/Optimal Hierarchical Policy Extraction From Noisy Imperfect Demonstrations.pdf:pdf},
title = {{Optimal Hierarchical Policy Extraction From Noisy Imperfect Demonstrations}},
url = {https://drive.google.com/file/d/101FsZkczKMfGeUBTP-089mhkTeepj8IW/view},
year = {2017}
}
@inproceedings{Mannor2004a,
abstract = {We consider a graph theoretic approach for automatic construction of options in a dynamic environment. A map of the environment is generated on-line by the learning agent, representing the topological structure of the state transitions. A clustering algorithm is then used to partition the state space to different regions. Policies for reaching the different parts of the space are separately learned and added to the model in a form of options (macro-actions). The options are used for accelerating the Q-Learning algorithm. We extend the basic algorithm and consider building a map that includes preliminary indication of the location of "interesting" regions of the state space, where the value gradient is significant and additional exploration might be beneficial. Experiments indicate significant speedups, especially in the initial learning phase.},
address = {New York, New York, USA},
author = {Mannor, Shie and Menache, Ishai and Hoze, Amit and Klein, Uri},
booktitle = {Twenty-first international conference on Machine learning - ICML '04},
doi = {10.1145/1015330.1015355},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Mannor et al/2004/Dynamic abstraction in reinforcement learning via clustering.pdf:pdf},
isbn = {1581138285},
keywords = {clustering,hierarchical reinforcement learning,options,q-learning,reinforcement learning},
pages = {71},
publisher = {ACM Press},
title = {{Dynamic abstraction in reinforcement learning via clustering}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015355},
year = {2004}
}
@article{Daniel2016a,
abstract = {Tasks that require many sequential decisions are hard to solve using conventional rein- forcement learning algorithms. Based on the option framework, we propose a model which aims to alleviate these concerns. Instead of learning a single monolithic policy, the agent learns a set of simpler sub-policies as well as the initiation and termination probabili- ties for each of those sub-policies. While ex- isting option learning algorithms frequently require manual specification of components such as the sub-policies, we present an algo- rithm which infers all relevant components of the option framework from data. We present results on SMDPs with discrete as well as continuous state-action spaces. The results show that the presented algorithm can combine simple sub-policies to solve complex tasks and can improve learning performance on simpler tasks.},
author = {Daniel, Christian and van Hoof, Herke and Peters, Jan and Neumann, Gerhard},
doi = {10.1007/s10994-016-5580-x},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Daniel et al/2016/Probabilistic inference for determining options in reinforcement learning.pdf:pdf},
issn = {15730565},
journal = {Machine Learning},
keywords = {Options,Reinforcement learning,Robot learning,Semi Markov decision process},
number = {2-3},
pages = {337--357},
publisher = {Springer US},
title = {{Probabilistic inference for determining options in reinforcement learning}},
volume = {104},
year = {2016}
}
@incollection{Menache2002a,
abstract = {We present the Q-Cut algorithm, a graph theoretic approach for automatic detection of sub-goals in a dynamic environment, which is used for acceleration of the Q-Learning algorithm. The learning agent creates an on-line map of the process history, and uses an efficient Max-Flow/Min-Cut algorithm for identifying bottlenecks. The policies for reaching bottlenecks are separately learned and added to the model in a form of options (macro-actions). We then extend the basic Q-Cut algorithm to the Segmented Q-Cut algorithm, which uses previously identified bottlenecks for state space partitioning, necessary for finding additional bottlenecks in complex environments. Experiments show significant performance improvements, particulary in the initial learning phase.},
author = {Menache, Ishai and Mannor, Shie and Shimkin, Nahum},
booktitle = {ECML 2002: Machine Learning: ECML 2002},
doi = {10.1007/3-540-36755-1_25},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Menache, Mannor, Shimkin/2002/Q-Cut—Dynamic Discovery of Sub-goals in Reinforcement Learning.pdf:pdf},
isbn = {978-3-540-44036-9, 978-3-540-36755-0},
issn = {16113349},
pages = {295--306},
title = {{Q-Cut—Dynamic Discovery of Sub-goals in Reinforcement Learning}},
url = {http://link.springer.com/10.1007/3-540-36755-1{\_}25},
year = {2002}
}
