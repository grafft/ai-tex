Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Fernando2017,
abstract = {For artificial general intelligence (AGI) it would be efficient if multiple users trained the same giant neural network, permitting parameter reuse, without catastrophic forgetting. PathNet is a first step in this direction. It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks. Agents are pathways (views) through the network which determine the subset of parameters that are used and updated by the forwards and backwards passes of the backpropogation algorithm. During learning, a tournament selection genetic algorithm is used to select pathways through the neural network for replication and mutation. Pathway fitness is the performance of that pathway measured according to a cost function. We demonstrate successful transfer learning; fixing the parameters along a path learned on task A and re-evolving a new population of paths for task B, allows task B to be learned faster than it could be learned from scratch or after fine-tuning. Paths evolved on task B re-use parts of the optimal path evolved on task A. Positive transfer was demonstrated for binary MNIST, CIFAR, and SVHN supervised learning classification tasks, and a set of Atari and Labyrinth reinforcement learning tasks, suggesting PathNets have general applicability for neural network training. Finally, PathNet also significantly improves the robustness to hyperparameter choices of a parallel asynchronous reinforcement learning algorithm (A3C).},
archivePrefix = {arXiv},
arxivId = {1701.08734},
author = {Fernando, Chrisantha and Banarse, Dylan and Blundell, Charles and Zwols, Yori and Ha, David and Rusu, Andrei A. and Pritzel, Alexander and Wierstra, Daan},
eprint = {1701.08734},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Fernando et al/2017/PathNet Evolution Channels Gradient Descent in Super Neural Networks.pdf:pdf},
issn = {1701.08734},
journal = {ArXiv},
keywords = {basal ganglia,continual learning,evolution and,giant networks,learning,multitask,path evolution algorithm,transfer learning},
title = {{PathNet: Evolution Channels Gradient Descent in Super Neural Networks}},
url = {http://arxiv.org/abs/1701.08734},
year = {2017}
}
@article{Scellier2017,
abstract = {We introduce Equilibrium Propagation, a learning framework for energy-based models. It involves only one kind of neural computation, performed in both the first phase (when the prediction is made) and the second phase of training (after the target or prediction error is revealed). Although this algorithm computes the gradient of an objective function just like Backpropagation, it does not need a special computation or circuit for the second phase, where errors are implicitly propagated. Equilibrium Propagation shares similarities with Contrastive Hebbian Learning and Contrastive Divergence while solving the theoretical issues of both algorithms: our algorithm computes the gradient of a well defined objective function. Because the objective function is defined in terms of local perturbations, the second phase of Equilibrium Propagation corresponds to only nudging the prediction (fixed point, or stationary distribution) towards a configuration that reduces prediction error. In the case of a recurrent multi-layer supervised network, the output units are slightly nudged towards their target in the second phase, and the perturbation introduced at the output layer propagates backward in the hidden layers. We show that the signal 'back-propagated' during this second phase corresponds to the propagation of error derivatives and encodes the gradient of the objective function, when the synaptic update corresponds to a standard form of spike-timing dependent plasticity. This work makes it more plausible that a mechanism similar to Backpropagation could be implemented by brains, since leaky integrator neural computation performs both inference and error back-propagation in our model. The only local difference between the two phases is whether synaptic changes are allowed or not.},
author = {Scellier, Benjamin and Bengio, Yoshua},
doi = {10.3389/fncom.2017.00024},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Scellier, Bengio/2017/Equilibrium Propagation Bridging the Gap Between Energy-Based Models and Backpropagation.pdf:pdf},
issn = {1662-5188},
journal = {Frontiers in Computational Neuroscience},
keywords = {01 december 2016,artificial neural network,backpropagation algorit,backpropagation algorithm,biologically plausible learning rule,contrastive,deep learning,fixed point,hebbian learning,hopfield networks,received,senior fellow of cifar,spike-timing dependent plasticity},
pages = {1--13},
title = {{Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation}},
volume = {11},
year = {2017}
}
@article{Beck2007,
abstract = {From first principles, we derive a quadratic nonlinear, first-order dynamical system capable of performing exact Bayes-Markov inferences for a wide class of biologically plausible stimulus-dependent patterns of activity while simultaneously providing an online estimate of model performance. This is accomplished by constructing a dynamical system that has solutions proportional to the probability distribution over the stimulus space, but with a constant of proportionality adjusted to provide a local estimate of the probability of the recent observations of stimulus-dependent activity-given model parameters. Next, we transform this exact equation to generate nonlinear equations for the exact evolution of log likelihood and log-likelihood ratios and show that when the input has low amplitude, linear rate models for both the likelihood and the log-likelihood functions follow naturally from these equations. We use these four explicit representations of the probability distribution to argue that, in contrast to the arguments of previous work, the dynamical system for the exact evolution of the likelihood (as opposed to the log likelihood or log-likelihood ratios) not only can be mapped onto a biologically plausible network but is also more consistent with physiological observations.},
author = {Beck, Jeffrey M and Pouget, Alexandre},
doi = {10.1162/neco.2007.19.5.1344},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Beck, Pouget/2007/Exact inferences in a neural implementation of a hidden Markov model.pdf:pdf},
isbn = {0899-7667 (Print)},
issn = {0899-7667},
journal = {Neural computation},
number = {5},
pages = {1344--1361},
pmid = {17381269},
title = {{Exact inferences in a neural implementation of a hidden Markov model}},
volume = {19},
year = {2007}
}
