Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Chaplot2017,
abstract = {To perform tasks specified by natural language instructions, autonomous agents need to extract semantically meaningful representations of language and map it to visual elements and actions in the environment. This problem is called task-oriented language grounding. We propose an end-to-end trainable neural architecture for task-oriented language grounding in 3D environments which assumes no prior linguistic or perceptual knowledge and requires only raw pixels from the environment and the natural language instruction as input. The proposed model combines the image and text representations using a Gated-Attention mechanism and learns a policy to execute the natural language instruction using standard reinforcement and imitation learning methods. We show the effectiveness of the proposed model on unseen instructions as well as unseen maps, both quantitatively and qualitatively. We also introduce a novel environment based on a 3D game engine to simulate the challenges of task-oriented language grounding over a rich set of instructions and environment states.},
archivePrefix = {arXiv},
arxivId = {1706.07230},
author = {Chaplot, Devendra Singh and Sathyendra, Kanthashree Mysore and Pasumarthi, Rama Kumar and Rajagopal, Dheeraj and Salakhutdinov, Ruslan},
eprint = {1706.07230},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Chaplot et al/Chaplot et al. - 2017 - Gated-Attention Architectures for Task-Oriented Language Grounding.pdf:pdf},
title = {{Gated-Attention Architectures for Task-Oriented Language Grounding}},
url = {http://arxiv.org/abs/1706.07230},
year = {2017}
}
@article{Levine2017,
abstract = {Deep reinforcement learning (DRL) methods such as the Deep Q-Network (DQN) have achieved state-of-the-art results in a variety of challenging, high-dimensional domains. This success is mainly attributed to the power of deep neural networks to learn rich domain representations for approximating the value function or policy. Batch reinforcement learning methods with linear representations, on the other hand, are more stable and require less hyper parameter tuning. Yet, substantial feature engineering is necessary to achieve good results. In this work we propose a hybrid approach -- the Least Squares Deep Q-Network (LS-DQN), which combines rich feature representations learned by a DRL algorithm with the stability of a linear least squares method. We do this by periodically re-training the last hidden layer of a DRL network with a batch least squares update. Key to our approach is a Bayesian regularization term for the least squares update, which prevents over-fitting to the more recent data. We tested LS-DQN on five Atari games and demonstrate significant improvement over vanilla DQN and Double-DQN. We also investigated the reasons for the superior performance of our method. Interestingly, we found that the performance improvement can be attributed to the large batch size used by the LS method when optimizing the last layer.},
archivePrefix = {arXiv},
arxivId = {1705.07461},
author = {Levine, Nir and Zahavy, Tom and Mankowitz, Daniel J. and Tamar, Aviv and Mannor, Shie},
eprint = {1705.07461},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Levine et al/Levine et al. - 2017 - Shallow Updates for Deep Reinforcement Learning.pdf:pdf},
issn = {10495258},
number = {Nips 2017},
pages = {1--14},
title = {{Shallow Updates for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1705.07461},
year = {2017}
}
@article{Haarnoja2018,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy - that is, succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
archivePrefix = {arXiv},
arxivId = {1801.01290},
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
eprint = {1801.01290},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Haarnoja et al/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf:pdf},
title = {{Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}},
url = {http://arxiv.org/abs/1801.01290},
year = {2018}
}
@article{Al-Emran2015,
author = {Al-Emran, Mostafa},
doi = {10.12785/ijcds/040207},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/International Journal of Computing and Digital Systems/Al-Emran/Al-Emran - 2015 - Hierarchical Reinforcement Learning - A Survey.pdf:pdf},
issn = {2210-142X},
journal = {International Journal of Computing and Digital Systems},
keywords = {hierarchical reinforcement learning,q-learning,reinforcement learning},
month = {apr},
number = {2},
pages = {137--143},
title = {{Hierarchical Reinforcement Learning - A Survey}},
url = {https://journal.journals.uob.edu.bh//Article/ArticleFile/2144},
volume = {4},
year = {2015}
}
@article{Oh2016,
abstract = {In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.},
archivePrefix = {arXiv},
arxivId = {1605.09128},
author = {Oh, Junhyuk and Chockalingam, Valliappa and Singh, Satinder and Lee, Honglak},
doi = {10.1109/CVPR.2014.180},
eprint = {1605.09128},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Oh et al/Oh et al. - 2016 - Control of Memory, Active Perception, and Action in Minecraft.pdf:pdf},
isbn = {9781510829008},
issn = {10636919},
title = {{Control of Memory, Active Perception, and Action in Minecraft}},
url = {http://arxiv.org/abs/1605.09128},
year = {2016}
}
@inproceedings{Ghadirzadeh2017,
abstract = {Skilled robot task learning is best implemented by predictive action policies due to the inherent latency of sensorimotor processes. However, training such predictive policies is challenging as it involves finding a trajectory of motor activations for the full duration of the action. We propose a data-efficient deep predictive policy training (DPPT) framework with a deep neural network policy architecture which maps an image observation to a sequence of motor activations. The architecture consists of three sub-networks referred to as the perception, policy and behavior super-layers. The perception and behavior super-layers force an abstraction of visual and motor data trained with synthetic and simulated training samples, respectively. The policy super-layer is a small sub-network with fewer parameters that maps data in-between the abstracted manifolds. It is trained for each task using methods for policy search reinforcement learning. We demonstrate the suitability of the proposed architecture and learning framework by training predictive policies for skilled object grasping and ball throwing on a PR2 robot. The effectiveness of the method is illustrated by the fact that these tasks are trained using only about 180 real robot attempts with qualitative terminal rewards.},
archivePrefix = {arXiv},
arxivId = {1703.00727},
author = {Ghadirzadeh, Ali and Maki, Atsuto and Kragic, Danica and Bjorkman, Marten},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2017.8206046},
eprint = {1703.00727},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IEEE International Conference on Intelligent Robots and Systems/Ghadirzadeh et al/Ghadirzadeh et al. - 2017 - Deep predictive policy training using reinforcement learning.pdf:pdf},
isbn = {9781538626825},
issn = {21530866},
pages = {2351--2358},
title = {{Deep predictive policy training using reinforcement learning}},
year = {2017}
}
@article{Kober2013,
abstract = {Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
doi = {10.1177/0278364913495721},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/The International Journal of Robotics Research/Kober, Bagnell, Peters/Kober, Bagnell, Peters - 2013 - Reinforcement learning in robotics A survey.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {learning control,reinforcement learning,robot,survey},
month = {sep},
number = {11},
pages = {1238--1274},
title = {{Reinforcement learning in robotics: A survey}},
url = {http://link.springer.com/10.1007/978-3-319-03194-1{\_}2 http://journals.sagepub.com/doi/10.1177/0278364913495721},
volume = {32},
year = {2013}
}
@book{WieringMarco;vanOtterlo2012,
abstract = {Reinforcement Learning was originally developed for Markov Decision Processes (MDPs). It allows a single agent to learn a policy that maximizes a pos- sibly delayed reward signal in a stochastic stationary environment. It guarantees convergence to the optimal policy, provided that the agent can sufficiently experi- ment and the environment in which it is operating is Markovian. However, when multiple agents apply reinforcement learning in a shared environment, thismight be beyond theMDP model. In such systems, the optimal policy of an agent depends not only on the environment, but on the policies of the other agents aswell. These situa- tions arise naturally in a variety of domains, such as: robotics, telecommunications, economics, distributed control, auctions, traffic light control, etc. In these domains multi-agent learning is used, either because of the complexity of the domain or be- cause control is inherently decentralized. In such systems it is important that agents are capable of discovering good solutions to the problem at hand either by coordi- nating with other learners or by competing with them. This chapter focuses on the application reinforcement learning techniques in multi-agent systems.We describe a basic learning framework based on the economic research into game theory, and illustrate the additional complexity that arises in such systems. We also described a representative selection of algorithms for the different areas of multi-agent rein- forcement learning research.},
doi = {10.1007/978-3-642-27645-3},
editor = {Wiering, Marco and van Otterlo, Martijn},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Unknown/Unknown - 2012 - Reinforcement Learning The State of the Art.pdf:pdf},
isbn = {978-3-642-27644-6},
pages = {652},
publisher = {Springer},
title = {{Reinforcement Learning: The State of the Art}},
year = {2012}
}
@inproceedings{Sutton2011a,
abstract = {Maintaining accurate world knowledge in a complex and changing environment is a perennial problem for robots and other arti cial intelligence systems. Our architecture for addressing this problem, called Horde, consists of a large number of independent reinforcement learning sub-agents, or demons. Each demon is responsible for answering a sin- gle predictive or goal-oriented question about the world, thereby contributing in a factored, modular way to the sys- tem's overall knowledge. The questions are in the form of a value function, but each demon has its own policy, re- ward function, termination function, and terminal-reward function unrelated to those of the base problem. Learning proceeds in parallel by all demons simultaneously so as to extract the maximal training information from whatever ac- tions are taken by the system as a whole. Gradient-based temporal-di erence learning methods are used to learn ef- ciently and reliably with function approximation in this o -policy setting. Horde runs in constant time and memory per time step, and is thus suitable for learning online in real- time applications such as robotics. We present results using Horde on a multi-sensored mobile robot to successfully learn goal-oriented behaviors and long-term predictions from o - policy experience. Horde is a signi cant incremental step towards a real-time architecture for ecient learning of gen-},
author = {Sutton, Richard S and Modayil, Joseph and Delp, Michael and Degris, Thomas and Pilarski, Patrick M and White, Adam},
booktitle = {Proc. of 10th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS 2011)},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proc. of 10th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS 2011)/Sutton et al/Sutton et al. - 2011 - Horde A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorim.pdf:pdf},
keywords = {artificial intelligence,difference learning,inforcement learning,knowledge representation,off policy learning,re,real time,robotics,temporal,value function approximation},
pages = {761--768},
title = {{Horde : A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction}},
url = {http://webdocs.cs.ualberta.ca/{~}sutton/papers/horde-aamas-11.pdf},
year = {2011}
}
@article{Srinivas2018,
abstract = {A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differentiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image-based goals. We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities.},
annote = {Код - https://github.com/aravind0706/upn},
archivePrefix = {arXiv},
arxivId = {1804.00645},
author = {Srinivas, Aravind and Jabri, Allan and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea},
eprint = {1804.00645},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Srinivas et al/Srinivas et al. - 2018 - Universal Planning Networks.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Srinivas et al/Srinivas et al. - 2018 - Universal Planning Networks.jpg:jpg},
title = {{Universal Planning Networks}},
url = {http://arxiv.org/abs/1804.00645},
year = {2018}
}
@article{Co-Reyes2018,
abstract = {In this work, we take a representation learning perspective on hierarchical reinforcement learning, where the problem of learning lower layers in a hierarchy is transformed into the problem of learning trajectory-level generative models. We show that we can learn continuous latent representations of trajectories, which are effective in solving temporally extended and multi-stage problems. Our proposed model, SeCTAR, draws inspiration from variational autoencoders, and learns latent representations of trajectories. A key component of this method is to learn both a latent-conditioned policy and a latent-conditioned model which are consistent with each other. Given the same latent, the policy generates a trajectory which should match the trajectory predicted by the model. This model provides a built-in prediction mechanism, by predicting the outcome of closed loop policy behavior. We propose a novel algorithm for performing hierarchical RL with this model, combining model-based planning in the learned latent space with an unsupervised exploration objective. We show that our model is effective at reasoning over long horizons with sparse rewards for several simulated tasks, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration.},
archivePrefix = {arXiv},
arxivId = {1806.02813},
author = {Co-Reyes, John D. and Liu, YuXuan and Gupta, Abhishek and Eysenbach, Benjamin and Abbeel, Pieter and Levine, Sergey},
eprint = {1806.02813},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Co-Reyes et al/Co-Reyes et al. - 2018 - Self-Consistent Trajectory Autoencoder Hierarchical Reinforcement Learning with Trajectory Embeddings.jpg:jpg;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Co-Reyes et al/Co-Reyes et al. - 2018 - Self-Consistent Trajectory Autoencoder Hierarchical Reinforcement Learning with Trajectory Embeddings.pdf:pdf},
issn = {1938-7228},
title = {{Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings}},
url = {http://arxiv.org/abs/1806.02813},
year = {2018}
}
@article{Arulkumaran2017,
abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep {\$}Q{\$}-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
archivePrefix = {arXiv},
arxivId = {1708.05866},
author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
doi = {10.1109/MSP.2017.2743240},
eprint = {1708.05866},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IEEE Signal Processing Magazine/Arulkumaran et al/Arulkumaran et al. - 2017 - Deep Reinforcement Learning A Brief Survey.pdf:pdf},
isbn = {9781424469178},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
month = {nov},
number = {6},
pages = {26--38},
pmid = {25719670},
title = {{Deep Reinforcement Learning: A Brief Survey}},
url = {http://arxiv.org/abs/1708.05866{\%}0Ahttp://dx.doi.org/10.1109/MSP.2017.2743240 http://ieeexplore.ieee.org/document/8103164/},
volume = {34},
year = {2017}
}
@article{Henderson2017,
abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
archivePrefix = {arXiv},
arxivId = {1709.06560},
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
eprint = {1709.06560},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Henderson et al/Henderson et al. - 2017 - Deep Reinforcement Learning that Matters.pdf:pdf},
title = {{Deep Reinforcement Learning that Matters}},
url = {http://arxiv.org/abs/1709.06560},
year = {2017}
}
@inproceedings{Davoodabadi2011a,
abstract = {In this paper the problem of automatically discovering subtasks and hierarchies in reinforcement learning is considered. We present a novel method that allows an agent to autonomously discover subgoals and create a hierarchy from actions. Our method identifies subgoals by partitioning local state transition graphs. Options constructed for reaching these subgoals are added to action choices and used for accelerating the Q-Learning algorithm. Experimental results show significant performance improvements, especially in the initial learning phase.},
author = {Davoodabadi, M and Beigy, H},
booktitle = {Proceedings of the 5th Indian International Conference on Artificial Intelligence, IICAI 2011},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 5th Indian International Conference on Artificial Intelligence, IICAI 2011/Davoodabadi, Beigy/Davoodabadi, Beigy - 2011 - A new method for discovering subgoals and constructing options in r.pdf:pdf},
isbn = {9780972741286},
keywords = {Artificial intelligence,Autonomously discovering subgoals,Community detection,Hierarchical reinforcement learning,Learning algorithms,Learning phase,Local state,Option,Performance improvements,Q-learning algorithms,Reinforcement learning,Subgoals,Subtasks},
pages = {441--450},
title = {{A new method for discovering subgoals and constructing options in reinforcement learning}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84872195092{\&}partnerID=40{\&}md5=9bf17b3f8f09d77ec8cd05285124028d},
year = {2011}
}
@article{YangLuShujuanYiYurongLiu2016,
author = {Lu, Yang and Yi, Shujuan and Liu, Yurong and Ji, Yuling},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Assembly Automation/Lu et al/Lu et al. - 2016 - A Novel Path Planning Method for Biomimetic Robot Based on Deep Learning.pdf:pdf},
journal = {Assembly Automation},
number = {2},
title = {{A Novel Path Planning Method for Biomimetic Robot Based on Deep Learning}},
volume = {36},
year = {2016}
}
@incollection{Riedmiller2005,
abstract = {This paper introduces NFQ, an algorithm for efficient and ef- fective training of a Q-value function represented by amulti-layer percep- tron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.},
author = {Riedmiller, Martin},
booktitle = {Machine Learning: ECML 2005},
doi = {10.1007/11564096_32},
editor = {Gama, Jo{\~{a}}o and Camacho, Rui and Brazdil, Pavel B. and Jorge, Al{\'{i}}pio M{\'{a}}rio and Torgo, Lu{\'{i}}s},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Machine Learning ECML 2005/Riedmiller/Riedmiller - 2005 - Neural Fitted Q Iteration – First Experiences with a Data Efficient Neural Reinforcement Learning Method.pdf:pdf},
isbn = {3540292438},
issn = {03029743},
pages = {317--328},
publisher = {Springer Berlin Heidelberg},
title = {{Neural Fitted Q Iteration – First Experiences with a Data Efficient Neural Reinforcement Learning Method}},
url = {http://link.springer.com/10.1007/11564096{\_}32},
year = {2005}
}
@article{Sutton1999,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Artificial Intelligence/Sutton, Precup, Singh/Sutton, Precup, Singh - 1999 - Between MDPs and Semi-MDPs A Framework for Temporal Abstraction in Reinforcement Learning.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Artificial Intelligence},
pages = {181--211},
pmid = {25246403},
title = {{Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning}},
volume = {112},
year = {1999}
}
@article{Kaplanis2018,
abstract = {Unlike humans, who are capable of continual learning over their lifetimes, artificial neural networks have long been known to suffer from a phenomenon known as catastrophic forgetting, whereby new learning can lead to abrupt erasure of previously acquired knowledge. Whereas in a neural network the parameters are typically modelled as scalar values, an individual synapse in the brain comprises a complex network of interacting biochemical components that evolve at different timescales. In this paper, we show that by equipping tabular and deep reinforcement learning agents with a synaptic model that incorporates this biological complexity (Benna {\&} Fusi, 2016), catastrophic forgetting can be mitigated at multiple timescales. In particular, we find that as well as enabling continual learning across sequential training of two simple tasks, it can also be used to overcome within-task forgetting by reducing the need for an experience replay database.},
archivePrefix = {arXiv},
arxivId = {1802.07239},
author = {Kaplanis, Christos and Shanahan, Murray and Clopath, Claudia},
eprint = {1802.07239},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Kaplanis, Shanahan, Clopath/Kaplanis, Shanahan, Clopath - 2018 - Continual Reinforcement Learning with Complex Synapses.pdf:pdf},
issn = {1938-7228},
title = {{Continual Reinforcement Learning with Complex Synapses}},
url = {http://arxiv.org/abs/1802.07239},
year = {2018}
}
@article{Shankar2017,
abstract = {Deep Reinforcement Learning has enabled the learning of policies for complex tasks in partially observable environments, without explicitly learning the underlying model of the tasks. While such model-free methods achieve considerable performance, they often ignore the structure of task. We present a natural representation of to Reinforcement Learning (RL) problems using Recurrent Convolutional Neural Networks (RCNNs), to better exploit this inherent structure. We define 3 such RCNNs, whose forward passes execute an efficient Value Iteration, propagate beliefs of state in partially observable environments, and choose optimal actions respectively. Backpropagating gradients through these RCNNs allows the system to explicitly learn the Transition Model and Reward Function associated with the underlying MDP, serving as an elegant alternative to classical model-based RL. We evaluate the proposed algorithms in simulation, considering a robot planning problem. We demonstrate the capability of our framework to reduce the cost of replanning, learn accurate MDP models, and finally re-plan with learnt models to achieve near-optimal policies.},
archivePrefix = {arXiv},
arxivId = {1701.02392},
author = {Shankar, Tanmay and Dwivedy, Santosha K. and Guha, Prithwijit},
doi = {10.1109/ICPR.2016.7900026},
eprint = {1701.02392},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/2016 23rd International Conference on Pattern Recognition (ICPR)/Shankar, Dwivedy, Guha/Shankar, Dwivedy, Guha - 2017 - Reinforcement Learning via Recurrent Convolutional Neural Networks.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/2016 23rd International Conference on Pattern Recognition (ICPR)/Shankar, Dwivedy, Guha/Shankar, Dwivedy, Guha - 2017 - Reinforcement Learning via Recurrent Convolutional Neural Networks.pdf:pdf},
isbn = {978-1-5090-4847-2},
issn = {10514651},
journal = {2016 23rd International Conference on Pattern Recognition (ICPR)},
month = {dec},
pages = {2592--2597},
publisher = {IEEE},
title = {{Reinforcement Learning via Recurrent Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1701.02392 http://ieeexplore.ieee.org/document/7900026/},
year = {2017}
}
@article{Lanctot2017,
abstract = {To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and fictitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.},
archivePrefix = {arXiv},
arxivId = {1711.00832},
author = {Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and Perolat, Julien and Silver, David and Graepel, Thore},
eprint = {1711.00832},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Lanctot et al/Lanctot et al. - 2017 - A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning.pdf:pdf},
number = {Nips},
title = {{A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning}},
url = {http://arxiv.org/abs/1711.00832},
year = {2017}
}
@article{Thomas2017,
abstract = {In the artificial intelligence field, learning often corresponds to changing the parameters of a parameterized function. A learning rule is an algorithm or mathematical expression that specifies precisely how the parameters should be changed. When creating an artificial intelligence system, we must make two decisions: what representation should be used (i.e., what parameterized function should be used) and what learning rule should be used to search through the resulting set of representable functions. Using most learning rules, these two decisions are coupled in a subtle (and often unintentional) way. That is, using the same learning rule with two different representations that can represent the same sets of functions can result in two different outcomes. After arguing that this coupling is undesirable, particularly when using artificial neural networks, we present a method for partially decoupling these two decisions for a broad class of learning rules that span unsupervised learning, reinforcement learning, and supervised learning.},
archivePrefix = {arXiv},
arxivId = {1706.03100},
author = {Thomas, Philip S. and Dann, Christoph and Brunskill, Emma},
eprint = {1706.03100},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Thomas, Dann, Brunskill/Thomas, Dann, Brunskill - 2017 - Decoupling Learning Rules from Representations.pdf:pdf},
keywords = {TODO},
title = {{Decoupling Learning Rules from Representations}},
url = {http://arxiv.org/abs/1706.03100},
volume = {1},
year = {2017}
}
@inproceedings{Sharma2017,
author = {Sharma, Avinash and Gupta, Kanika and Kumar, Anirudha and Sharma, Aishwarya and Kumar, Rajesh and Member, Senior},
booktitle = {2017 IEEE International Conference on Industrial Technology (ICIT)},
doi = {10.1109/ICIT.2017.7915468},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/2017 IEEE International Conference on Industrial Technology (ICIT)/Sharma et al/Sharma et al. - 2017 - Model based path planning using Q-Learning.pdf:pdf},
keywords = {Control System,Robotics and Automation},
pages = {837--842},
publisher = {IEEE},
title = {{Model based path planning using Q-Learning}},
year = {2017}
}
@article{Osband2016,
abstract = {Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.},
archivePrefix = {arXiv},
arxivId = {1602.04621},
author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and {Van Roy}, Benjamin},
doi = {10.1145/2661829.2661935},
eprint = {1602.04621},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Osband et al/Osband et al. - 2016 - Deep Exploration via Bootstrapped DQN.pdf:pdf},
isbn = {9781479969227},
issn = {10495258},
pages = {1--18},
pmid = {15003161},
title = {{Deep Exploration via Bootstrapped DQN}},
url = {http://arxiv.org/abs/1602.04621},
year = {2016}
}
@article{Schaul2015,
abstract = {Value functions are a core component of rein- forcement learning systems. The main idea is to to construct a single function approximator V (s; $\theta$) that estimates the long-term reward from any state s, using parameters $\theta$. In this paper we introduce universal value function approx- imators (UVFAs) V (s, g; $\theta$) that generalise not just over states s but also over goals g. We de- velop an efficient technique for supervised learn- ing of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a re- inforcement learning algorithm that updates the UVFAsolely from observed rewards. Finally, we demonstrate that a UVFAcan successfully gener- alise to previously unseen goals. 1.},
author = {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of The 32nd International Conference on Machine Learning/Schaul et al/Schaul et al. - 2015 - Universal Value Function Approximators.pdf:pdf},
isbn = {9781510810587},
issn = {1938-7228},
journal = {Proceedings of The 32nd International Conference on Machine Learning},
pages = {1312--1320},
title = {{Universal Value Function Approximators}},
url = {http://jmlr.org/proceedings/papers/v37/schaul15.html},
year = {2015}
}
@article{Dimakopoulou2018,
abstract = {We consider a team of reinforcement learning agents that concurrently operate in a common environment, and we develop an approach to efficient coordinated exploration that is suitable for problems of practical scale. Our approach builds on seed sampling (Dimakopoulou and Van Roy, 2018) and randomized value function learning (Osband et al., 2016). We demonstrate that, for simple tabular contexts, the approach is competitive with previously proposed tabular model learning methods (Dimakopoulou and Van Roy, 2018). With a higher-dimensional problem and a neural network value function representation, the approach learns quickly with far fewer agents than alternative exploration schemes.},
archivePrefix = {arXiv},
arxivId = {1805.08948},
author = {Dimakopoulou, Maria and Osband, Ian and {Van Roy}, Benjamin},
doi = {10.1.1.151.8250},
eprint = {1805.08948},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Dimakopoulou, Osband, Van Roy/Dimakopoulou, Osband, Van Roy - 2018 - Scalable Coordinated Exploration in Concurrent Reinforcement Learning.pdf:pdf},
isbn = {0262042088},
issn = {1049-5258},
title = {{Scalable Coordinated Exploration in Concurrent Reinforcement Learning}},
url = {http://arxiv.org/abs/1805.08948},
year = {2018}
}
@unpublished{Sutton2016,
author = {Sutton, Richard S and Barto, Andrew G},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Sutton, Barto/Sutton, Barto - 2017 - Reinforcement learning An introduction.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Sutton, Barto/Sutton, Barto - 2017 - Reinforcement learning An introduction(2).pdf:pdf},
pages = {445},
title = {{Reinforcement learning: An introduction}},
year = {2017}
}
@article{Nachum2017,
abstract = {We establish a new connection between value and policy based reinforcement learning (RL) based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization. Specifically, we show that softmax consistent action values correspond to optimal entropy regularized policy probabilities along any action sequence, regardless of provenance. From this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes a notion of soft consistency error along multi-step action sequences extracted from both on- and off-policy traces. We examine the behavior of PCL in different scenarios and show that PCL can be interpreted as generalizing both actor-critic and Q-learning algorithms. We subsequently deepen the relationship by showing how a single model can be used to represent both a policy and the corresponding softmax state values, eliminating the need for a separate critic. The experimental evaluation demonstrates that PCL significantly outperforms strong actor-critic and Q-learning baselines across several benchmarks.},
archivePrefix = {arXiv},
arxivId = {1702.08892},
author = {Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
eprint = {1702.08892},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Nachum et al/Nachum et al. - 2017 - Bridging the Gap Between Value and Policy Based Reinforcement Learning.pdf:pdf},
number = {Nips},
pages = {1--21},
title = {{Bridging the Gap Between Value and Policy Based Reinforcement Learning}},
url = {http://arxiv.org/abs/1702.08892},
year = {2017}
}
@article{Cao2012a,
abstract = {We describe an approach to incorporating Bayesian priors in the MAXQ framework for hierarchical reinforcement learning (HRL). We define priors on the primitive environment model and on task pseudo-rewards. Since models for composite tasks can be complex, we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy. We show empirically that (i) our approach results in improved convergence over non-Bayesian baselines, (ii) using both task hierarchies and Bayesian priors is better than either alone, (iii) taking advantage of the task hierarchy reduces the computational cost of Bayesian reinforcement learning and (iv) in this framework, task pseudo-rewards can be learned instead of being manually specified, leading to hierarchically optimal rather than recursively optimal policies.},
author = {Cao, F and Ray, Soumya},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Advances in Neural Information Processing Systems/Cao, Ray/Cao, Ray - 2012 - Bayesian hierarchical reinforcement learning.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {73--81},
title = {{Bayesian hierarchical reinforcement learning}},
url = {http://papers.nips.cc/paper/4752-bay},
year = {2012}
}
@article{Deisenroth2011,
abstract = {Policy search is a subfield in reinforcement learning which focuses on finding good parameters for a given policy parametrization. It is well suited for robotics as it can cope with high-dimensional state and action spaces, one of the main challenges in robot learning. We review recent successes of both model-free and model-based policy search in robot learning. Model-free policy search is a general approach to learn policies based on sampled trajectories. We classify model-free methods based on their policy evaluation strategy, policy update strategy, and exploration strategy and present a unified view on existing algorithms. Learning a policy is often easier than learning an accurate forward model, and, hence, model-free methods are more frequently used in practice. However, for each sampled trajectory, it is necessary to interact with the robot, which can be time consuming and challenging in practice. Model-based policy search addresses this problem by first learning a simulator of the robot's dynamics from data. Subsequently, the simulator generates trajectories that are used for policy learning. For both model-free and model-based policy search methods, we review their respective properties and their applicability to robotic systems.},
author = {Deisenroth, Marc Peter and Neumann, Gerhard and Peters, Jan},
doi = {10.1561/2300000021},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Foundations and Trends in Robotics/Deisenroth, Neumann, Peters/Deisenroth, Neumann, Peters - 2011 - A Survey on Policy Search for Robotics.pdf:pdf},
isbn = {9781601987037},
issn = {1935-8253},
journal = {Foundations and Trends in Robotics},
keywords = {Artificial Intelligence in Robotics,Markov Decision Processes,Planning and Control,Policy Search},
number = {1-2},
pages = {1--142},
title = {{A Survey on Policy Search for Robotics}},
url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-robotics/ROB-021},
volume = {2},
year = {2011}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Schulman et al/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:pdf},
pages = {1--12},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@inproceedings{Turner2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1706.00387v1},
author = {Turner, Richard E and Lillicrap, Timothy and Sch{\"{o}}lkopf, Bernhard},
booktitle = {ArXiv},
eprint = {arXiv:1706.00387v1},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/ArXiv/Turner, Lillicrap, Sch{\"{o}}lkopf/Turner, Lillicrap, Sch{\"{o}}lkopf - 2016 - Interpolated Policy Gradient Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinfo.pdf:pdf},
title = {{Interpolated Policy Gradient : Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning}},
year = {2016}
}
@article{Lee2018,
abstract = {Value Iteration Networks (VINs) are effective differentiable path planning modules that can be used by agents to perform navigation while still maintaining end-to-end differentiability of the entire architecture. Despite their effectiveness, they suffer from several disadvantages including training instability, random seed sensitivity, and other optimization problems. In this work, we reframe VINs as recurrent-convolutional networks which demonstrates that VINs couple recurrent convolutions with an unconventional max-pooling activation. From this perspective, we argue that standard gated recurrent update equations could potentially alleviate the optimization issues plaguing VIN. The resulting architecture, which we call the Gated Path Planning Network, is shown to empirically outperform VIN on a variety of metrics such as learning speed, hyperparameter sensitivity, iteration count, and even generalization. Furthermore, we show that this performance gap is consistent across different maze transition types, maze sizes and even show success on a challenging 3D environment, where the planner is only provided with first-person RGB images.},
archivePrefix = {arXiv},
arxivId = {1806.06408},
author = {Lee, Lisa and Parisotto, Emilio and Chaplot, Devendra Singh and Xing, Eric and Salakhutdinov, Ruslan},
eprint = {1806.06408},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Lee et al/Lee et al. - 2018 - Gated Path Planning Networks.pdf:pdf},
issn = {1938-7228},
title = {{Gated Path Planning Networks}},
url = {http://arxiv.org/abs/1806.06408},
year = {2018}
}
@article{Hayes2016b,
abstract = {Collaboration between humans and robots requires solutions to an array of challenging problems, including multi-agent planning, state estimation, and goal inference. There already exist feasible solutions for many of these challenges, but they depend upon having rich task models. In this work we detail a novel type of Hierarchical Task Network we call a Clique/Chain HTN (CC-HTN), alongside an algorithm for autonomously constructing them from topological properties derived from graphical task representations. As the presented method relies on the structure of the task itself, our work imposes no particular type of symbolic insight into motor primitives or environmental representation, making it applicable to a wide variety of use cases critical to human-robot interaction. We present evaluations within a multi-resolution goal inference task and a transfer learning application showing the utility of our approach.},
author = {Hayes, Bradley and Scassellati, Brian},
doi = {10.1109/ICRA.2016.7487760},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings - IEEE International Conference on Robotics and Automation/Hayes, Scassellati/Hayes, Scassellati - 2016 - Autonomously constructing hierarchical task networks for planning and human-robot collabor.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {5469--5476},
title = {{Autonomously constructing hierarchical task networks for planning and human-robot collaboration}},
volume = {2016-June},
year = {2016}
}
@inproceedings{MacGlashan2010a,
abstract = {We present skill bootstrapping, a proposed new research direction for agent learning and planning that allows an agent to start with low-level primitive actions, and develop skills that can be used for higher-level planning. Skills are developed over the course of solving many different problems in a domain, using reinforcement learning techniques to complement the benefits and disadvantages of heuristic-search planning. We describe the overall architecture of the proposed approach, discuss how it relates to other work, and give motivating examples for why this approach would be successful.},
author = {MacGlashan, James and DesJArdins, Marie},
booktitle = {24th AAAI Conference on Artificial Intelligence},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/24th AAAI Conference on Artificial Intelligence/MacGlashan, DesJArdins/MacGlashan, DesJArdins - 2010 - Hierarchical Skill Learning for High-Level Planning.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/24th AAAI Conference on Artificial Intelligence/MacGlashan, DesJArdins/MacGlashan, DesJArdins - 2010 - Hierarchical Skill Learning for High-Level Planning.pdf:pdf},
isbn = {9781577354666},
keywords = {abstraction,approximation,planning,reinforcement learning},
pages = {1988--1989},
title = {{Hierarchical Skill Learning for High-Level Planning}},
volume = {3},
year = {2010}
}
@incollection{Xia2015,
address = {Paris},
author = {Xia, Chen and {El Kamel}, A.},
booktitle = {Proceedings of the 21st International Conference on Industrial Engineering and Engineering Management 2014},
doi = {10.2991/978-94-6239-102-4_136},
editor = {Qi, Ershi and Shen, Jiang and Dou, Runliang},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 21st International Conference on Industrial Engineering and Engineering Management 2014/Xia, El Kamel/Xia, El Kamel - 2015 - A Reinforcement Learning Method of Obstacle Avoidance for Indust.pdf:pdf},
isbn = {978-94-6239-101-7},
keywords = {automation,capacity building,f rxqghu,ngo,npo,resource mismanagement,syndrome,v},
pages = {671--675},
publisher = {Atlantis Press},
series = {Proceedings of the International Conference on Industrial Engineering and Engineering Management},
title = {{A Reinforcement Learning Method of Obstacle Avoidance for Industrial Mobile Vehicles in Unknown Environments Using Neural Network}},
url = {http://link.springer.com/10.2991/978-94-6239-102-4 http://link.springer.com/10.2991/978-94-6239-102-4{\_}136},
volume = {2014},
year = {2015}
}
@inproceedings{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
booktitle = {arXiv: 1312.5602},
eprint = {1312.5602},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/arXiv 1312.5602/Mnih et al/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:pdf},
pages = {1--9},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Daniel2016a,
abstract = {Tasks that require many sequential decisions are hard to solve using conventional rein- forcement learning algorithms. Based on the option framework, we propose a model which aims to alleviate these concerns. Instead of learning a single monolithic policy, the agent learns a set of simpler sub-policies as well as the initiation and termination probabili- ties for each of those sub-policies. While ex- isting option learning algorithms frequently require manual specification of components such as the sub-policies, we present an algo- rithm which infers all relevant components of the option framework from data. We present results on SMDPs with discrete as well as continuous state-action spaces. The results show that the presented algorithm can combine simple sub-policies to solve complex tasks and can improve learning performance on simpler tasks.},
author = {Daniel, Christian and van Hoof, Herke and Peters, Jan and Neumann, Gerhard},
doi = {10.1007/s10994-016-5580-x},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Machine Learning/Daniel et al/Daniel et al. - 2016 - Probabilistic inference for determining options in reinforcement learning.pdf:pdf},
issn = {15730565},
journal = {Machine Learning},
keywords = {Options,Reinforcement learning,Robot learning,Semi Markov decision process},
number = {2-3},
pages = {337--357},
publisher = {Springer US},
title = {{Probabilistic inference for determining options in reinforcement learning}},
volume = {104},
year = {2016}
}
@article{Efroni2018,
abstract = {The famous Policy Iteration algorithm alternates between policy improvement and policy evaluation. Implementations of this algorithm with several variants of the latter evaluation stage, e.g, {\$}n{\$}-step and trace-based returns, have been analyzed in previous works. However, the case of multiple-step lookahead policy improvement, despite the recent increase in empirical evidence of its strength, has to our knowledge not been carefully analyzed yet. In this work, we introduce the first such analysis. Namely, we formulate variants of multiple-step policy improvement, derive new algorithms using these definitions and prove their convergence. Moreover, we show that recent prominent Reinforcement Learning algorithms are, in fact, instances of our framework. We thus shed light on their empirical success and give a recipe for deriving new algorithms for future study.},
archivePrefix = {arXiv},
arxivId = {1802.03654},
author = {Efroni, Yonathan and Dalal, Gal and Scherrer, Bruno and Mannor, Shie},
eprint = {1802.03654},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Efroni et al/Efroni et al. - 2018 - Beyond the One Step Greedy Approach in Reinforcement Learning.pdf:pdf},
title = {{Beyond the One Step Greedy Approach in Reinforcement Learning}},
url = {http://arxiv.org/abs/1802.03654},
year = {2018}
}
@inproceedings{Lange2010,
author = {Lange, Sascha and Riedmiller, Martin},
booktitle = {The 2010 International Joint Conference on Neural Networks (IJCNN)},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/The 2010 International Joint Conference on Neural Networks (IJCNN)/Lange, Riedmiller/Lange, Riedmiller - 2010 - Deep Auto-Encoder Neural Networks in Reinforcement Learning.pdf:pdf},
pages = {1--8},
publisher = {IEEE},
title = {{Deep Auto-Encoder Neural Networks in Reinforcement Learning}},
year = {2010}
}
@inproceedings{Singh2005a,
author = {Singh, Satinder and Barto, Andrew G. and Chentanez, Nuttapong},
booktitle = {Proceeding of NIPS 2005},
doi = {10.1.1.123.395},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceeding of NIPS 2005/Singh, Barto, Chentanez/Singh, Barto, Chentanez - 2005 - Intrinsically motivated reinforcement learning.pdf:pdf},
pages = {1281--1288},
title = {{Intrinsically motivated reinforcement learning}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2005{\_}724.pdf},
year = {2005}
}
@inproceedings{Johnson2016,
abstract = {We present Project Malmo – an AI experimenta-tion platform built on top of the popular computer game Minecraft, and designed to support funda-mental research in artificial intelligence. As the AI research community pushes for artificial gen-eral intelligence (AGI), experimentation platforms are needed that support the development of flexible agents that learn to solve diverse tasks in complex environments. Minecraft is an ideal foundation for such a platform, as it exposes agents to complex 3D worlds, coupled with infinitely varied game-play. Project Malmo provides a sophisticated abstraction layer on top of Minecraft that supports a wide range of experimentation scenarios, ranging from navi-gation and survival to collaboration and problem solving tasks. In this demo we present the Malmo platform and its capabilities. The platform is pub-licly released as open source software at IJCAI, to support openness and collaboration in AI research.},
author = {Johnson, Matthew and Hofmann, Katja and Hutton, Tim and Bignell, David},
booktitle = {Proc. 25th International Joint Conference on Artificial Intelligence},
editor = {Kambhampati, S.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proc. 25th International Joint Conference on Artificial Intelligence/Johnson et al/Johnson et al. - 2016 - The Malmo Platform for Artificial Intelligence Experimentation.pdf:pdf},
issn = {10450823},
keywords = {Demonstrations},
pages = {4246--4247},
publisher = {AAAI Press},
title = {{The Malmo Platform for Artificial Intelligence Experimentation}},
year = {2016}
}
@article{Mania2018,
abstract = {A common belief in model-free reinforcement learning is that methods based on random search in the parameter space of policies exhibit significantly worse sample complexity than those that explore the space of actions. We dispel such beliefs by introducing a random search method for training static, linear policies for continuous control problems, matching state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a nearly optimal controller for a challenging instance of the Linear Quadratic Regulator, a classical problem in control theory, when the dynamics are not known. Computationally, our random search algorithm is at least 15 times more efficient than the fastest competing model-free methods on these benchmarks. We take advantage of this computational efficiency to evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. Our simulations highlight a high variability in performance in these benchmark tasks, suggesting that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms.},
archivePrefix = {arXiv},
arxivId = {1803.07055},
author = {Mania, Horia and Guy, Aurelia and Recht, Benjamin},
eprint = {1803.07055},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Mania, Guy, Recht/Mania, Guy, Recht - 2018 - Simple random search provides a competitive approach to reinforcement learning.pdf:pdf},
pages = {1--22},
title = {{Simple random search provides a competitive approach to reinforcement learning}},
url = {http://arxiv.org/abs/1803.07055},
year = {2018}
}
@article{Wang2012b,
abstract = {Reinforcement learning has been an important category of machine learning approaches exhibiting self-learning and online learning characteristics. Using reinforcement learning, an agent can learn its behaviors through trial-and-error interactions with a dynamic environment and finally come up with an optimal strategy. Reinforcement learning suffers the curse of dimensionality, though there has been significant progress to overcome this issue in recent years. MAXQ is one of the most common approaches for reinforcement learning. To function properly, MAXQ requires a decomposition of the agent's task into a task hierarchy. Previously, the decomposition can only be done manually. In this paper, we propose a mechanism for automatic subtask discovery. The mechanism applies clustering to automatically construct task hierarchy required by MAXQ, such that MAXQ can be fully automated. We present the design of our mechanism, and demonstrate its effectiveness through theoretical analysis and an extensive experimental evaluation. {\textcopyright} 2012 IEEE.},
author = {Wang, Hongbing and Li, Wenya and Zhou, Xuan},
doi = {10.1109/ICTAI.2012.165},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of International Conference on Tools with Artificial Intelligence, ICTAI/Wang, Li, Zhou/Wang, Li, Zhou - 2012 - Automatic discovery and transfer of MAXQ hierarchies in a complex system.pdf:pdf},
isbn = {9780769549156},
issn = {10823409},
journal = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
keywords = {Clustering,MAXQ,Reinforcement Learning,System of Systems},
pages = {1157--1162},
title = {{Automatic discovery and transfer of MAXQ hierarchies in a complex system}},
volume = {1},
year = {2012}
}
@article{Dabney2018,
archivePrefix = {arXiv},
arxivId = {1806.06923},
author = {Dabney, Will},
eprint = {1806.06923},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Icml/Dabney/Dabney - 2018 - Implicit Quantile Networks for Distributional Reinforcement Learning.pdf:pdf},
issn = {1938-7228},
journal = {Icml},
title = {{Implicit Quantile Networks for Distributional Reinforcement Learning}},
year = {2018}
}
@article{Silver2014,
abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic pol- icy gradient has a particularly appealing form: it is the expected gradient of the action-value func- tion. This simple form means that the deter- ministic policy gradient can be estimated much more efficiently than the usual stochastic pol- icy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counter- parts in high-dimensional action spaces.},
author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 31st International Conference on Machine Learning (ICML-14)/Silver et al/Silver et al. - 2014 - Deterministic Policy Gradient Algorithms.pdf:pdf},
isbn = {9781634393973},
issn = {1938-7228},
journal = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
pages = {387--395},
title = {{Deterministic Policy Gradient Algorithms}},
year = {2014}
}
@article{Fu2006,
abstract = {The authors propose a reinforcement-learning mechanism as a model for recurrent choice and extend it to account for skill learning. The model was inspired by recent research in neurophysiological studies of the basal ganglia and provides an integrated explanation of recurrent choice behavior and skill learning. The behavior includes effects of differential probabilities, magnitudes, variabilities, and delay of reinforcement. The model can also produce the violation of independence, preference reversals, and the goal gradient of reinforcement in maze learning. An experiment was conducted to study learning of action sequences in a multistep task. The fit of the model to the data demonstrated its ability to account for complex skill learning. The advantages of incorporating the mechanism into a larger cognitive architecture are discussed.},
author = {Fu, Wai-Tat and Anderson, John R},
doi = {10.1037/0096-3445.135.2.184},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Journal of experimental psychology. General/Fu, Anderson/Fu, Anderson - 2006 - From recurrent choice to skill learning a reinforcement-learning model.pdf:pdf},
isbn = {0096-3445$\backslash$n1939-2222},
issn = {0096-3445},
journal = {Journal of experimental psychology. General},
keywords = {Animals,Basal Ganglia,Basal Ganglia: physiology,Choice Behavior,Cognition,Cognition: physiology,Humans,Maze Learning,Models, Psychological,Probability,Rats,Reinforcement (Psychology),Reinforcement Schedule,Serial Learning},
number = {2},
pages = {184--206},
pmid = {16719650},
title = {{From recurrent choice to skill learning: a reinforcement-learning model.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16719650},
volume = {135},
year = {2006}
}
@article{Zhang2017,
author = {Zhang, Fengyun and Duan, Shukai and Wang, Lidan},
doi = {10.1007/s11571-017-9423-7},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Cognitive Neurodynamics/Zhang, Duan, Wang/Zhang, Duan, Wang - 2017 - Route searching based on neural networks and heuristic reinforcement learning.pdf:pdf},
issn = {1871-4080},
journal = {Cognitive Neurodynamics},
keywords = {Route searching,Neural network,Heuristic reinforce,exploitation,network {\'{a}} heuristic,reinforcement learning {\'{a}} greedy,route searching {\'{a}} neural},
publisher = {Springer Netherlands},
title = {{Route searching based on neural networks and heuristic reinforcement learning}},
url = {http://link.springer.com/10.1007/s11571-017-9423-7},
year = {2017}
}
@article{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q,-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
archivePrefix = {arXiv},
arxivId = {1412.3409},
author = {Watkins, Christopher J. C. H. and Dayan, Peter},
doi = {10.1023/A:1022676722315},
eprint = {1412.3409},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Machine Learning/Watkins, Dayan/Watkins, Dayan - 1992 - Q-Learning.pdf:pdf},
isbn = {978-1-4613-6608-9},
issn = {15730565},
journal = {Machine Learning},
keywords = {(Formula presented.)-learning,asynchronous dynamic programming,reinforcement learning,temporal differences},
number = {3},
pages = {279--292},
pmid = {7761831},
title = {{Q-Learning}},
volume = {8},
year = {1992}
}
@article{Qu2009,
author = {Qu, Hong and Yang, Simon X and Willms, Allan R and Yi, Zhang},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IEEE Transactions on Neural Networks/Qu et al/Qu et al. - 2009 - Real-time robot path planning based on a modified pulse-coupled neural network model.pdf:pdf},
isbn = {1045-9227},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
number = {11},
pages = {1724--1739},
title = {{Real-time robot path planning based on a modified pulse-coupled neural network model}},
volume = {20},
year = {2009}
}
@inproceedings{Mehta2008b,
abstract = {We present an algorithm, HI-MAT (Hierar- chy Induction via Models And Trajectories), that discovers MAXQ task hierarchies by ap- plying dynamic Bayesian network models to a successful trajectory from a source rein- forcement learning task. HI-MAT discovers subtasks by analyzing the causal and tem- poral relationships among the actions in the trajectory. Under appropriate assumptions, HI-MAT induces hierarchies that are consis- tent with the observed trajectory and have compact value-function tables employing safe state abstractions. We demonstrate empir- ically that HI-MAT constructs compact hi- erarchies that are comparable to manually- engineered hierarchies and facilitate signifi- cant speedup in learning when transferred to a target task.},
address = {New York, New York, USA},
author = {Mehta, Neville and Ray, Soumya and Tadepalli, Prasad and Dietterich, Thomas},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390238},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 25th international conference on Machine learning - ICML '08/Mehta et al/Mehta et al. - 2008 - Automatic discovery and transfer of MAXQ hierarchies.pdf:pdf},
isbn = {9781605582054},
pages = {648--655},
publisher = {ACM Press},
title = {{Automatic discovery and transfer of MAXQ hierarchies}},
url = {http://portal.acm.org/citation.cfm?id=1390238 http://portal.acm.org/citation.cfm?doid=1390156.1390238},
year = {2008}
}
@article{Rasmussen2017a,
abstract = {We present the first model capable of performing hierarchical reinforcement learning in a general, neurally detailed imple-mentation. We show that this model is able to learn a spatial pickup and delivery task more quickly than one without hier-archical abilities. In addition, we show that this model is able to leverage its hierarchical structure to transfer learned knowl-edge between related tasks. These results point towards the advantages to be gained by using a hierarchical RL framework to understand the brain's powerful learning ability.},
author = {Rasmussen, Daniel and Voelker, Aaron and Eliasmith, Chris},
doi = {10.1371/journal.pone.0180234},
editor = {Cymbalyuk, Gennady},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/PLOS ONE/Rasmussen, Voelker, Eliasmith/Rasmussen, Voelker, Eliasmith - 2017 - A neural model of hierarchical reinforcement learning(2).pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/PLOS ONE/Rasmussen, Voelker, Eliasmith/Rasmussen, Voelker, Eliasmith - 2017 - A neural model of hierarchical reinforcement learning.pdf:pdf},
isbn = {1111111111},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {brain could achieve its,hierarchical,hrl into a theory,neural engineering framework,neural model,of neural function,providing a hypothe-,reinforcement learning,sis for how the,strengths in scaling},
month = {jul},
number = {7},
pages = {e0180234},
title = {{A neural model of hierarchical reinforcement learning}},
url = {http://dx.plos.org/10.1371/journal.pone.0180234},
volume = {12},
year = {2017}
}
@article{Konidaris2016a,
abstract = {We describe a framework for building abstraction hierarchies whereby an agent alternates skill- and representation-acquisition phases to construct a sequence of increasingly abstract Markov decision processes. Our formulation builds on recent results showing that the appropriate abstract representation of a problem is specified by the agent's skills. We describe how such a hierarchy can be used for fast planning, and illustrate the construction of an appropriate hierarchy for the Taxi domain.},
archivePrefix = {arXiv},
arxivId = {1509.07582},
author = {Konidaris, George},
doi = {10.1016/j.surg.2015.03.052.Rate},
eprint = {1509.07582},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IJCAI International Joint Conference on Artificial Intelligence/Konidaris/Konidaris - 2016 - Constructing abstraction hierarchies using a skill-symbol loop.pdf:pdf},
isbn = {6176366127},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning},
pages = {1648--1654},
pmid = {28579718},
title = {{Constructing abstraction hierarchies using a skill-symbol loop}},
volume = {2016-Janua},
year = {2016}
}
@inproceedings{Harutyunyan2017,
abstract = {A temporally abstract action, or an option, is specified by a policy and a termination condition: the policy guides option behavior, and the termination condition roughly determines its length. Generally, learning with longer options (like learning with multi-step returns) is known to be more efficient. However, if the option set for the task is not ideal, and cannot express the primitive optimal policy exactly, shorter options offer more flexibility and can yield a better solution. Thus, the termination condition puts learning efficiency at odds with solution quality. We propose to resolve this dilemma by decoupling the behavior and target terminations, just like it is done with policies in off-policy learning. To this end, we give a new algorithm, Q($\backslash$beta), that learns the solution with respect to any termination condition, regardless of how the options actually terminate. We derive Q($\backslash$beta) by casting learning with options into a common framework with well-studied multi-step off-policy learning. We validate our algorithm empirically, and show that it holds up to its motivating claims.},
archivePrefix = {arXiv},
arxivId = {1711.03817},
author = {Harutyunyan, Anna and Vrancx, Peter and Bacon, Pierre-Luc and Precup, Doina and Nowe, Ann},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
eprint = {1711.03817},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Harutyunyan et al/Harutyunyan et al. - 2017 - Learning with Options that Terminate Off-Policy.pdf:pdf},
title = {{Learning with Options that Terminate Off-Policy}},
url = {http://arxiv.org/abs/1711.03817},
year = {2017}
}
@article{Waltz1965,
author = {Waltz, M and Fu, K},
doi = {10.1109/TAC.1965.1098193},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Automatic Control, IEEE Transactions on/Waltz, Fu/Waltz, Fu - 1965 - A heuristic approach to reinforcement learning control systems.pdf:pdf},
issn = {0018-9286},
journal = {Automatic Control, IEEE Transactions on},
number = {4},
pages = {390--398},
title = {{A heuristic approach to reinforcement learning control systems}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1098193},
volume = {AC-10},
year = {1965}
}
@article{Duan2016,
abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophis-ticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, bene-fiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a " fast " reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL 2 , the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose (" slow ") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including ob-servations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the " fast " RL algorithm on the current (previously unseen) MDP. We evaluate RL 2 experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL 2 is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL 2 on a vision-based navigation task and show that it scales up to high-dimensional problems.},
archivePrefix = {arXiv},
arxivId = {1611.02779},
author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter and Sutskever, Ilya and Abbeel, Pieter},
doi = {10.1051/0004-6361/201527329},
eprint = {1611.02779},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/arXiv/Duan et al/Duan et al. - 2016 - RL2 Fast Reinforcement Learning Via Slow Reinforcement Learning.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
journal = {arXiv},
pages = {1--14},
title = {{RL{\^{}}2: Fast Reinforcement Learning Via Slow Reinforcement Learning}},
year = {2016}
}
@article{Dosovitskiy2016,
abstract = {We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.},
archivePrefix = {arXiv},
arxivId = {1611.01779},
author = {Dosovitskiy, Alexey and Koltun, Vladlen},
eprint = {1611.01779},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Dosovitskiy, Koltun/Dosovitskiy, Koltun - 2016 - Learning to Act by Predicting the Future.pdf:pdf},
pages = {1--14},
title = {{Learning to Act by Predicting the Future}},
url = {http://arxiv.org/abs/1611.01779},
year = {2016}
}
@article{Parisotto2017,
abstract = {A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training.},
archivePrefix = {arXiv},
arxivId = {1702.08360},
author = {Parisotto, Emilio and Salakhutdinov, Ruslan},
eprint = {1702.08360},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Parisotto, Salakhutdinov/Parisotto, Salakhutdinov - 2017 - Neural Map Structured Memory for Deep Reinforcement Learning.pdf:pdf},
issn = {1702.08360},
pages = {1--13},
title = {{Neural Map: Structured Memory for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1702.08360},
year = {2017}
}
@article{Singh2010a,
abstract = {There is great interest in building intrinsic motivation into artificial systems using the reinforcement learning framework. Yet, what intrinsic motivation may mean computationally, and how it may differ from extrinsic motivation, remains a murky and controversial subject. In this paper, we adopt an evolutionary perspective and define a new optimal reward framework that captures the pressure to design good primary reward functions that lead to evolutionary success across environments. The results of two computational experiments show that optimal primary reward signals may yield both emergent intrinsic and extrinsic motivation. The evolutionary perspective and the associated optimal reward framework thus lead to the conclusion that there are no hard and fast features distinguishing intrinsic and extrinsic reward computationally. Rather, the directness of the relationship between rewarding behavior and evolutionary success varies along a continuum.},
author = {Singh, Satinder and Lewis, Richard L. and Barto, Andrew G. and Sorg, Jonathan},
doi = {10.1109/TAMD.2010.2051031},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IEEE Transactions on Autonomous Mental Development/Singh et al/Singh et al. - 2010 - Intrinsically Motivated Reinforcement Learning An Evolutionary Perspective.pdf:pdf},
isbn = {0262195348, 9780262195348},
issn = {1943-0604},
journal = {IEEE Transactions on Autonomous Mental Development},
number = {2},
pages = {70--82},
title = {{Intrinsically Motivated Reinforcement Learning: An Evolutionary Perspective}},
url = {http://ieeexplore.ieee.org/document/5471106/},
volume = {2},
year = {2010}
}
@inproceedings{Dickens2010,
author = {Dickens, Luke and Broda, Krysia and Russo, Alessandra},
booktitle = {ECAI 2010: 19th European Conference on Artificial Intelligence, 16-20 August 2010, Lisbon, Portugal : Including Prestigious Applications of Artificial Intelligence (PAIS-2010) : Proceedings},
doi = {10.3233/978-1-60750-606-5-367},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/ECAI 2010 19th European Conference on Artificial Intelligence, 16-20 August 2010, Lisbon, Portugal Including Prestigious Applications of Ar./Dickens, Broda, Russo/Dickens, Broda, Russo - 2010 - The Dynamics.pdf:pdf},
isbn = {9781607506065},
pages = {367--372},
title = {{The Dynamics of Multi-Agent Reinforcement Learning}},
year = {2010}
}
@article{Kaelbling1996,
abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
doi = {10.1613/jair.301},
eprint = {9605103},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Journal of Artificial Intelligence Research/Kaelbling, Littman, Moore/Kaelbling, Littman, Moore - 1996 - Reinforcement learning A survey.pdf:pdf},
isbn = {0-7803-3213-X},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {237--285},
pmid = {17255001},
primaryClass = {cs},
title = {{Reinforcement learning: A survey}},
volume = {4},
year = {1996}
}
@article{Liu2015,
abstract = {The K Shortest Paths (KSPs) problem with non-numerable applications has been researched widely, which aims to compute KSPs between two nodes in a non-decreasing order. However, less effort has been devoted to single-source KSP problem than to single-pair KSP computation, especially by using parallel methods. This paper proposes a Modified Continued Pulse Coupled Neural Network (MCPCNN) model to solve the two kinds of KSP problems. Theoretical analysis of MCPCNN and two algorithms for KSPs computation are presented. By using the parallel pulse transmission characteristic of pulse coupled neural networks, the method is able to find k shortest paths quickly. The computational complexity is only related to the length of the longest shortest path. Simulative results for route planning show that the proposed MCPCNN method for KSPs computation outperforms many other current efficient algorithms.},
author = {Liu, Guisong and Qiu, Zhao and Qu, Hong and Ji, Luping},
doi = {10.1016/j.neucom.2014.09.012},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Neurocomputing/Liu et al/Liu et al. - 2015 - Computing k shortest paths using modified pulse-coupled neural network.pdf:pdf},
isbn = {9780874216561},
issn = {18728286},
journal = {Neurocomputing},
keywords = {K Shortest paths,Pulse coupled neural network,Single-pair KSP,Single-source KSP},
number = {PC},
pages = {1162--1176},
publisher = {Elsevier},
title = {{Computing k shortest paths using modified pulse-coupled neural network}},
url = {http://dx.doi.org/10.1016/j.neucom.2014.09.012},
volume = {149},
year = {2015}
}
@article{Author2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1807.06763v1},
author = {Author, Anonymous and Address, Affiliation},
eprint = {arXiv:1807.06763v1},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Author, Address/Author, Address - 2018 - General Value Function Networks.pdf:pdf},
number = {Nips},
title = {{General Value Function Networks}},
year = {2018}
}
@article{Zoph2016,
abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
archivePrefix = {arXiv},
arxivId = {1611.01578},
author = {Zoph, Barret and Le, Quoc V.},
doi = {10.1016/j.knosys.2015.01.010},
eprint = {1611.01578},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Zoph, Le/Zoph, Le - 2016 - Neural Architecture Search with Reinforcement Learning.pdf:pdf},
isbn = {9781424425051},
issn = {1938-7228},
pages = {1--16},
title = {{Neural Architecture Search with Reinforcement Learning}},
url = {http://arxiv.org/abs/1611.01578},
year = {2016}
}
@article{Grover2018,
abstract = {Modeling agent behavior is central to understand-ing the emergence of complex phenomena in mul-tiagent systems. Prior work in agent modeling has largely been task-specific and driven by hand-engineering domain-specific prior knowledge. We propose a general learning framework for model-ing agent behavior in any multiagent system using only a handful of interaction data. Our framework casts agent modeling as a representation learning problem. Consequently, we construct a novel ob-jective inspired by imitation learning and agent identification and design an algorithm for unsu-pervised learning of representations of agent poli-cies. We demonstrate empirically the utility of the proposed framework in (i) a challenging high-dimensional competitive environment for continu-ous control and (ii) a cooperative environment for communication, on supervised predictive tasks, unsupervised clustering, and policy optimization using deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1806.06464},
author = {Grover, Aditya and Al-Shedivat, Maruan and Gupta, Jayesh K and Burda, Yura and Edwards, Harrison},
eprint = {1806.06464},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Grover et al/Grover et al. - 2018 - Learning Policy Representations in Multiagent Systems.pdf:pdf},
title = {{Learning Policy Representations in Multiagent Systems}},
url = {https://arxiv.org/pdf/1806.06464.pdf},
year = {2018}
}
@incollection{Castro2012a,
abstract = {Abstract. Temporally extended actions are usually effective in speeding up reinforcement learning. In this paper we present a mechanism for automatically constructing such actions, expressed as options [Sutton et al., 1999], in a finite Markov Decision Process (MDP). To do this, we compute a bisimulation metric [Ferns et al., 2004] between the states in a small MDP and the states in a large MDP, which we want to solve. The shape of this metric is then used to completely define a set of options for the large MDP. We demonstrate empirically that our approach is able to improve the speed of reinforcement learning, and is generally not sensitive to parameter tuning.},
author = {Castro, Pablo Samuel and Precup, Doina},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-29946-9_16},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Castro, Precup/Castro, Precup - 2012 - Automatic Construction of Temporally.pdf:pdf},
isbn = {9783642299452},
issn = {03029743},
pages = {140--152},
title = {{Automatic Construction of Temporally Extended Actions for MDPs Using Bisimulation Metrics}},
url = {http://link.springer.com/10.1007/978-3-642-29946-9{\_}16},
volume = {7188 LNAI},
year = {2012}
}
@article{Wang2016,
abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
archivePrefix = {arXiv},
arxivId = {1611.05763},
author = {Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
eprint = {1611.05763},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Wang et al/Wang et al. - 2016 - Learning to reinforcement learn.pdf:pdf},
month = {nov},
pages = {1--17},
title = {{Learning to reinforcement learn}},
url = {http://arxiv.org/abs/1611.05763},
year = {2016}
}
@article{Frans2017,
abstract = {We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives---policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.},
archivePrefix = {arXiv},
arxivId = {1710.09767},
author = {Frans, Kevin and Ho, Jonathan and Chen, Xi and Abbeel, Pieter and Schulman, John},
eprint = {1710.09767},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Frans et al/Frans et al. - 2017 - Meta Learning Shared Hierarchies.pdf:pdf},
pages = {1--11},
title = {{Meta Learning Shared Hierarchies}},
url = {http://arxiv.org/abs/1710.09767},
year = {2017}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Nature/Mnih et al/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {518},
year = {2015}
}
@article{Simsek2005a,
abstract = {We present a new subgoal-based method for automatically creating useful skills in reinforcement learning. Our method identifies subgoals by partitioning local state transition graphs—those that are constructed using only the most recent experiences of the agent. The local scope of our subgoal discovery method allows it to successfully identify the type of subgoals we seek—states that lie between two densely-connected regions of the state space—while producing an algorithm with low computational cost.},
author = {Şimşek, {\"{O}}zg{\"{u}}r and Wolfe, Alicia P. and Barto, Andrew G.},
doi = {10.1145/1102351.1102454},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 22nd international conference on Machine learning - ICML '05/Şimşek, Wolfe, Barto/Şimşek, Wolfe, Barto - 2005 - Identifying useful subgoals in reinforcement learning by local graph partitio.pdf:pdf},
isbn = {1595931805},
journal = {Proceedings of the 22nd international conference on Machine learning  - ICML '05},
pages = {816--823},
title = {{Identifying useful subgoals in reinforcement learning by local graph partitioning}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102454},
year = {2005}
}
@book{Sutton2011,
address = {М.},
author = {Саттон, Р.С. and Барто, Э. Г.},
edition = {2-е},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Саттон, Барто/Саттон, Барто - 2011 - Обучение с подкреплением.pdf:pdf},
language = {russian},
pages = {399},
publisher = {БИНОМ. Лаборатория знаний},
title = {{Обучение с подкреплением}},
year = {2011}
}
@inproceedings{Stulp2011,
abstract = {Temporal abstraction and task decomposition drastically reduce the search space for planning and control, and are fundamental to making complex tasks amenable to learning. In the context of reinforcement learning, temporal abstractions are studied within the paradigm of hierarchical reinforcement learning.},
author = {Stulp, Freek and Schaal, Stefan},
booktitle = {2011 11th IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/Humanoids.2011.6100841},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/2011 11th IEEE-RAS International Conference on Humanoid Robots/Stulp, Schaal/Stulp, Schaal - 2011 - Hierarchical reinforcement learning with movement primitives.pdf:pdf},
isbn = {978-1-61284-868-6},
issn = {21640572},
month = {oct},
pages = {231--238},
publisher = {IEEE},
title = {{Hierarchical reinforcement learning with movement primitives}},
url = {http://ieeexplore.ieee.org/document/6100841/},
year = {2011}
}
@book{Sutton2012,
address = {London},
author = {Sutton, Richard S. and Barto, Andrew G.},
edition = {2nd},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Sutton, Barto/Sutton, Barto - 2012 - Reinforcement learning An Introduction.pdf:pdf},
pages = {320},
publisher = {The MIT Press},
title = {{Reinforcement learning: An Introduction}},
url = {https://books.google.com/books?id=CAFR6IBF4xYC{\&}pgis=1{\%}5Cnhttp://incompleteideas.net/sutton/book/the-book.html{\%}5Cnhttps://www.dropbox.com/s/f4tnuhipchpkgoj/book2012.pdf},
year = {2012}
}
@article{Zheng2018,
abstract = {In many sequential decision making tasks, it is challenging to design reward functions that help an RL agent efficiently learn behavior that is considered good by the agent designer. A number of different formulations of the reward-design problem, or close variants thereof, have been proposed in the literature. In this paper we build on the Optimal Rewards Framework of Singh et.al. that defines the optimal intrinsic reward function as one that when used by an RL agent achieves behavior that optimizes the task-specifying or extrinsic reward function. Previous work in this framework has shown how good intrinsic reward functions can be learned for lookahead search based planning agents. Whether it is possible to learn intrinsic reward functions for learning agents remains an open problem. In this paper we derive a novel algorithm for learning intrinsic rewards for policy-gradient based learning agents. We compare the performance of an augmented agent that uses our algorithm to provide additive intrinsic rewards to an A2C-based policy learner (for Atari games) and a PPO-based policy learner (for Mujoco domains) with a baseline agent that uses the same policy learners but with only extrinsic rewards. Our results show improved performance on most but not all of the domains.},
archivePrefix = {arXiv},
arxivId = {1804.06459},
author = {Zheng, Zeyu and Oh, Junhyuk and Singh, Satinder},
eprint = {1804.06459},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Zheng, Oh, Singh/Zheng, Oh, Singh - 2018 - On Learning Intrinsic Rewards for Policy Gradient Methods.pdf:pdf},
title = {{On Learning Intrinsic Rewards for Policy Gradient Methods}},
url = {http://arxiv.org/abs/1804.06459},
year = {2018}
}
@article{Levine2016,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
archivePrefix = {arXiv},
arxivId = {1504.00702},
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1504.00702},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Journal of Machine Learning Research/Levine et al/Levine et al. - 2016 - End-to-End Training of Deep Visuomotor Policies.pdf:pdf},
isbn = {9781479969227},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Neural Networks,Optimal Control,Reinforcement Learning,Vision},
pages = {1--40},
pmid = {15003161},
title = {{End-to-End Training of Deep Visuomotor Policies}},
volume = {17},
year = {2016}
}
@misc{Brockam2016,
abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
archivePrefix = {arXiv},
arxivId = {1606.01540},
author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
booktitle = {ArXiv: 1606.01540},
eprint = {1606.01540},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/ArXiv 1606.01540/Brockman et al/Brockman et al. - 2016 - OpenAI Gym.pdf:pdf},
pages = {1--4},
title = {{OpenAI Gym}},
url = {http://arxiv.org/abs/1606.01540},
year = {2016}
}
@article{Teh2017,
abstract = {Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (Distill {\&} transfer learning). Instead of sharing parameters between the different workers, we propose to share a "distilled" policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust and more stable---attributes that are critical in deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1707.04175},
author = {Teh, Yee Whye and Bapst, Victor and Czarnecki, Wojciech Marian and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan},
eprint = {1707.04175},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Teh et al/Teh et al. - 2017 - Distral Robust Multitask Reinforcement Learning.pdf:pdf},
issn = {10495258},
title = {{Distral: Robust Multitask Reinforcement Learning}},
url = {http://arxiv.org/abs/1707.04175},
year = {2017}
}
@article{Xia2016,
abstract = {Designing intelligent and robust autonomous navigation systems remains a great challenge in mobile robotics. Inverse reinforcement learning (IRL) offers an efficient learning technique from expert demonstrations to teach robots how to perform specific tasks without manually specifying the reward function. Most of existing IRL algorithms assume the expert policy to be optimal and deterministic, and are applied to experiments with relatively small-size state spaces. However, in autonomous navigation tasks, the state spaces are frequently large and demonstrations can hardly visit all the states. Meanwhile the expert policy may be non-optimal and stochastic. In this paper, we focus on IRL with large-scale and high-dimensional state spaces by introducing the neural network to generalize the expert's behaviors to unvisited regions of the state space and an explicit policy representation is easily expressed by neural network, even for the stochastic expert policy. An efficient and convenient algorithm, Neural Inverse Reinforcement Learning (NIRL), is proposed. Experimental results on simulated autonomous navigation tasks show that a mobile robot using our approach can successfully navigate to the target position without colliding with unpredicted obstacles, largely reduce the learning time, and has a good generalization performance on undemonstrated states. Hence prove the robot intelligence of autonomous navigation transplanted from limited demonstrations to completely unknown tasks.},
author = {Xia, Chen and {El Kamel}, Abdelkader},
doi = {10.1016/j.robot.2016.06.003},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Robotics and Autonomous Systems/Xia, El Kamel/Xia, El Kamel - 2016 - Neural inverse reinforcement learning in autonomous navigation.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Autonomous navigation,Dynamic environments,Inverse reinforcement learning,Learning from demonstration,Markov decision processes,Neural network},
pages = {1--14},
publisher = {Elsevier B.V.},
title = {{Neural inverse reinforcement learning in autonomous navigation}},
url = {http://dx.doi.org/10.1016/j.robot.2016.06.003},
volume = {84},
year = {2016}
}
@incollection{Menache2002a,
abstract = {We present the Q-Cut algorithm, a graph theoretic approach for automatic detection of sub-goals in a dynamic environment, which is used for acceleration of the Q-Learning algorithm. The learning agent creates an on-line map of the process history, and uses an efficient Max-Flow/Min-Cut algorithm for identifying bottlenecks. The policies for reaching bottlenecks are separately learned and added to the model in a form of options (macro-actions). We then extend the basic Q-Cut algorithm to the Segmented Q-Cut algorithm, which uses previously identified bottlenecks for state space partitioning, necessary for finding additional bottlenecks in complex environments. Experiments show significant performance improvements, particulary in the initial learning phase.},
author = {Menache, Ishai and Mannor, Shie and Shimkin, Nahum},
booktitle = {ECML 2002: Machine Learning: ECML 2002},
doi = {10.1007/3-540-36755-1_25},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/ECML 2002 Machine Learning ECML 2002/Menache, Mannor, Shimkin/Menache, Mannor, Shimkin - 2002 - Q-Cut—Dynamic Discovery of Sub-goals in Reinforcement Learning.pdf:pdf},
isbn = {978-3-540-44036-9, 978-3-540-36755-0},
issn = {16113349},
pages = {295--306},
title = {{Q-Cut—Dynamic Discovery of Sub-goals in Reinforcement Learning}},
url = {http://link.springer.com/10.1007/3-540-36755-1{\_}25},
year = {2002}
}
@article{Paxton2017,
abstract = {We consider task and motion planning in complex dynamic environments for problems expressed in terms of a set of Linear Temporal Logic (LTL) constraints, and a reward function. We propose a methodology based on reinforcement learning that employs deep neural networks to learn low-level control policies as well as task-level option policies. A major challenge in this setting, both for neural network approaches and classical planning, is the need to explore future worlds of a complex and interactive environment. To this end, we integrate Monte Carlo Tree Search with hierarchical neural net control policies trained on expressive LTL specifications. This paper investigates the ability of neural networks to learn both LTL constraints and control policies in order to generate task plans in complex environments. We demonstrate our approach in a simulated autonomous driving setting, where a vehicle must drive down a road in traffic, avoid collisions, and navigate an intersection, all while obeying given rules of the road.},
archivePrefix = {arXiv},
arxivId = {1703.07887},
author = {Paxton, Chris and Raman, Vasumathi and Hager, Gregory D. and Kobilarov, Marin},
eprint = {1703.07887},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/ArXiv 1703.07887/Paxton et al/Paxton et al. - 2017 - Combining Neural Networks and Tree Search for Task and Motion Planning in Challenging Environments.pdf:pdf},
journal = {ArXiv: 1703.07887},
title = {{Combining Neural Networks and Tree Search for Task and Motion Planning in Challenging Environments}},
url = {http://arxiv.org/abs/1703.07887},
year = {2017}
}
@article{Sun2005,
abstract = {This article explicates the interaction between implicit and explicit processes in skill learning, in contrast to the tendency of researchers to study each type in isolation. It highlights various effects of the interaction on learning (including synergy effects). The authors argue for an integrated model of skill learning that takes into account both implicit and explicit processes. Moreover, they argue for a bottom-up approach (first learning implicit knowledge and then explicit knowledge) in the integrated model. A variety of qualitative data can be accounted for by the approach. A computational model, CLARION, is then used to simulate a range of quantitative data. The results demonstrate the plausibility of the model, which provides a new perspective on skill learning.},
author = {Sun, Ron and Slusarz, Paul and Terry, Chris},
doi = {10.1037/0033-295X.112.1.159},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Psychological Review/Sun, Slusarz, Terry/Sun, Slusarz, Terry - 2005 - The Interaction of the Explicit and the Implicit in Skill Learning A Dual-Process Approach.pdf:pdf},
isbn = {0033-295X},
issn = {1939-1471},
journal = {Psychological Review},
keywords = {artificial grammars,connectionist,decision-making,information,memory,model,nonconscious acquisition,procedural knowledge,tacit knowledge,task-performance},
number = {1},
pages = {159--192},
pmid = {15631592},
title = {{The Interaction of the Explicit and the Implicit in Skill Learning: A Dual-Process Approach.}},
volume = {112},
year = {2005}
}
@article{Li2013,
abstract = {Pulse Coupled Neural Network (PCNN) is suitable for dealing with the classical shortest path problem, because of its autowave characteristic. However, most methods suggest that the autowave of PCNN models should keep a constant speed in finding the shortest paths. This paper proposes a novel self-adaptive autowave pulse-coupled neural network (SAPCNN) model for the shortest path problem. The autowave generated by SAPCNN propagates adaptively according to the current network state, which guarantees it spreads more effectively in finding the shortest paths. Our experiments, which have been carried out for both the shortest paths problem and K shortest paths problem, show that our proposed algorithm outperforms classical algorithms. ?? 2013 Elsevier B.V.},
author = {Li, Xiaojun and Ma, Yide and Feng, Xiaowen},
doi = {10.1016/j.neucom.2012.12.030},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Neurocomputing/Li, Ma, Feng/Li, Ma, Feng - 2013 - Self-adaptive autowave pulse-coupled neural network for shortest-path problem.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Autowave,Pulse-coupled neural network,Self-adaptive autowave PCNN,Shortest path},
pages = {63--71},
publisher = {Elsevier},
title = {{Self-adaptive autowave pulse-coupled neural network for shortest-path problem}},
url = {http://dx.doi.org/10.1016/j.neucom.2012.12.030},
volume = {115},
year = {2013}
}
@phdthesis{Lin1993,
author = {Lin, Long-ji},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Lin/Lin - 1993 - Reinforcement Learning for Robots Using Neural Networks.pdf:pdf},
pages = {1--155},
school = {CMU},
title = {{Reinforcement Learning for Robots Using Neural Networks}},
year = {1993}
}
@book{Sun2001,
abstract = {This paper presents a skill learning model Clarion. Different from existing models of mostly high-level skill learning that use a top-down approach (that is, turning declarative knowledge into procedural knowledge through practice), we adopt a bottom-up approach toward low-level skill learning, where procedural knowledge develops first and declarative knowledge develops later. Our model is formed by integrating connectionist, reinforcement, and symbolic learning methods to perform on-line reactive learning. It adopts a two-level dual-representation framework (Sun, 1995), with a combination of localist and distributed representation. We compare the model with human data in a minefield navigation task, demonstrating some match between the model and human data in several respects. ?? 2001 Cognitive Science Society, Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sun, Ron and Merrill, Edward and Peterson, Todd},
booktitle = {Cognitive Science},
doi = {10.1016/S0364-0213(01)00035-0},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Cognitive Science/Sun, Merrill, Peterson/Sun, Merrill, Peterson - 2001 - From implicit skills to explicit knowledge A bottom-up model of skill learning.pdf:pdf},
isbn = {1573884766},
issn = {03640213},
number = {2},
pages = {203--244},
pmid = {15003161},
title = {{From implicit skills to explicit knowledge: A bottom-up model of skill learning}},
volume = {25},
year = {2001}
}
@article{Tesauro1995,
author = {Tesauro, G.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Communications of the ACM/Tesauro/Tesauro - 1995 - Temporal Difference Learning and TD-Gammon.pdf:pdf},
journal = {Communications of the ACM},
number = {5},
pages = {58--68},
title = {{Temporal Difference Learning and TD-Gammon}},
volume = {38},
year = {1995}
}
@article{Andrychowicz2018,
abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
archivePrefix = {arXiv},
arxivId = {1707.01495},
author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
eprint = {1707.01495},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Andrychowicz et al/Andrychowicz et al. - 2018 - Hindsight Experience Replay.pdf:pdf},
number = {Nips},
title = {{Hindsight Experience Replay}},
url = {http://arxiv.org/abs/1707.01495},
year = {2018}
}
@inproceedings{Goel2017a,
author = {Goel, Karan and Mu, Tong and Brunskill, Emma},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Goel, Mu, Brunskill/Goel, Mu, Brunskill - 2017 - Optimal Hierarchical Policy Extraction From Noisy Imperfect Demonstrations.pdf:pdf},
title = {{Optimal Hierarchical Policy Extraction From Noisy Imperfect Demonstrations}},
url = {https://drive.google.com/file/d/101FsZkczKMfGeUBTP-089mhkTeepj8IW/view},
year = {2017}
}
@article{Rabinowitz2018,
abstract = {Theory of mind (ToM; Premack {\&} Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test (Wimmer {\&} Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.},
archivePrefix = {arXiv},
arxivId = {1802.07740},
author = {Rabinowitz, Neil C. and Perbet, Frank and Song, H. Francis and Zhang, Chiyuan and Eslami, S. M. Ali and Botvinick, Matthew},
eprint = {1802.07740},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Rabinowitz et al/Rabinowitz et al. - 2018 - Machine Theory of Mind.pdf:pdf},
issn = {1938-7228},
title = {{Machine Theory of Mind}},
url = {http://arxiv.org/abs/1802.07740},
year = {2018}
}
@article{Bargiacchi2018,
abstract = {Learning to coordinate between multiple agents is an important problem in many reinforcement learning problems. Key to learning to coordi-nate is exploiting loose couplings, i.e., condi-tional independences between agents. In this paper we study learning in repeated fully coop-erative games, multi-agent multi-armed bandits (MAMABs), in which the expected rewards can be expressed as a coordination graph. We pro-pose multi-agent upper confidence exploration (MAUCE), a new algorithm for MAMABs that ex-ploits loose couplings, which enables us to prove a regret bound that is logarithmic in the number of arm pulls and only linear in the number of agents. We empirically compare MAUCE to sparse coop-erative Q-learning, and a state-of-the-art combina-torial bandit approach, and show that it performs much better on a variety of settings, including learning control policies for wind farms.},
author = {Bargiacchi, Eugenio and Verstraeten, Timothy and Roijers, Diederik M and Now{\'{e}}, Ann and {Van Hasselt}, Hado},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Bargiacchi et al/Bargiacchi et al. - 2018 - Learning to Coordinate with Coordination Graphs in Repeated Single-Stage Multi-Agent Decision Problems.pdf:pdf},
title = {{Learning to Coordinate with Coordination Graphs in Repeated Single-Stage Multi-Agent Decision Problems}},
url = {http://proceedings.mlr.press/v80/bargiacchi18a/bargiacchi18a.pdf},
year = {2018}
}
@article{Schulman2015,
abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1502.05477},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
doi = {10.1063/1.4927398},
eprint = {1502.05477},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Schulman et al/Schulman et al. - 2015 - Trust Region Policy Optimization.pdf:pdf},
isbn = {0375-9687},
issn = {2158-3226},
title = {{Trust Region Policy Optimization}},
url = {http://arxiv.org/abs/1502.05477},
year = {2015}
}
@article{Nachum2018,
abstract = {State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value. We show that such smoothed Q-values still satisfy a Bellman equation, making them learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships, we develop new algorithms for training a Gaussian policy directly from a learned smoothed Q-value approximator. The approach is additionally amenable to proximal optimization by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training leads to significantly improved results on standard continuous control benchmarks.},
archivePrefix = {arXiv},
arxivId = {1803.02348},
author = {Nachum, Ofir and Norouzi, Mohammad and Tucker, George and Schuurmans, Dale},
eprint = {1803.02348},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Nachum et al/Nachum et al. - 2018 - Smoothed Action Value Functions for Learning Gaussian Policies.pdf:pdf},
title = {{Smoothed Action Value Functions for Learning Gaussian Policies}},
url = {http://arxiv.org/abs/1803.02348},
year = {2018}
}
@article{Srouji2018,
abstract = {In recent years, Deep Reinforcement Learning has made impressive advances in solving several important benchmark problems for sequential decision making. Many control applications use a generic multilayer perceptron (MLP) for non-vision parts of the policy network. In this work, we propose a new neural network architecture for the policy network representation that is simple yet effective. The proposed Structured Control Net (SCN) splits the generic MLP into two separate sub-modules: a nonlinear control module and a linear control module. Intuitively, the nonlinear control is for forward-looking and global control, while the linear control stabilizes the local dynamics around the residual of global control. We hypothesize that this will bring together the benefits of both linear and nonlinear policies: improve training sample efficiency, final episodic reward, and generalization of learned policy, while requiring a smaller network and being generally applicable to different training methods. We validated our hypothesis with competitive results on simulations from OpenAI MuJoCo, Roboschool, Atari, and a custom 2D urban driving environment, with various ablation and generalization tests, trained with multiple black-box and policy gradient training methods. The proposed architecture has the potential to improve upon broader control tasks by incorporating problem specific priors into the architecture. As a case study, we demonstrate much improved performance for locomotion tasks by emulating the biological central pattern generators (CPGs) as the nonlinear part of the architecture.},
archivePrefix = {arXiv},
arxivId = {1802.08311},
author = {Srouji, Mario and Zhang, Jian and Salakhutdinov, Ruslan},
eprint = {1802.08311},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Srouji, Zhang, Salakhutdinov/Srouji, Zhang, Salakhutdinov - 2018 - Structured Control Nets for Deep Reinforcement Learning.pdf:pdf},
title = {{Structured Control Nets for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1802.08311},
year = {2018}
}
@inproceedings{Mannor2004a,
abstract = {We consider a graph theoretic approach for automatic construction of options in a dynamic environment. A map of the environment is generated on-line by the learning agent, representing the topological structure of the state transitions. A clustering algorithm is then used to partition the state space to different regions. Policies for reaching the different parts of the space are separately learned and added to the model in a form of options (macro-actions). The options are used for accelerating the Q-Learning algorithm. We extend the basic algorithm and consider building a map that includes preliminary indication of the location of "interesting" regions of the state space, where the value gradient is significant and additional exploration might be beneficial. Experiments indicate significant speedups, especially in the initial learning phase.},
address = {New York, New York, USA},
author = {Mannor, Shie and Menache, Ishai and Hoze, Amit and Klein, Uri},
booktitle = {Twenty-first international conference on Machine learning - ICML '04},
doi = {10.1145/1015330.1015355},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Twenty-first international conference on Machine learning - ICML '04/Mannor et al/Mannor et al. - 2004 - Dynamic abstraction in reinforcement learning via clustering.pdf:pdf},
isbn = {1581138285},
keywords = {clustering,hierarchical reinforcement learning,options,q-learning,reinforcement learning},
pages = {71},
publisher = {ACM Press},
title = {{Dynamic abstraction in reinforcement learning via clustering}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015355},
year = {2004}
}
@article{Melo1997,
abstract = {We address the problem of computing the optimal Q-function in Markov decision problems with infinite state-space. We analyze the convergence properties of several variations of Q-learning when combined with function approximation, extending the analysis of TD-learning in (Tsitsiklis {\&} Van Roy, 1996a) to stochastic control settings. We identify conditions under which such approximate methods converge with probability 1. We conclude with a brief discussion on the general applicability of our results and compare them with several related works.},
author = {Melo, F S and Meyn, S P and Ribeiro, M I},
doi = {10.1109/9.580874},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IEEE Transactions on Automatic Control/Melo, Meyn, Ribeiro/Melo, Meyn, Ribeiro - 1997 - An analysis of reinforcement learning with function approximation.pdf:pdf},
isbn = {9781605582054},
issn = {00189286},
journal = {IEEE Transactions on Automatic Control},
keywords = {planning reinforcement learning Q-learning tempora},
number = {5},
pages = {674--690},
title = {{An analysis of reinforcement learning with function approximation}},
volume = {42},
year = {1997}
}
@inproceedings{Yahya2017,
abstract = {In principle, reinforcement learning and policy search methods can enable robots to learn highly complex and general skills that may allow them to function amid the complexity and diversity of the real world. However, training a policy that generalizes well across a wide range of real-world conditions requires far greater quantity and diversity of experience than is practical to collect with a single robot. Fortunately, it is possible for multiple robots to share their experience with one another, and thereby, learn a policy collectively. In this work, we explore distributed and asynchronous policy learning as a means to achieve generalization and improved training times on challenging, real-world manipulation tasks. We propose a distributed and asynchronous version of Guided Policy Search and use it to demonstrate collective policy learning on a vision-based door opening task using four robots. We show that it achieves better generalization, utilization, and training times than the single robot alternative.},
archivePrefix = {arXiv},
arxivId = {1610.00673},
author = {Yahya, Ali and Li, Adrian and Kalakrishnan, Mrinal and Chebotar, Yevgen and Levine, Sergey},
booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
eprint = {1610.00673},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/2017 IEEERSJ International Conference on Intelligent Robots and Systems (IROS)/Yahya et al/Yahya et al. - 2017 - Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search.pdf:pdf},
isbn = {9781538626818},
keywords = {Deep Learning in Rob,Learning and Adaptive Systems},
pages = {79--86},
publisher = {IEEE},
title = {{Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search}},
url = {http://arxiv.org/abs/1610.00673},
year = {2017}
}
@article{Li2017,
abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We start with background of deep learning and reinforcement learning, as well as introduction of testbeds. Next we discuss Deep Q-Network (DQN) and its extensions, asynchronous methods, policy optimization, reward, and planning. After that, we talk about attention and memory, unsupervised learning, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, spoken dialogue systems (a.k.a. chatbot), machine translation, text sequence prediction, neural architecture design, personalized web services, healthcare, finance, and music generation. We mention topics/papers not reviewed yet. After listing a collection of RL resources, we close with discussions.},
archivePrefix = {arXiv},
arxivId = {1701.07274},
author = {Li, Yuxi},
eprint = {1701.07274},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/ArXiv 1701.07274/Li/Li - 2017 - Deep Reinforcement Learning An Overview.pdf:pdf},
journal = {ArXiv: 1701.07274},
pages = {1--30},
title = {{Deep Reinforcement Learning: An Overview}},
url = {http://arxiv.org/abs/1701.07274},
year = {2017}
}
@article{Russell2018,
abstract = {In recent years, deep generative models have been shown to ‘imagine' convincing high-dimensional observations such as images, audio, and even video, learning directly from raw data. In this work, we ask how to imagine goal-directed visual plans – a plausible sequence of observations that transition a dynamical system from its current configuration to a desired goal state, which can later be used as a reference trajectory for control. We focus on systems with high-dimensional observations, such as images, and propose an approach that naturally combines representation learning and planning. Our framework learns a generative model of sequential observations, where the generative process is induced by a transition in a low-dimensional planning model, and an additional noise. By maximizing the mutual information between the generated observations and the transition in the planning model, we obtain a low-dimensional representation that best explains the causal nature of the data. We structure the planning model to be compatible with efficient planning algorithms, and we propose several such models based on either discrete or continuous states. Finally, to generate a visual plan, we project the current and goal observations onto their respective states in the planning model, plan a trajectory, and then use the generative model to transform the trajectory to a sequence of observations. We demonstrate our method on imagining plausible visual plans of rope manipulation.},
author = {Russell, Stuart and Tamar, Aviv and Berkeley, U C and Abbeel, Pieter},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Russell et al/Russell et al. - 2018 - Learning Plannable Representations with Causal InfoGAN.pdf:pdf},
title = {{Learning Plannable Representations with Causal InfoGAN}},
year = {2018}
}
@article{Konidaris2012a,
abstract = {We present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to significantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that significantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-specific skills.},
author = {Konidaris, George and Scheidwasser, Ilya and Barto, Andrew G.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Journal of Machine Learning Research/Konidaris, Scheidwasser, Barto/Konidaris, Scheidwasser, Barto - 2012 - Transfer in reinforcement learning via shared features.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {reinforcement learning,shaping,skills,transfer},
number = {1},
pages = {1333--1371},
title = {{Transfer in reinforcement learning via shared features}},
url = {http://dl.acm.org/citation.cfm?id=2503308.2343689{\%}5Cnhttp://dl.acm.org/ft{\_}gateway.cfm?id=2343689{\&}type=pdf},
volume = {13},
year = {2012}
}
@incollection{Ponsen2010,
abstract = {In this paper we survey the basics of reinforcement learning, gener- alization and abstraction. We start with an introduction to the fundamentals of reinforcement learning and motivate the necessity for generalization and abstrac- tion. Next we summarize themost important techniques available to achieve both generalization and abstraction in reinforcement learning. We discuss basic func- tion approximation techniques and delve into hierarchical, relational and transfer learning. All concepts and techniques are illustrated with examples.},
author = {Ponsen, Marc and Taylor, Matthew E. and Tuyls, Karl},
booktitle = {Adaptive and Learning Agents},
doi = {10.1007/978-3-642-11814-2_1},
editor = {Taylor, Matthew E. and Tuyls, Karl},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Adaptive and Learning Agents/Ponsen, Taylor, Tuyls/Ponsen, Taylor, Tuyls - 2010 - Abstraction and generalization in reinforcement learning A summary and framework.pdf:pdf},
isbn = {3642118135},
issn = {03029743},
pages = {1--32},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Abstraction and generalization in reinforcement learning: A summary and framework}},
year = {2010}
}
@article{Yang2004,
author = {Yang, S.X. and Luo, Chaomin},
doi = {10.1109/TSMCB.2003.811769},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics)/Yang, Luo/Yang, Luo - 2004 - A Neural Network Approach to Complete Coverage Path Planning.pdf:pdf},
issn = {1083-4419},
journal = {IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics)},
month = {feb},
number = {1},
pages = {718--724},
title = {{A Neural Network Approach to Complete Coverage Path Planning}},
url = {http://ieeexplore.ieee.org/document/1262545/},
volume = {34},
year = {2004}
}
@article{Raileanu2018,
abstract = {We consider the multi-agent reinforcement learning setting with imperfect information in which each agent is trying to maximize its own utility. The reward function depends on the hidden state (or goal) of both agents, so the agents must infer the other players' hidden goals from their observed behavior in order to solve the tasks. We propose a new approach for learning in these domains: Self Other-Modeling (SOM), in which an agent uses its own policy to predict the other agent's actions and update its belief of their hidden state in an online manner. We evaluate this approach on three different tasks and show that the agents are able to learn better policies using their estimate of the other players' hidden states, in both cooperative and adversarial settings.},
archivePrefix = {arXiv},
arxivId = {1802.09640},
author = {Raileanu, Roberta and Denton, Emily and Szlam, Arthur and Fergus, Rob},
eprint = {1802.09640},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Raileanu et al/Raileanu et al. - 2018 - Modeling Others using Oneself in Multi-Agent Reinforcement Learning.pdf:pdf},
title = {{Modeling Others using Oneself in Multi-Agent Reinforcement Learning}},
url = {http://arxiv.org/abs/1802.09640},
year = {2018}
}
@article{For2018,
author = {For, Ethod},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/For/For - 2018 - an Inference-Based Policy Gradient Method for Learning Options.pdf:pdf},
pages = {1--10},
title = {{an Inference-Based Policy Gradient Method for Learning Options}},
year = {2018}
}
@article{Oh2017,
abstract = {As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions.},
archivePrefix = {arXiv},
arxivId = {1706.05064},
author = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak and Kohli, Pushmeet},
eprint = {1706.05064},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Oh et al/Oh et al. - 2017 - Multi-task Deep Reinforcement Learning for Zero-shot Generalization with Hierarchical Task Dependencies.pdf:pdf},
title = {{Multi-task Deep Reinforcement Learning for Zero-shot Generalization with Hierarchical Task Dependencies}},
url = {http://arxiv.org/abs/1706.05064},
year = {2017}
}
@article{Li2017a,
abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.},
archivePrefix = {arXiv},
arxivId = {1701.07274},
author = {Li, Yuxi},
doi = {10.1007/978-3-319-56991-8_32},
eprint = {1701.07274},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Li/Li - 2017 - Deep Reinforcement Learning An Overview.pdf:pdf},
isbn = {9781509011216},
issn = {1701.07274},
pages = {70},
title = {{Deep Reinforcement Learning: An Overview}},
url = {http://arxiv.org/abs/1701.07274},
year = {2017}
}
@article{BouAmmar,
abstract = {Online multi-task learning is an important capa-bility for lifelong learning agents, enabling them to acquire models for diverse tasks over time and rapidly learn new tasks by building upon prior ex-perience. However, recent progress toward lifelong reinforcement learning (RL) has been limited to learning from within a single task domain. For truly versatile lifelong learning, the agent must be able to autonomously transfer knowledge between differ-ent task domains. A few methods for cross-domain transfer have been developed, but these methods are computationally inefficient for scenarios where the agent must learn tasks consecutively. In this paper, we develop the first cross-domain life-long RL framework. Our approach efficiently op-timizes a shared repository of transferable knowl-edge and learns projection matrices that specialize that knowledge to different task domains. We pro-vide rigorous theoretical guarantees on the stability of this approach, and empirically evaluate its per-formance on diverse dynamical systems. Our re-sults show that the proposed method can learn ef-fectively from interleaved task domains and rapidly acquire high performance in new domains.},
author = {{Bou Ammar}, Haitham and Eaton, Eric and Luna, Jos{\'{e}} Marcio and Ruvolo, Paul},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Bou Ammar et al/Bou Ammar et al. - Unknown - Autonomous Cross-Domain Knowledge Transfer in Lifelong Policy Gradient Reinforcement Learning.pdf:pdf},
title = {{Autonomous Cross-Domain Knowledge Transfer in Lifelong Policy Gradient Reinforcement Learning}},
url = {https://scholar.princeton.edu/sites/default/files/bouammar/files/bouammar2015autonomous.pdf}
}
@article{ODoherty2015,
abstract = {Here we review recent developments in the application of reinforcement-learning theory as a means of understanding how the brain learns to select actions to maximize future reward, with a focus on human neuroimaging studies. We evaluate evidence for the distinction between model-based and model-free reinforcement-learning and their arbitration, and consider hierarchical reinforcement-learning schemes and structure learning. Finally we discuss the possibility of integrating across these different domains as a means of gaining a more complete understanding of how it is the brain learns from reinforcement.},
author = {O'Doherty, John P and Lee, Sang Wan and McNamee, Daniel},
doi = {10.1016/j.cobeha.2014.10.004},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Current Opinion in Behavioral Sciences/O'Doherty, Lee, McNamee/O'Doherty, Lee, McNamee - 2015 - The structure of reinforcement-learning mechanisms in the human brain.pdf:pdf},
isbn = {2352-1546},
issn = {23521546},
journal = {Current Opinion in Behavioral Sciences},
month = {feb},
pages = {94--100},
pmid = {1000104749},
publisher = {Elsevier Ltd},
title = {{The structure of reinforcement-learning mechanisms in the human brain}},
url = {http://dx.doi.org/10.1016/j.cobeha.2014.10.004 http://linkinghub.elsevier.com/retrieve/pii/S2352154614000242},
volume = {1},
year = {2015}
}
@article{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task involving finding rewards in random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/arXiv/Mnih et al/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf:pdf},
isbn = {9781510829008},
journal = {arXiv},
pages = {1--28},
pmid = {1000272564},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}
