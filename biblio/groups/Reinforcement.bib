Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Mannor2004a,
abstract = {We consider a graph theoretic approach for automatic construction of options in a dynamic environment. A map of the environment is generated on-line by the learning agent, representing the topological structure of the state transitions. A clustering algorithm is then used to partition the state space to different regions. Policies for reaching the different parts of the space are separately learned and added to the model in a form of options (macro-actions). The options are used for accelerating the Q-Learning algorithm. We extend the basic algorithm and consider building a map that includes preliminary indication of the location of "interesting" regions of the state space, where the value gradient is significant and additional exploration might be beneficial. Experiments indicate significant speedups, especially in the initial learning phase.},
address = {New York, New York, USA},
author = {Mannor, Shie and Menache, Ishai and Hoze, Amit and Klein, Uri},
booktitle = {Twenty-first international conference on Machine learning - ICML '04},
doi = {10.1145/1015330.1015355},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Mannor et al/2004/Dynamic abstraction in reinforcement learning via clustering.pdf:pdf},
isbn = {1581138285},
keywords = {clustering,hierarchical reinforcement learning,options,q-learning,reinforcement learning},
pages = {71},
publisher = {ACM Press},
title = {{Dynamic abstraction in reinforcement learning via clustering}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015355},
year = {2004}
}
@inproceedings{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
booktitle = {arXiv: 1312.5602},
eprint = {1312.5602},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Mnih et al/2013/Playing Atari with Deep Reinforcement Learning.pdf:pdf},
pages = {1--9},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@inproceedings{Goel2017a,
author = {Goel, Karan and Mu, Tong and Brunskill, Emma},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Goel, Mu, Brunskill/2017/Optimal Hierarchical Policy Extraction From Noisy Imperfect Demonstrations.pdf:pdf},
title = {{Optimal Hierarchical Policy Extraction From Noisy Imperfect Demonstrations}},
url = {https://drive.google.com/file/d/101FsZkczKMfGeUBTP-089mhkTeepj8IW/view},
year = {2017}
}
@incollection{Riedmiller2005,
abstract = {This paper introduces NFQ, an algorithm for efficient and ef- fective training of a Q-value function represented by amulti-layer percep- tron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.},
author = {Riedmiller, Martin},
booktitle = {Machine Learning: ECML 2005},
doi = {10.1007/11564096_32},
editor = {Gama, Jo{\~{a}}o and Camacho, Rui and Brazdil, Pavel B. and Jorge, Al{\'{i}}pio M{\'{a}}rio and Torgo, Lu{\'{i}}s},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Riedmiller/2005/Neural Fitted Q Iteration – First Experiences with a Data Efficient Neural Reinforcement Learning Method.pdf:pdf},
isbn = {3540292438},
issn = {03029743},
pages = {317--328},
publisher = {Springer Berlin Heidelberg},
title = {{Neural Fitted Q Iteration – First Experiences with a Data Efficient Neural Reinforcement Learning Method}},
url = {http://link.springer.com/10.1007/11564096{\_}32},
year = {2005}
}
@article{Hayes2016b,
abstract = {Collaboration between humans and robots requires solutions to an array of challenging problems, including multi-agent planning, state estimation, and goal inference. There already exist feasible solutions for many of these challenges, but they depend upon having rich task models. In this work we detail a novel type of Hierarchical Task Network we call a Clique/Chain HTN (CC-HTN), alongside an algorithm for autonomously constructing them from topological properties derived from graphical task representations. As the presented method relies on the structure of the task itself, our work imposes no particular type of symbolic insight into motor primitives or environmental representation, making it applicable to a wide variety of use cases critical to human-robot interaction. We present evaluations within a multi-resolution goal inference task and a transfer learning application showing the utility of our approach.},
author = {Hayes, Bradley and Scassellati, Brian},
doi = {10.1109/ICRA.2016.7487760},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Hayes, Scassellati/2016/Autonomously constructing hierarchical task networks for planning and human-robot collaboration.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {5469--5476},
title = {{Autonomously constructing hierarchical task networks for planning and human-robot collaboration}},
volume = {2016-June},
year = {2016}
}
@article{Al-Emran2015,
author = {Al-Emran, Mostafa},
doi = {10.12785/ijcds/040207},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Al-Emran/2015/Hierarchical Reinforcement Learning - A Survey.pdf:pdf},
issn = {2210-142X},
journal = {International Journal of Computing and Digital Systems},
keywords = {hierarchical reinforcement learning,q-learning,reinforcement learning},
month = {apr},
number = {2},
pages = {137--143},
title = {{Hierarchical Reinforcement Learning - A Survey}},
url = {https://journal.journals.uob.edu.bh//Article/ArticleFile/2144},
volume = {4},
year = {2015}
}
@article{Wang2016,
abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
archivePrefix = {arXiv},
arxivId = {1611.05763},
author = {Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
eprint = {1611.05763},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Wang et al/2016/Learning to reinforcement learn.pdf:pdf},
month = {nov},
pages = {1--17},
title = {{Learning to reinforcement learn}},
url = {http://arxiv.org/abs/1611.05763},
year = {2016}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Mnih et al/2015/Human-level control through deep reinforcement learning.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {518},
year = {2015}
}
@article{Konidaris2016a,
abstract = {We describe a framework for building abstraction hierarchies whereby an agent alternates skill- and representation-acquisition phases to construct a sequence of increasingly abstract Markov decision processes. Our formulation builds on recent results showing that the appropriate abstract representation of a problem is specified by the agent's skills. We describe how such a hierarchy can be used for fast planning, and illustrate the construction of an appropriate hierarchy for the Taxi domain.},
archivePrefix = {arXiv},
arxivId = {1509.07582},
author = {Konidaris, George},
doi = {10.1016/j.surg.2015.03.052.Rate},
eprint = {1509.07582},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Konidaris/2016/Constructing abstraction hierarchies using a skill-symbol loop.pdf:pdf},
isbn = {6176366127},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning},
pages = {1648--1654},
pmid = {28579718},
title = {{Constructing abstraction hierarchies using a skill-symbol loop}},
volume = {2016-Janua},
year = {2016}
}
@article{Sutton1999,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Sutton, Precup, Singh/1999/Between MDPs and Semi-MDPs A Framework for Temporal Abstraction in Reinforcement Learning.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Artificial Intelligence},
pages = {181--211},
pmid = {25246403},
title = {{Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning}},
volume = {112},
year = {1999}
}
@article{Daniel2016a,
abstract = {Tasks that require many sequential decisions are hard to solve using conventional rein- forcement learning algorithms. Based on the option framework, we propose a model which aims to alleviate these concerns. Instead of learning a single monolithic policy, the agent learns a set of simpler sub-policies as well as the initiation and termination probabili- ties for each of those sub-policies. While ex- isting option learning algorithms frequently require manual specification of components such as the sub-policies, we present an algo- rithm which infers all relevant components of the option framework from data. We present results on SMDPs with discrete as well as continuous state-action spaces. The results show that the presented algorithm can combine simple sub-policies to solve complex tasks and can improve learning performance on simpler tasks.},
author = {Daniel, Christian and van Hoof, Herke and Peters, Jan and Neumann, Gerhard},
doi = {10.1007/s10994-016-5580-x},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Daniel et al/2016/Probabilistic inference for determining options in reinforcement learning.pdf:pdf},
issn = {15730565},
journal = {Machine Learning},
keywords = {Options,Reinforcement learning,Robot learning,Semi Markov decision process},
number = {2-3},
pages = {337--357},
publisher = {Springer US},
title = {{Probabilistic inference for determining options in reinforcement learning}},
volume = {104},
year = {2016}
}
@inproceedings{Singh2005a,
author = {Singh, Satinder and Barto, Andrew G. and Chentanez, Nuttapong},
booktitle = {Proceeding of NIPS 2005},
doi = {10.1.1.123.395},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Singh, Barto, Chentanez/2005/Intrinsically motivated reinforcement learning.pdf:pdf},
pages = {1281--1288},
title = {{Intrinsically motivated reinforcement learning}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2005{\_}724.pdf},
year = {2005}
}
@article{Arulkumaran2017,
abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep {\$}Q{\$}-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
archivePrefix = {arXiv},
arxivId = {1708.05866},
author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
doi = {10.1109/MSP.2017.2743240},
eprint = {1708.05866},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Arulkumaran et al/2017/Deep Reinforcement Learning A Brief Survey.pdf:pdf},
isbn = {9781424469178},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
month = {nov},
number = {6},
pages = {26--38},
pmid = {25719670},
title = {{Deep Reinforcement Learning: A Brief Survey}},
url = {http://arxiv.org/abs/1708.05866{\%}0Ahttp://dx.doi.org/10.1109/MSP.2017.2743240 http://ieeexplore.ieee.org/document/8103164/},
volume = {34},
year = {2017}
}
@article{Fu2006,
abstract = {The authors propose a reinforcement-learning mechanism as a model for recurrent choice and extend it to account for skill learning. The model was inspired by recent research in neurophysiological studies of the basal ganglia and provides an integrated explanation of recurrent choice behavior and skill learning. The behavior includes effects of differential probabilities, magnitudes, variabilities, and delay of reinforcement. The model can also produce the violation of independence, preference reversals, and the goal gradient of reinforcement in maze learning. An experiment was conducted to study learning of action sequences in a multistep task. The fit of the model to the data demonstrated its ability to account for complex skill learning. The advantages of incorporating the mechanism into a larger cognitive architecture are discussed.},
author = {Fu, Wai-Tat and Anderson, John R},
doi = {10.1037/0096-3445.135.2.184},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Fu, Anderson/2006/From recurrent choice to skill learning a reinforcement-learning model.pdf:pdf},
isbn = {0096-3445$\backslash$n1939-2222},
issn = {0096-3445},
journal = {Journal of experimental psychology. General},
keywords = {Animals,Basal Ganglia,Basal Ganglia: physiology,Choice Behavior,Cognition,Cognition: physiology,Humans,Maze Learning,Models, Psychological,Probability,Rats,Reinforcement (Psychology),Reinforcement Schedule,Serial Learning},
number = {2},
pages = {184--206},
pmid = {16719650},
title = {{From recurrent choice to skill learning: a reinforcement-learning model.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16719650},
volume = {135},
year = {2006}
}
@unpublished{Sutton2016,
author = {Sutton, Richard S and Barto, Andrew G},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Sutton, Barto/2017/Reinforcement learning An introduction.pdf:pdf;:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Sutton, Barto/2017/Reinforcement learning An introduction(2).pdf:pdf},
pages = {445},
title = {{Reinforcement learning: An introduction}},
year = {2017}
}
@article{ODoherty2015,
abstract = {Here we review recent developments in the application of reinforcement-learning theory as a means of understanding how the brain learns to select actions to maximize future reward, with a focus on human neuroimaging studies. We evaluate evidence for the distinction between model-based and model-free reinforcement-learning and their arbitration, and consider hierarchical reinforcement-learning schemes and structure learning. Finally we discuss the possibility of integrating across these different domains as a means of gaining a more complete understanding of how it is the brain learns from reinforcement.},
author = {O'Doherty, John P and Lee, Sang Wan and McNamee, Daniel},
doi = {10.1016/j.cobeha.2014.10.004},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/O'Doherty, Lee, McNamee/2015/The structure of reinforcement-learning mechanisms in the human brain.pdf:pdf},
isbn = {2352-1546},
issn = {23521546},
journal = {Current Opinion in Behavioral Sciences},
month = {feb},
pages = {94--100},
pmid = {1000104749},
publisher = {Elsevier Ltd},
title = {{The structure of reinforcement-learning mechanisms in the human brain}},
url = {http://dx.doi.org/10.1016/j.cobeha.2014.10.004 http://linkinghub.elsevier.com/retrieve/pii/S2352154614000242},
volume = {1},
year = {2015}
}
@book{Sutton2011,
address = {М.},
author = {Саттон, Р.С. and Барто, Э. Г.},
edition = {2-е},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Саттон, Барто/2011/Обучение с подкреплением.pdf:pdf},
language = {russian},
pages = {399},
publisher = {БИНОМ. Лаборатория знаний},
title = {{Обучение с подкреплением}},
year = {2011}
}
@inproceedings{Mehta2008b,
abstract = {We present an algorithm, HI-MAT (Hierar- chy Induction via Models And Trajectories), that discovers MAXQ task hierarchies by ap- plying dynamic Bayesian network models to a successful trajectory from a source rein- forcement learning task. HI-MAT discovers subtasks by analyzing the causal and tem- poral relationships among the actions in the trajectory. Under appropriate assumptions, HI-MAT induces hierarchies that are consis- tent with the observed trajectory and have compact value-function tables employing safe state abstractions. We demonstrate empir- ically that HI-MAT constructs compact hi- erarchies that are comparable to manually- engineered hierarchies and facilitate signifi- cant speedup in learning when transferred to a target task.},
address = {New York, New York, USA},
author = {Mehta, Neville and Ray, Soumya and Tadepalli, Prasad and Dietterich, Thomas},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390238},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Mehta et al/2008/Automatic discovery and transfer of MAXQ hierarchies.pdf:pdf},
isbn = {9781605582054},
pages = {648--655},
publisher = {ACM Press},
title = {{Automatic discovery and transfer of MAXQ hierarchies}},
url = {http://portal.acm.org/citation.cfm?id=1390238 http://portal.acm.org/citation.cfm?doid=1390156.1390238},
year = {2008}
}
@incollection{Menache2002a,
abstract = {We present the Q-Cut algorithm, a graph theoretic approach for automatic detection of sub-goals in a dynamic environment, which is used for acceleration of the Q-Learning algorithm. The learning agent creates an on-line map of the process history, and uses an efficient Max-Flow/Min-Cut algorithm for identifying bottlenecks. The policies for reaching bottlenecks are separately learned and added to the model in a form of options (macro-actions). We then extend the basic Q-Cut algorithm to the Segmented Q-Cut algorithm, which uses previously identified bottlenecks for state space partitioning, necessary for finding additional bottlenecks in complex environments. Experiments show significant performance improvements, particulary in the initial learning phase.},
author = {Menache, Ishai and Mannor, Shie and Shimkin, Nahum},
booktitle = {ECML 2002: Machine Learning: ECML 2002},
doi = {10.1007/3-540-36755-1_25},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Menache, Mannor, Shimkin/2002/Q-Cut—Dynamic Discovery of Sub-goals in Reinforcement Learning.pdf:pdf},
isbn = {978-3-540-44036-9, 978-3-540-36755-0},
issn = {16113349},
pages = {295--306},
title = {{Q-Cut—Dynamic Discovery of Sub-goals in Reinforcement Learning}},
url = {http://link.springer.com/10.1007/3-540-36755-1{\_}25},
year = {2002}
}
@article{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task involving finding rewards in random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Mnih et al/2016/Asynchronous Methods for Deep Reinforcement Learning.pdf:pdf},
isbn = {9781510829008},
journal = {arXiv},
pages = {1--28},
pmid = {1000272564},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}
@inproceedings{MacGlashan2010a,
abstract = {We present skill bootstrapping, a proposed new research direction for agent learning and planning that allows an agent to start with low-level primitive actions, and develop skills that can be used for higher-level planning. Skills are developed over the course of solving many different problems in a domain, using reinforcement learning techniques to complement the benefits and disadvantages of heuristic-search planning. We describe the overall architecture of the proposed approach, discuss how it relates to other work, and give motivating examples for why this approach would be successful.},
author = {MacGlashan, James and DesJArdins, Marie},
booktitle = {24th AAAI Conference on Artificial Intelligence},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/MacGlashan, DesJArdins/2010/Hierarchical Skill Learning for High-Level Planning.pdf:pdf;:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/MacGlashan, DesJArdins/2010/Hierarchical Skill Learning for High-Level Planning(2).pdf:pdf},
isbn = {9781577354666},
keywords = {abstraction,approximation,planning,reinforcement learning},
pages = {1988--1989},
title = {{Hierarchical Skill Learning for High-Level Planning}},
volume = {3},
year = {2010}
}
@article{Cao2012a,
abstract = {We describe an approach to incorporating Bayesian priors in the MAXQ framework for hierarchical reinforcement learning (HRL). We define priors on the primitive environment model and on task pseudo-rewards. Since models for composite tasks can be complex, we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy. We show empirically that (i) our approach results in improved convergence over non-Bayesian baselines, (ii) using both task hierarchies and Bayesian priors is better than either alone, (iii) taking advantage of the task hierarchy reduces the computational cost of Bayesian reinforcement learning and (iv) in this framework, task pseudo-rewards can be learned instead of being manually specified, leading to hierarchically optimal rather than recursively optimal policies.},
author = {Cao, F and Ray, Soumya},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cao, Ray/2012/Bayesian hierarchical reinforcement learning.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {73--81},
title = {{Bayesian hierarchical reinforcement learning}},
url = {http://papers.nips.cc/paper/4752-bay},
year = {2012}
}
@inproceedings{Davoodabadi2011a,
abstract = {In this paper the problem of automatically discovering subtasks and hierarchies in reinforcement learning is considered. We present a novel method that allows an agent to autonomously discover subgoals and create a hierarchy from actions. Our method identifies subgoals by partitioning local state transition graphs. Options constructed for reaching these subgoals are added to action choices and used for accelerating the Q-Learning algorithm. Experimental results show significant performance improvements, especially in the initial learning phase.},
author = {Davoodabadi, M and Beigy, H},
booktitle = {Proceedings of the 5th Indian International Conference on Artificial Intelligence, IICAI 2011},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Davoodabadi, Beigy/2011/A new method for discovering subgoals and constructing options in reinforcement learning.pdf:pdf},
isbn = {9780972741286},
keywords = {Artificial intelligence,Autonomously discovering subgoals,Community detection,Hierarchical reinforcement learning,Learning algorithms,Learning phase,Local state,Option,Performance improvements,Q-learning algorithms,Reinforcement learning,Subgoals,Subtasks},
pages = {441--450},
title = {{A new method for discovering subgoals and constructing options in reinforcement learning}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84872195092{\&}partnerID=40{\&}md5=9bf17b3f8f09d77ec8cd05285124028d},
year = {2011}
}
@article{Wang2012b,
abstract = {Reinforcement learning has been an important category of machine learning approaches exhibiting self-learning and online learning characteristics. Using reinforcement learning, an agent can learn its behaviors through trial-and-error interactions with a dynamic environment and finally come up with an optimal strategy. Reinforcement learning suffers the curse of dimensionality, though there has been significant progress to overcome this issue in recent years. MAXQ is one of the most common approaches for reinforcement learning. To function properly, MAXQ requires a decomposition of the agent's task into a task hierarchy. Previously, the decomposition can only be done manually. In this paper, we propose a mechanism for automatic subtask discovery. The mechanism applies clustering to automatically construct task hierarchy required by MAXQ, such that MAXQ can be fully automated. We present the design of our mechanism, and demonstrate its effectiveness through theoretical analysis and an extensive experimental evaluation. {\textcopyright} 2012 IEEE.},
author = {Wang, Hongbing and Li, Wenya and Zhou, Xuan},
doi = {10.1109/ICTAI.2012.165},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Wang, Li, Zhou/2012/Automatic discovery and transfer of MAXQ hierarchies in a complex system.pdf:pdf},
isbn = {9780769549156},
issn = {10823409},
journal = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
keywords = {Clustering,MAXQ,Reinforcement Learning,System of Systems},
pages = {1157--1162},
title = {{Automatic discovery and transfer of MAXQ hierarchies in a complex system}},
volume = {1},
year = {2012}
}
@article{Melo1997,
abstract = {We address the problem of computing the optimal Q-function in Markov decision problems with infinite state-space. We analyze the convergence properties of several variations of Q-learning when combined with function approximation, extending the analysis of TD-learning in (Tsitsiklis {\&} Van Roy, 1996a) to stochastic control settings. We identify conditions under which such approximate methods converge with probability 1. We conclude with a brief discussion on the general applicability of our results and compare them with several related works.},
author = {Melo, F S and Meyn, S P and Ribeiro, M I},
doi = {10.1109/9.580874},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Melo, Meyn, Ribeiro/1997/An analysis of reinforcement learning with function approximation.pdf:pdf},
isbn = {9781605582054},
issn = {00189286},
journal = {IEEE Transactions on Automatic Control},
keywords = {planning reinforcement learning Q-learning tempora},
number = {5},
pages = {674--690},
title = {{An analysis of reinforcement learning with function approximation}},
volume = {42},
year = {1997}
}
@article{Sun2005,
abstract = {This article explicates the interaction between implicit and explicit processes in skill learning, in contrast to the tendency of researchers to study each type in isolation. It highlights various effects of the interaction on learning (including synergy effects). The authors argue for an integrated model of skill learning that takes into account both implicit and explicit processes. Moreover, they argue for a bottom-up approach (first learning implicit knowledge and then explicit knowledge) in the integrated model. A variety of qualitative data can be accounted for by the approach. A computational model, CLARION, is then used to simulate a range of quantitative data. The results demonstrate the plausibility of the model, which provides a new perspective on skill learning.},
author = {Sun, Ron and Slusarz, Paul and Terry, Chris},
doi = {10.1037/0033-295X.112.1.159},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Sun, Slusarz, Terry/2005/The Interaction of the Explicit and the Implicit in Skill Learning A Dual-Process Approach.pdf:pdf},
isbn = {0033-295X},
issn = {1939-1471},
journal = {Psychological Review},
keywords = {artificial grammars,connectionist,decision-making,information,memory,model,nonconscious acquisition,procedural knowledge,tacit knowledge,task-performance},
number = {1},
pages = {159--192},
pmid = {15631592},
title = {{The Interaction of the Explicit and the Implicit in Skill Learning: A Dual-Process Approach.}},
volume = {112},
year = {2005}
}
@inproceedings{Ghadirzadeh2017,
abstract = {Skilled robot task learning is best implemented by predictive action policies due to the inherent latency of sensorimotor processes. However, training such predictive policies is challenging as it involves finding a trajectory of motor activations for the full duration of the action. We propose a data-efficient deep predictive policy training (DPPT) framework with a deep neural network policy architecture which maps an image observation to a sequence of motor activations. The architecture consists of three sub-networks referred to as the perception, policy and behavior super-layers. The perception and behavior super-layers force an abstraction of visual and motor data trained with synthetic and simulated training samples, respectively. The policy super-layer is a small sub-network with fewer parameters that maps data in-between the abstracted manifolds. It is trained for each task using methods for policy search reinforcement learning. We demonstrate the suitability of the proposed architecture and learning framework by training predictive policies for skilled object grasping and ball throwing on a PR2 robot. The effectiveness of the method is illustrated by the fact that these tasks are trained using only about 180 real robot attempts with qualitative terminal rewards.},
archivePrefix = {arXiv},
arxivId = {1703.00727},
author = {Ghadirzadeh, Ali and Maki, Atsuto and Kragic, Danica and Bjorkman, Marten},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2017.8206046},
eprint = {1703.00727},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Ghadirzadeh et al/2017/Deep predictive policy training using reinforcement learning.pdf:pdf},
isbn = {9781538626825},
issn = {21530866},
pages = {2351--2358},
title = {{Deep predictive policy training using reinforcement learning}},
year = {2017}
}
@article{Li2017,
abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We start with background of deep learning and reinforcement learning, as well as introduction of testbeds. Next we discuss Deep Q-Network (DQN) and its extensions, asynchronous methods, policy optimization, reward, and planning. After that, we talk about attention and memory, unsupervised learning, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, spoken dialogue systems (a.k.a. chatbot), machine translation, text sequence prediction, neural architecture design, personalized web services, healthcare, finance, and music generation. We mention topics/papers not reviewed yet. After listing a collection of RL resources, we close with discussions.},
archivePrefix = {arXiv},
arxivId = {1701.07274},
author = {Li, Yuxi},
eprint = {1701.07274},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Li/2017/Deep Reinforcement Learning An Overview.pdf:pdf},
journal = {ArXiv: 1701.07274},
pages = {1--30},
title = {{Deep Reinforcement Learning: An Overview}},
url = {http://arxiv.org/abs/1701.07274},
year = {2017}
}
@article{Shankar2017,
abstract = {Deep Reinforcement Learning has enabled the learning of policies for complex tasks in partially observable environments, without explicitly learning the underlying model of the tasks. While such model-free methods achieve considerable performance, they often ignore the structure of task. We present a natural representation of to Reinforcement Learning (RL) problems using Recurrent Convolutional Neural Networks (RCNNs), to better exploit this inherent structure. We define 3 such RCNNs, whose forward passes execute an efficient Value Iteration, propagate beliefs of state in partially observable environments, and choose optimal actions respectively. Backpropagating gradients through these RCNNs allows the system to explicitly learn the Transition Model and Reward Function associated with the underlying MDP, serving as an elegant alternative to classical model-based RL. We evaluate the proposed algorithms in simulation, considering a robot planning problem. We demonstrate the capability of our framework to reduce the cost of replanning, learn accurate MDP models, and finally re-plan with learnt models to achieve near-optimal policies.},
archivePrefix = {arXiv},
arxivId = {1701.02392},
author = {Shankar, Tanmay and Dwivedy, Santosha K. and Guha, Prithwijit},
doi = {10.1109/ICPR.2016.7900026},
eprint = {1701.02392},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Shankar, Dwivedy, Guha/2017/Reinforcement Learning via Recurrent Convolutional Neural Networks.pdf:pdf;:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Shankar, Dwivedy, Guha/2017/Reinforcement Learning via Recurrent Convolutional Neural Networks(2).pdf:pdf},
isbn = {978-1-5090-4847-2},
issn = {10514651},
journal = {2016 23rd International Conference on Pattern Recognition (ICPR)},
month = {dec},
pages = {2592--2597},
publisher = {IEEE},
title = {{Reinforcement Learning via Recurrent Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1701.02392 http://ieeexplore.ieee.org/document/7900026/},
year = {2017}
}
@book{Sun2001,
abstract = {This paper presents a skill learning model Clarion. Different from existing models of mostly high-level skill learning that use a top-down approach (that is, turning declarative knowledge into procedural knowledge through practice), we adopt a bottom-up approach toward low-level skill learning, where procedural knowledge develops first and declarative knowledge develops later. Our model is formed by integrating connectionist, reinforcement, and symbolic learning methods to perform on-line reactive learning. It adopts a two-level dual-representation framework (Sun, 1995), with a combination of localist and distributed representation. We compare the model with human data in a minefield navigation task, demonstrating some match between the model and human data in several respects. ?? 2001 Cognitive Science Society, Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sun, Ron and Merrill, Edward and Peterson, Todd},
booktitle = {Cognitive Science},
doi = {10.1016/S0364-0213(01)00035-0},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Sun, Merrill, Peterson/2001/From implicit skills to explicit knowledge A bottom-up model of skill learning.pdf:pdf},
isbn = {1573884766},
issn = {03640213},
number = {2},
pages = {203--244},
pmid = {15003161},
title = {{From implicit skills to explicit knowledge: A bottom-up model of skill learning}},
volume = {25},
year = {2001}
}
@article{Tesauro1995,
author = {Tesauro, G.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Tesauro/1995/Temporal Difference Learning and TD-Gammon.pdf:pdf},
journal = {Communications of the ACM},
number = {5},
pages = {58--68},
title = {{Temporal Difference Learning and TD-Gammon}},
volume = {38},
year = {1995}
}
@inproceedings{Yahya2017,
abstract = {In principle, reinforcement learning and policy search methods can enable robots to learn highly complex and general skills that may allow them to function amid the complexity and diversity of the real world. However, training a policy that generalizes well across a wide range of real-world conditions requires far greater quantity and diversity of experience than is practical to collect with a single robot. Fortunately, it is possible for multiple robots to share their experience with one another, and thereby, learn a policy collectively. In this work, we explore distributed and asynchronous policy learning as a means to achieve generalization and improved training times on challenging, real-world manipulation tasks. We propose a distributed and asynchronous version of Guided Policy Search and use it to demonstrate collective policy learning on a vision-based door opening task using four robots. We show that it achieves better generalization, utilization, and training times than the single robot alternative.},
archivePrefix = {arXiv},
arxivId = {1610.00673},
author = {Yahya, Ali and Li, Adrian and Kalakrishnan, Mrinal and Chebotar, Yevgen and Levine, Sergey},
booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
eprint = {1610.00673},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Yahya et al/2017/Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search.pdf:pdf},
isbn = {9781538626818},
keywords = {Deep Learning in Rob,Learning and Adaptive Systems},
pages = {79--86},
publisher = {IEEE},
title = {{Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search}},
url = {http://arxiv.org/abs/1610.00673},
year = {2017}
}
@inproceedings{Dickens2010,
author = {Dickens, Luke and Broda, Krysia and Russo, Alessandra},
booktitle = {ECAI 2010: 19th European Conference on Artificial Intelligence, 16-20 August 2010, Lisbon, Portugal : Including Prestigious Applications of Artificial Intelligence (PAIS-2010) : Proceedings},
doi = {10.3233/978-1-60750-606-5-367},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Dickens, Broda, Russo/2010/The Dynamics of Multi-Agent Reinforcement Learning.pdf:pdf},
isbn = {9781607506065},
pages = {367--372},
title = {{The Dynamics of Multi-Agent Reinforcement Learning}},
year = {2010}
}
@inproceedings{Harutyunyan2017,
abstract = {A temporally abstract action, or an option, is specified by a policy and a termination condition: the policy guides option behavior, and the termination condition roughly determines its length. Generally, learning with longer options (like learning with multi-step returns) is known to be more efficient. However, if the option set for the task is not ideal, and cannot express the primitive optimal policy exactly, shorter options offer more flexibility and can yield a better solution. Thus, the termination condition puts learning efficiency at odds with solution quality. We propose to resolve this dilemma by decoupling the behavior and target terminations, just like it is done with policies in off-policy learning. To this end, we give a new algorithm, Q($\backslash$beta), that learns the solution with respect to any termination condition, regardless of how the options actually terminate. We derive Q($\backslash$beta) by casting learning with options into a common framework with well-studied multi-step off-policy learning. We validate our algorithm empirically, and show that it holds up to its motivating claims.},
archivePrefix = {arXiv},
arxivId = {1711.03817},
author = {Harutyunyan, Anna and Vrancx, Peter and Bacon, Pierre-Luc and Precup, Doina and Nowe, Ann},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
eprint = {1711.03817},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Harutyunyan et al/2017/Learning with Options that Terminate Off-Policy.pdf:pdf},
title = {{Learning with Options that Terminate Off-Policy}},
url = {http://arxiv.org/abs/1711.03817},
year = {2017}
}
@incollection{Castro2012a,
abstract = {Abstract. Temporally extended actions are usually effective in speeding up reinforcement learning. In this paper we present a mechanism for automatically constructing such actions, expressed as options [Sutton et al., 1999], in a finite Markov Decision Process (MDP). To do this, we compute a bisimulation metric [Ferns et al., 2004] between the states in a small MDP and the states in a large MDP, which we want to solve. The shape of this metric is then used to completely define a set of options for the large MDP. We demonstrate empirically that our approach is able to improve the speed of reinforcement learning, and is generally not sensitive to parameter tuning.},
author = {Castro, Pablo Samuel and Precup, Doina},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-29946-9_16},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Castro, Precup/2012/Automatic Construction of Temporally Extended Actions for MDPs Using Bisimulation Metrics.pdf:pdf},
isbn = {9783642299452},
issn = {03029743},
pages = {140--152},
title = {{Automatic Construction of Temporally Extended Actions for MDPs Using Bisimulation Metrics}},
url = {http://link.springer.com/10.1007/978-3-642-29946-9{\_}16},
volume = {7188 LNAI},
year = {2012}
}
@inproceedings{Stulp2011,
abstract = {Temporal abstraction and task decomposition drastically reduce the search space for planning and control, and are fundamental to making complex tasks amenable to learning. In the context of reinforcement learning, temporal abstractions are studied within the paradigm of hierarchical reinforcement learning.},
author = {Stulp, Freek and Schaal, Stefan},
booktitle = {2011 11th IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/Humanoids.2011.6100841},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Stulp, Schaal/2011/Hierarchical reinforcement learning with movement primitives.pdf:pdf},
isbn = {978-1-61284-868-6},
issn = {21640572},
month = {oct},
pages = {231--238},
publisher = {IEEE},
title = {{Hierarchical reinforcement learning with movement primitives}},
url = {http://ieeexplore.ieee.org/document/6100841/},
year = {2011}
}
@book{WieringMarco;vanOtterlo2012,
abstract = {Reinforcement Learning was originally developed for Markov Decision Processes (MDPs). It allows a single agent to learn a policy that maximizes a pos- sibly delayed reward signal in a stochastic stationary environment. It guarantees convergence to the optimal policy, provided that the agent can sufficiently experi- ment and the environment in which it is operating is Markovian. However, when multiple agents apply reinforcement learning in a shared environment, thismight be beyond theMDP model. In such systems, the optimal policy of an agent depends not only on the environment, but on the policies of the other agents aswell. These situa- tions arise naturally in a variety of domains, such as: robotics, telecommunications, economics, distributed control, auctions, traffic light control, etc. In these domains multi-agent learning is used, either because of the complexity of the domain or be- cause control is inherently decentralized. In such systems it is important that agents are capable of discovering good solutions to the problem at hand either by coordi- nating with other learners or by competing with them. This chapter focuses on the application reinforcement learning techniques in multi-agent systems.We describe a basic learning framework based on the economic research into game theory, and illustrate the additional complexity that arises in such systems. We also described a representative selection of algorithms for the different areas of multi-agent rein- forcement learning research.},
doi = {10.1007/978-3-642-27645-3},
editor = {Wiering, Marco and van Otterlo, Martijn},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2012/Reinforcement Learning The State of the Art.pdf:pdf},
isbn = {978-3-642-27644-6},
pages = {652},
publisher = {Springer},
title = {{Reinforcement Learning: The State of the Art}},
year = {2012}
}
@article{Liu2015,
abstract = {The K Shortest Paths (KSPs) problem with non-numerable applications has been researched widely, which aims to compute KSPs between two nodes in a non-decreasing order. However, less effort has been devoted to single-source KSP problem than to single-pair KSP computation, especially by using parallel methods. This paper proposes a Modified Continued Pulse Coupled Neural Network (MCPCNN) model to solve the two kinds of KSP problems. Theoretical analysis of MCPCNN and two algorithms for KSPs computation are presented. By using the parallel pulse transmission characteristic of pulse coupled neural networks, the method is able to find k shortest paths quickly. The computational complexity is only related to the length of the longest shortest path. Simulative results for route planning show that the proposed MCPCNN method for KSPs computation outperforms many other current efficient algorithms.},
author = {Liu, Guisong and Qiu, Zhao and Qu, Hong and Ji, Luping},
doi = {10.1016/j.neucom.2014.09.012},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Liu et al/2015/Computing k shortest paths using modified pulse-coupled neural network.pdf:pdf},
isbn = {9780874216561},
issn = {18728286},
journal = {Neurocomputing},
keywords = {K Shortest paths,Pulse coupled neural network,Single-pair KSP,Single-source KSP},
number = {PC},
pages = {1162--1176},
publisher = {Elsevier},
title = {{Computing k shortest paths using modified pulse-coupled neural network}},
url = {http://dx.doi.org/10.1016/j.neucom.2014.09.012},
volume = {149},
year = {2015}
}
@inproceedings{Sharma2017,
author = {Sharma, Avinash and Gupta, Kanika and Kumar, Anirudha and Sharma, Aishwarya and Kumar, Rajesh and Member, Senior},
booktitle = {2017 IEEE International Conference on Industrial Technology (ICIT)},
doi = {10.1109/ICIT.2017.7915468},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Sharma et al/2017/Model based path planning using Q-Learning.pdf:pdf},
keywords = {Control System,Robotics and Automation},
pages = {837--842},
publisher = {IEEE},
title = {{Model based path planning using Q-Learning}},
year = {2017}
}
@article{YangLuShujuanYiYurongLiu2016,
author = {Lu, Yang and Yi, Shujuan and Liu, Yurong and Ji, Yuling},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Lu et al/2016/A Novel Path Planning Method for Biomimetic Robot Based on Deep Learning.pdf:pdf},
journal = {Assembly Automation},
number = {2},
title = {{A Novel Path Planning Method for Biomimetic Robot Based on Deep Learning}},
volume = {36},
year = {2016}
}
@article{Levine2016,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
archivePrefix = {arXiv},
arxivId = {1504.00702},
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1504.00702},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Levine et al/2016/End-to-End Training of Deep Visuomotor Policies.pdf:pdf},
isbn = {9781479969227},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Neural Networks,Optimal Control,Reinforcement Learning,Vision},
pages = {1--40},
pmid = {15003161},
title = {{End-to-End Training of Deep Visuomotor Policies}},
volume = {17},
year = {2016}
}
@article{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q,-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
archivePrefix = {arXiv},
arxivId = {1412.3409},
author = {Watkins, Christopher J. C. H. and Dayan, Peter},
doi = {10.1023/A:1022676722315},
eprint = {1412.3409},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Watkins, Dayan/1992/Q-Learning.pdf:pdf},
isbn = {978-1-4613-6608-9},
issn = {15730565},
journal = {Machine Learning},
keywords = {(Formula presented.)-learning,asynchronous dynamic programming,reinforcement learning,temporal differences},
number = {3},
pages = {279--292},
pmid = {7761831},
title = {{Q-Learning}},
volume = {8},
year = {1992}
}
@article{Rasmussen2017a,
abstract = {We present the first model capable of performing hierarchical reinforcement learning in a general, neurally detailed imple-mentation. We show that this model is able to learn a spatial pickup and delivery task more quickly than one without hier-archical abilities. In addition, we show that this model is able to leverage its hierarchical structure to transfer learned knowl-edge between related tasks. These results point towards the advantages to be gained by using a hierarchical RL framework to understand the brain's powerful learning ability.},
author = {Rasmussen, Daniel and Voelker, Aaron and Eliasmith, Chris},
doi = {10.1371/journal.pone.0180234},
editor = {Cymbalyuk, Gennady},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Rasmussen, Voelker, Eliasmith/2017/A neural model of hierarchical reinforcement learning.pdf:pdf;:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Rasmussen, Voelker, Eliasmith/2017/A neural model of hierarchical reinforcement learning(2).pdf:pdf},
isbn = {1111111111},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {brain could achieve its,hierarchical,hrl into a theory,neural engineering framework,neural model,of neural function,providing a hypothe-,reinforcement learning,sis for how the,strengths in scaling},
month = {jul},
number = {7},
pages = {e0180234},
title = {{A neural model of hierarchical reinforcement learning}},
url = {http://dx.plos.org/10.1371/journal.pone.0180234},
volume = {12},
year = {2017}
}
@article{Qu2009,
author = {Qu, Hong and Yang, Simon X and Willms, Allan R and Yi, Zhang},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Qu et al/2009/Real-time robot path planning based on a modified pulse-coupled neural network model.pdf:pdf},
isbn = {1045-9227},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
number = {11},
pages = {1724--1739},
title = {{Real-time robot path planning based on a modified pulse-coupled neural network model}},
volume = {20},
year = {2009}
}
@incollection{Xia2015,
address = {Paris},
author = {Xia, Chen and {El Kamel}, A.},
booktitle = {Proceedings of the 21st International Conference on Industrial Engineering and Engineering Management 2014},
doi = {10.2991/978-94-6239-102-4_136},
editor = {Qi, Ershi and Shen, Jiang and Dou, Runliang},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Xia, El Kamel/2015/A Reinforcement Learning Method of Obstacle Avoidance for Industrial Mobile Vehicles in Unknown Environments Using Neural Network.pdf:pdf},
isbn = {978-94-6239-101-7},
keywords = {automation,capacity building,f rxqghu,ngo,npo,resource mismanagement,syndrome,v},
pages = {671--675},
publisher = {Atlantis Press},
series = {Proceedings of the International Conference on Industrial Engineering and Engineering Management},
title = {{A Reinforcement Learning Method of Obstacle Avoidance for Industrial Mobile Vehicles in Unknown Environments Using Neural Network}},
url = {http://link.springer.com/10.2991/978-94-6239-102-4 http://link.springer.com/10.2991/978-94-6239-102-4{\_}136},
volume = {2014},
year = {2015}
}
@article{Singh2010a,
abstract = {There is great interest in building intrinsic motivation into artificial systems using the reinforcement learning framework. Yet, what intrinsic motivation may mean computationally, and how it may differ from extrinsic motivation, remains a murky and controversial subject. In this paper, we adopt an evolutionary perspective and define a new optimal reward framework that captures the pressure to design good primary reward functions that lead to evolutionary success across environments. The results of two computational experiments show that optimal primary reward signals may yield both emergent intrinsic and extrinsic motivation. The evolutionary perspective and the associated optimal reward framework thus lead to the conclusion that there are no hard and fast features distinguishing intrinsic and extrinsic reward computationally. Rather, the directness of the relationship between rewarding behavior and evolutionary success varies along a continuum.},
author = {Singh, Satinder and Lewis, Richard L. and Barto, Andrew G. and Sorg, Jonathan},
doi = {10.1109/TAMD.2010.2051031},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Singh et al/2010/Intrinsically Motivated Reinforcement Learning An Evolutionary Perspective(2).pdf:pdf},
isbn = {0262195348, 9780262195348},
issn = {1943-0604},
journal = {IEEE Transactions on Autonomous Mental Development},
number = {2},
pages = {70--82},
title = {{Intrinsically Motivated Reinforcement Learning: An Evolutionary Perspective}},
url = {http://ieeexplore.ieee.org/document/5471106/},
volume = {2},
year = {2010}
}
@inproceedings{Lange2010,
author = {Lange, Sascha and Riedmiller, Martin},
booktitle = {The 2010 International Joint Conference on Neural Networks (IJCNN)},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Lange, Riedmiller/2010/Deep Auto-Encoder Neural Networks in Reinforcement Learning.pdf:pdf},
pages = {1--8},
publisher = {IEEE},
title = {{Deep Auto-Encoder Neural Networks in Reinforcement Learning}},
year = {2010}
}
@article{Paxton2017,
abstract = {We consider task and motion planning in complex dynamic environments for problems expressed in terms of a set of Linear Temporal Logic (LTL) constraints, and a reward function. We propose a methodology based on reinforcement learning that employs deep neural networks to learn low-level control policies as well as task-level option policies. A major challenge in this setting, both for neural network approaches and classical planning, is the need to explore future worlds of a complex and interactive environment. To this end, we integrate Monte Carlo Tree Search with hierarchical neural net control policies trained on expressive LTL specifications. This paper investigates the ability of neural networks to learn both LTL constraints and control policies in order to generate task plans in complex environments. We demonstrate our approach in a simulated autonomous driving setting, where a vehicle must drive down a road in traffic, avoid collisions, and navigate an intersection, all while obeying given rules of the road.},
archivePrefix = {arXiv},
arxivId = {1703.07887},
author = {Paxton, Chris and Raman, Vasumathi and Hager, Gregory D. and Kobilarov, Marin},
eprint = {1703.07887},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Paxton et al/2017/Combining Neural Networks and Tree Search for Task and Motion Planning in Challenging Environments.pdf:pdf},
journal = {ArXiv: 1703.07887},
title = {{Combining Neural Networks and Tree Search for Task and Motion Planning in Challenging Environments}},
url = {http://arxiv.org/abs/1703.07887},
year = {2017}
}
@inproceedings{Johnson2016,
abstract = {We present Project Malmo – an AI experimenta-tion platform built on top of the popular computer game Minecraft, and designed to support funda-mental research in artificial intelligence. As the AI research community pushes for artificial gen-eral intelligence (AGI), experimentation platforms are needed that support the development of flexible agents that learn to solve diverse tasks in complex environments. Minecraft is an ideal foundation for such a platform, as it exposes agents to complex 3D worlds, coupled with infinitely varied game-play. Project Malmo provides a sophisticated abstraction layer on top of Minecraft that supports a wide range of experimentation scenarios, ranging from navi-gation and survival to collaboration and problem solving tasks. In this demo we present the Malmo platform and its capabilities. The platform is pub-licly released as open source software at IJCAI, to support openness and collaboration in AI research.},
author = {Johnson, Matthew and Hofmann, Katja and Hutton, Tim and Bignell, David},
booktitle = {Proc. 25th International Joint Conference on Artificial Intelligence},
editor = {Kambhampati, S.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Johnson et al/2016/The Malmo Platform for Artificial Intelligence Experimentation.pdf:pdf},
issn = {10450823},
keywords = {Demonstrations},
pages = {4246--4247},
publisher = {AAAI Press},
title = {{The Malmo Platform for Artificial Intelligence Experimentation}},
year = {2016}
}
@article{Kober2013,
abstract = {Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
doi = {10.1177/0278364913495721},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kober, Bagnell, Peters/2013/Reinforcement learning in robotics A survey.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {learning control,reinforcement learning,robot,survey},
month = {sep},
number = {11},
pages = {1238--1274},
title = {{Reinforcement learning in robotics: A survey}},
url = {http://link.springer.com/10.1007/978-3-319-03194-1{\_}2 http://journals.sagepub.com/doi/10.1177/0278364913495721},
volume = {32},
year = {2013}
}
@book{Sutton2012,
address = {London},
author = {Sutton, Richard S. and Barto, Andrew G.},
edition = {2nd},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Sutton, Barto/2012/Reinforcement learning An Introduction.pdf:pdf},
pages = {320},
publisher = {The MIT Press},
title = {{Reinforcement learning: An Introduction}},
url = {https://books.google.com/books?id=CAFR6IBF4xYC{\&}pgis=1{\%}5Cnhttp://incompleteideas.net/sutton/book/the-book.html{\%}5Cnhttps://www.dropbox.com/s/f4tnuhipchpkgoj/book2012.pdf},
year = {2012}
}
@misc{Brockam2016,
abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
archivePrefix = {arXiv},
arxivId = {1606.01540},
author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
booktitle = {ArXiv: 1606.01540},
eprint = {1606.01540},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Brockman et al/2016/OpenAI Gym.pdf:pdf},
pages = {1--4},
title = {{OpenAI Gym}},
url = {http://arxiv.org/abs/1606.01540},
year = {2016}
}
@article{Xia2016,
abstract = {Designing intelligent and robust autonomous navigation systems remains a great challenge in mobile robotics. Inverse reinforcement learning (IRL) offers an efficient learning technique from expert demonstrations to teach robots how to perform specific tasks without manually specifying the reward function. Most of existing IRL algorithms assume the expert policy to be optimal and deterministic, and are applied to experiments with relatively small-size state spaces. However, in autonomous navigation tasks, the state spaces are frequently large and demonstrations can hardly visit all the states. Meanwhile the expert policy may be non-optimal and stochastic. In this paper, we focus on IRL with large-scale and high-dimensional state spaces by introducing the neural network to generalize the expert's behaviors to unvisited regions of the state space and an explicit policy representation is easily expressed by neural network, even for the stochastic expert policy. An efficient and convenient algorithm, Neural Inverse Reinforcement Learning (NIRL), is proposed. Experimental results on simulated autonomous navigation tasks show that a mobile robot using our approach can successfully navigate to the target position without colliding with unpredicted obstacles, largely reduce the learning time, and has a good generalization performance on undemonstrated states. Hence prove the robot intelligence of autonomous navigation transplanted from limited demonstrations to completely unknown tasks.},
author = {Xia, Chen and {El Kamel}, Abdelkader},
doi = {10.1016/j.robot.2016.06.003},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Xia, El Kamel/2016/Neural inverse reinforcement learning in autonomous navigation.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Autonomous navigation,Dynamic environments,Inverse reinforcement learning,Learning from demonstration,Markov decision processes,Neural network},
pages = {1--14},
publisher = {Elsevier B.V.},
title = {{Neural inverse reinforcement learning in autonomous navigation}},
url = {http://dx.doi.org/10.1016/j.robot.2016.06.003},
volume = {84},
year = {2016}
}
@article{Konidaris2012a,
abstract = {We present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to significantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that significantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-specific skills.},
author = {Konidaris, George and Scheidwasser, Ilya and Barto, Andrew G.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Konidaris, Scheidwasser, Barto/2012/Transfer in reinforcement learning via shared features.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {reinforcement learning,shaping,skills,transfer},
number = {1},
pages = {1333--1371},
title = {{Transfer in reinforcement learning via shared features}},
url = {http://dl.acm.org/citation.cfm?id=2503308.2343689{\%}5Cnhttp://dl.acm.org/ft{\_}gateway.cfm?id=2343689{\&}type=pdf},
volume = {13},
year = {2012}
}
@article{Zhang2017,
author = {Zhang, Fengyun and Duan, Shukai and Wang, Lidan},
doi = {10.1007/s11571-017-9423-7},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Zhang, Duan, Wang/2017/Route searching based on neural networks and heuristic reinforcement learning.pdf:pdf},
issn = {1871-4080},
journal = {Cognitive Neurodynamics},
keywords = {Route searching,Neural network,Heuristic reinforce,exploitation,network {\'{a}} heuristic,reinforcement learning {\'{a}} greedy,route searching {\'{a}} neural},
publisher = {Springer Netherlands},
title = {{Route searching based on neural networks and heuristic reinforcement learning}},
url = {http://link.springer.com/10.1007/s11571-017-9423-7},
year = {2017}
}
@phdthesis{Lin1993,
author = {Lin, Long-ji},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Lin/1993/Reinforcement Learning for Robots Using Neural Networks.pdf:pdf},
pages = {1--155},
school = {CMU},
title = {{Reinforcement Learning for Robots Using Neural Networks}},
year = {1993}
}
@article{Duan2016,
abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophis-ticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, bene-fiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a " fast " reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL 2 , the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose (" slow ") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including ob-servations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the " fast " RL algorithm on the current (previously unseen) MDP. We evaluate RL 2 experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL 2 is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL 2 on a vision-based navigation task and show that it scales up to high-dimensional problems.},
archivePrefix = {arXiv},
arxivId = {1611.02779},
author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter and Sutskever, Ilya and Abbeel, Pieter},
doi = {10.1051/0004-6361/201527329},
eprint = {1611.02779},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Duan et al/2016/RL2 Fast Reinforcement Learning Via Slow Reinforcement Learning.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
journal = {arXiv},
pages = {1--14},
title = {{RL{\^{}}2: Fast Reinforcement Learning Via Slow Reinforcement Learning}},
year = {2016}
}
@article{Kaelbling1996,
abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
doi = {10.1613/jair.301},
eprint = {9605103},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kaelbling, Littman, Moore/1996/Reinforcement learning A survey.pdf:pdf},
isbn = {0-7803-3213-X},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {237--285},
pmid = {17255001},
primaryClass = {cs},
title = {{Reinforcement learning: A survey}},
volume = {4},
year = {1996}
}
@article{Simsek2005a,
abstract = {We present a new subgoal-based method for automatically creating useful skills in reinforcement learning. Our method identifies subgoals by partitioning local state transition graphs—those that are constructed using only the most recent experiences of the agent. The local scope of our subgoal discovery method allows it to successfully identify the type of subgoals we seek—states that lie between two densely-connected regions of the state space—while producing an algorithm with low computational cost.},
author = {Şimşek, {\"{O}}zg{\"{u}}r and Wolfe, Alicia P. and Barto, Andrew G.},
doi = {10.1145/1102351.1102454},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Şimşek, Wolfe, Barto/2005/Identifying useful subgoals in reinforcement learning by local graph partitioning.pdf:pdf},
isbn = {1595931805},
journal = {Proceedings of the 22nd international conference on Machine learning  - ICML '05},
pages = {816--823},
title = {{Identifying useful subgoals in reinforcement learning by local graph partitioning}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102454},
year = {2005}
}
@article{Deisenroth2011,
abstract = {Policy search is a subfield in reinforcement learning which focuses on finding good parameters for a given policy parametrization. It is well suited for robotics as it can cope with high-dimensional state and action spaces, one of the main challenges in robot learning. We review recent successes of both model-free and model-based policy search in robot learning. Model-free policy search is a general approach to learn policies based on sampled trajectories. We classify model-free methods based on their policy evaluation strategy, policy update strategy, and exploration strategy and present a unified view on existing algorithms. Learning a policy is often easier than learning an accurate forward model, and, hence, model-free methods are more frequently used in practice. However, for each sampled trajectory, it is necessary to interact with the robot, which can be time consuming and challenging in practice. Model-based policy search addresses this problem by first learning a simulator of the robot's dynamics from data. Subsequently, the simulator generates trajectories that are used for policy learning. For both model-free and model-based policy search methods, we review their respective properties and their applicability to robotic systems.},
author = {Deisenroth, Marc Peter and Neumann, Gerhard and Peters, Jan},
doi = {10.1561/2300000021},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Deisenroth, Neumann, Peters/2011/A Survey on Policy Search for Robotics.pdf:pdf},
isbn = {9781601987037},
issn = {1935-8253},
journal = {Foundations and Trends in Robotics},
keywords = {Artificial Intelligence in Robotics,Markov Decision Processes,Planning and Control,Policy Search},
number = {1-2},
pages = {1--142},
title = {{A Survey on Policy Search for Robotics}},
url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-robotics/ROB-021},
volume = {2},
year = {2011}
}
@article{Yang2004,
author = {Yang, S.X. and Luo, Chaomin},
doi = {10.1109/TSMCB.2003.811769},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Yang, Luo/2004/A Neural Network Approach to Complete Coverage Path Planning.pdf:pdf},
issn = {1083-4419},
journal = {IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics)},
month = {feb},
number = {1},
pages = {718--724},
title = {{A Neural Network Approach to Complete Coverage Path Planning}},
url = {http://ieeexplore.ieee.org/document/1262545/},
volume = {34},
year = {2004}
}
@incollection{Ponsen2010,
abstract = {In this paper we survey the basics of reinforcement learning, gener- alization and abstraction. We start with an introduction to the fundamentals of reinforcement learning and motivate the necessity for generalization and abstrac- tion. Next we summarize themost important techniques available to achieve both generalization and abstraction in reinforcement learning. We discuss basic func- tion approximation techniques and delve into hierarchical, relational and transfer learning. All concepts and techniques are illustrated with examples.},
author = {Ponsen, Marc and Taylor, Matthew E. and Tuyls, Karl},
booktitle = {Adaptive and Learning Agents},
doi = {10.1007/978-3-642-11814-2_1},
editor = {Taylor, Matthew E. and Tuyls, Karl},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Ponsen, Taylor, Tuyls/2010/Abstraction and generalization in reinforcement learning A summary and framework.pdf:pdf},
isbn = {3642118135},
issn = {03029743},
pages = {1--32},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Abstraction and generalization in reinforcement learning: A summary and framework}},
year = {2010}
}
@article{Li2013,
abstract = {Pulse Coupled Neural Network (PCNN) is suitable for dealing with the classical shortest path problem, because of its autowave characteristic. However, most methods suggest that the autowave of PCNN models should keep a constant speed in finding the shortest paths. This paper proposes a novel self-adaptive autowave pulse-coupled neural network (SAPCNN) model for the shortest path problem. The autowave generated by SAPCNN propagates adaptively according to the current network state, which guarantees it spreads more effectively in finding the shortest paths. Our experiments, which have been carried out for both the shortest paths problem and K shortest paths problem, show that our proposed algorithm outperforms classical algorithms. ?? 2013 Elsevier B.V.},
author = {Li, Xiaojun and Ma, Yide and Feng, Xiaowen},
doi = {10.1016/j.neucom.2012.12.030},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Li, Ma, Feng/2013/Self-adaptive autowave pulse-coupled neural network for shortest-path problem.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Autowave,Pulse-coupled neural network,Self-adaptive autowave PCNN,Shortest path},
pages = {63--71},
publisher = {Elsevier},
title = {{Self-adaptive autowave pulse-coupled neural network for shortest-path problem}},
url = {http://dx.doi.org/10.1016/j.neucom.2012.12.030},
volume = {115},
year = {2013}
}
