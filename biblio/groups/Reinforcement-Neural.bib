Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Levine2017,
abstract = {Deep reinforcement learning (DRL) methods such as the Deep Q-Network (DQN) have achieved state-of-the-art results in a variety of challenging, high-dimensional domains. This success is mainly attributed to the power of deep neural networks to learn rich domain representations for approximating the value function or policy. Batch reinforcement learning methods with linear representations, on the other hand, are more stable and require less hyper parameter tuning. Yet, substantial feature engineering is necessary to achieve good results. In this work we propose a hybrid approach -- the Least Squares Deep Q-Network (LS-DQN), which combines rich feature representations learned by a DRL algorithm with the stability of a linear least squares method. We do this by periodically re-training the last hidden layer of a DRL network with a batch least squares update. Key to our approach is a Bayesian regularization term for the least squares update, which prevents over-fitting to the more recent data. We tested LS-DQN on five Atari games and demonstrate significant improvement over vanilla DQN and Double-DQN. We also investigated the reasons for the superior performance of our method. Interestingly, we found that the performance improvement can be attributed to the large batch size used by the LS method when optimizing the last layer.},
archivePrefix = {arXiv},
arxivId = {1705.07461},
author = {Levine, Nir and Zahavy, Tom and Mankowitz, Daniel J. and Tamar, Aviv and Mannor, Shie},
eprint = {1705.07461},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Levine et al/Levine et al. - 2017 - Shallow Updates for Deep Reinforcement Learning.pdf:pdf},
issn = {10495258},
number = {Nips 2017},
pages = {1--14},
title = {{Shallow Updates for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1705.07461},
year = {2017}
}
@inproceedings{Ghadirzadeh2017,
abstract = {Skilled robot task learning is best implemented by predictive action policies due to the inherent latency of sensorimotor processes. However, training such predictive policies is challenging as it involves finding a trajectory of motor activations for the full duration of the action. We propose a data-efficient deep predictive policy training (DPPT) framework with a deep neural network policy architecture which maps an image observation to a sequence of motor activations. The architecture consists of three sub-networks referred to as the perception, policy and behavior super-layers. The perception and behavior super-layers force an abstraction of visual and motor data trained with synthetic and simulated training samples, respectively. The policy super-layer is a small sub-network with fewer parameters that maps data in-between the abstracted manifolds. It is trained for each task using methods for policy search reinforcement learning. We demonstrate the suitability of the proposed architecture and learning framework by training predictive policies for skilled object grasping and ball throwing on a PR2 robot. The effectiveness of the method is illustrated by the fact that these tasks are trained using only about 180 real robot attempts with qualitative terminal rewards.},
archivePrefix = {arXiv},
arxivId = {1703.00727},
author = {Ghadirzadeh, Ali and Maki, Atsuto and Kragic, Danica and Bjorkman, Marten},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2017.8206046},
eprint = {1703.00727},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IEEE International Conference on Intelligent Robots and Systems/Ghadirzadeh et al/Ghadirzadeh et al. - 2017 - Deep predictive policy training using reinforcement learning.pdf:pdf},
isbn = {9781538626825},
issn = {21530866},
pages = {2351--2358},
title = {{Deep predictive policy training using reinforcement learning}},
year = {2017}
}
@article{Henderson2017,
abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
archivePrefix = {arXiv},
arxivId = {1709.06560},
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
eprint = {1709.06560},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Henderson et al/Henderson et al. - 2017 - Deep Reinforcement Learning that Matters.pdf:pdf},
title = {{Deep Reinforcement Learning that Matters}},
url = {http://arxiv.org/abs/1709.06560},
year = {2017}
}
@article{Osband2016,
abstract = {Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.},
archivePrefix = {arXiv},
arxivId = {1602.04621},
author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and {Van Roy}, Benjamin},
doi = {10.1145/2661829.2661935},
eprint = {1602.04621},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Osband et al/Osband et al. - 2016 - Deep Exploration via Bootstrapped DQN.pdf:pdf},
isbn = {9781479969227},
issn = {10495258},
pages = {1--18},
pmid = {15003161},
title = {{Deep Exploration via Bootstrapped DQN}},
url = {http://arxiv.org/abs/1602.04621},
year = {2016}
}
@article{Parisotto2017,
abstract = {A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training.},
archivePrefix = {arXiv},
arxivId = {1702.08360},
author = {Parisotto, Emilio and Salakhutdinov, Ruslan},
eprint = {1702.08360},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Parisotto, Salakhutdinov/Parisotto, Salakhutdinov - 2017 - Neural Map Structured Memory for Deep Reinforcement Learning.pdf:pdf},
issn = {1702.08360},
pages = {1--13},
title = {{Neural Map: Structured Memory for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1702.08360},
year = {2017}
}
@article{Frans2017,
abstract = {We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives---policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.},
archivePrefix = {arXiv},
arxivId = {1710.09767},
author = {Frans, Kevin and Ho, Jonathan and Chen, Xi and Abbeel, Pieter and Schulman, John},
eprint = {1710.09767},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Frans et al/Frans et al. - 2017 - Meta Learning Shared Hierarchies.pdf:pdf},
pages = {1--11},
title = {{Meta Learning Shared Hierarchies}},
url = {http://arxiv.org/abs/1710.09767},
year = {2017}
}
@article{Srouji2018,
abstract = {In recent years, Deep Reinforcement Learning has made impressive advances in solving several important benchmark problems for sequential decision making. Many control applications use a generic multilayer perceptron (MLP) for non-vision parts of the policy network. In this work, we propose a new neural network architecture for the policy network representation that is simple yet effective. The proposed Structured Control Net (SCN) splits the generic MLP into two separate sub-modules: a nonlinear control module and a linear control module. Intuitively, the nonlinear control is for forward-looking and global control, while the linear control stabilizes the local dynamics around the residual of global control. We hypothesize that this will bring together the benefits of both linear and nonlinear policies: improve training sample efficiency, final episodic reward, and generalization of learned policy, while requiring a smaller network and being generally applicable to different training methods. We validated our hypothesis with competitive results on simulations from OpenAI MuJoCo, Roboschool, Atari, and a custom 2D urban driving environment, with various ablation and generalization tests, trained with multiple black-box and policy gradient training methods. The proposed architecture has the potential to improve upon broader control tasks by incorporating problem specific priors into the architecture. As a case study, we demonstrate much improved performance for locomotion tasks by emulating the biological central pattern generators (CPGs) as the nonlinear part of the architecture.},
archivePrefix = {arXiv},
arxivId = {1802.08311},
author = {Srouji, Mario and Zhang, Jian and Salakhutdinov, Ruslan},
eprint = {1802.08311},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Srouji, Zhang, Salakhutdinov/Srouji, Zhang, Salakhutdinov - 2018 - Structured Control Nets for Deep Reinforcement Learning.pdf:pdf},
title = {{Structured Control Nets for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1802.08311},
year = {2018}
}
@article{Russell2018,
abstract = {In recent years, deep generative models have been shown to ‘imagine' convincing high-dimensional observations such as images, audio, and even video, learning directly from raw data. In this work, we ask how to imagine goal-directed visual plans – a plausible sequence of observations that transition a dynamical system from its current configuration to a desired goal state, which can later be used as a reference trajectory for control. We focus on systems with high-dimensional observations, such as images, and propose an approach that naturally combines representation learning and planning. Our framework learns a generative model of sequential observations, where the generative process is induced by a transition in a low-dimensional planning model, and an additional noise. By maximizing the mutual information between the generated observations and the transition in the planning model, we obtain a low-dimensional representation that best explains the causal nature of the data. We structure the planning model to be compatible with efficient planning algorithms, and we propose several such models based on either discrete or continuous states. Finally, to generate a visual plan, we project the current and goal observations onto their respective states in the planning model, plan a trajectory, and then use the generative model to transform the trajectory to a sequence of observations. We demonstrate our method on imagining plausible visual plans of rope manipulation.},
author = {Russell, Stuart and Tamar, Aviv and Berkeley, U C and Abbeel, Pieter},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Russell et al/Russell et al. - 2018 - Learning Plannable Representations with Causal InfoGAN.pdf:pdf},
title = {{Learning Plannable Representations with Causal InfoGAN}},
year = {2018}
}
@article{Li2017a,
abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.},
archivePrefix = {arXiv},
arxivId = {1701.07274},
author = {Li, Yuxi},
doi = {10.1007/978-3-319-56991-8_32},
eprint = {1701.07274},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Li/Li - 2017 - Deep Reinforcement Learning An Overview.pdf:pdf},
isbn = {9781509011216},
issn = {1701.07274},
pages = {70},
title = {{Deep Reinforcement Learning: An Overview}},
url = {http://arxiv.org/abs/1701.07274},
year = {2017}
}
