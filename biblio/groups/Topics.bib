Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Icarte2018,
author = {Icarte, Rodrigo Toro and Klassen, Toryn Q. and Valenzano, Richard and McIlraith, Sheila A.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Icml/Icarte et al/Icarte et al. - 2018 - Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning.pdf:pdf},
journal = {Icml},
title = {{Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning}},
year = {2018}
}
@article{Gorodetsky2009,
abstract = {В работе рассматривается технология построения прикладных систем группового управления, со- стоящих из большого числа автономных подсистем, организованных в сеть, узлы которой могут работать под управлением различных операционных систем и в различных коммуникационных средах. Технология интегрирует подходы распределенного принятия решений, многоагентных систем, ориентированной на сервис архитектуры и вычислений на основе парных взаимодействий. Технология поддерживается инструментальными средствами, ко- торые обеспечивают эффективную разработку агентов и механизмов их взаимодействия. Приводятся примеры использования технологии в ряде приложений, в частности, для автономного управления воздушным движением в районе аэропорта.},
author = {Городецкий, В. И. and Карсаев, О. В. and Самойлов, В. В. and Серебряков, С. В.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Искусственный интеллект и принятие решений/Городецкий et al/Городецкий et al. - 2009 - Прикладные многоагентные системы группового управления.pdf:pdf},
journal = {Искусственный интеллект и принятие решений},
language = {russian},
number = {2},
pages = {3--24},
title = {{Прикладные многоагентные системы группового управления}},
year = {2009}
}
@inproceedings{Barto2004a,
abstract = {Humans and other animals often engage in activities for their own sakes rather than as steps toward solving practical problems. Psychologists call these intrinsically motivated behaviors. What we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy. At the core of the model are recent theoretical and algorithmic advances in computational reinforcement learning, specifically, new concepts related to skills and new learning algorithms for learning with skill hierarchies. 1},
author = {Barto, Andrew G. and Singh, Satinder},
booktitle = {Proceedings of the 3rd International Conference on Development and Learning (ICDL 2004)},
doi = {10.1.1.123.395},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 3rd International Conference on Development and Learning (ICDL 2004)/Barto, Singh/Barto, Singh - 2004 - Intrinsically motivated learning of hierarchical collections of skills.pdf:pdf},
pages = {112--119},
title = {{Intrinsically motivated learning of hierarchical collections of skills}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.117.6436{\%}5Cnhttp://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A083E47D2FE080D11716EC249E464BE2?doi=10.1.1.117.6436{\&}rep=rep1{\&}type=pdf},
year = {2004}
}
@article{Cox1999,
abstract = {A central problem in multistrategy learning systems is the selection and sequencing of machine learning algorithms for particular situations. This is typically done by the system designer who analyzes the learning task and implements the appropriate algorithm or sequence of algorithms for that task. A solution to this problem is proposed to enable an AI system with a library of machine learning algorithms to select and sequence appropriate algorithms autonomously.},
author = {Cox, Michael T. and Ashwin, Ram},
doi = {10.1016/S0004-3702(99)00047-8},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Artificial Intelligence/Cox, Ashwin/Cox, Ashwin - 1999 - Introspective multistrategy learning On the construction of learning strategies.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
number = {1},
pages = {1--55},
title = {{Introspective multistrategy learning: On the construction of learning strategies}},
volume = {112},
year = {1999}
}
@article{Al-Emran2015,
author = {Al-Emran, Mostafa},
doi = {10.12785/ijcds/040207},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/International Journal of Computing and Digital Systems/Al-Emran/Al-Emran - 2015 - Hierarchical Reinforcement Learning - A Survey.pdf:pdf},
issn = {2210-142X},
journal = {International Journal of Computing and Digital Systems},
keywords = {hierarchical reinforcement learning,q-learning,reinforcement learning},
month = {apr},
number = {2},
pages = {137--143},
title = {{Hierarchical Reinforcement Learning - A Survey}},
url = {https://journal.journals.uob.edu.bh//Article/ArticleFile/2144},
volume = {4},
year = {2015}
}
@inproceedings{Roncone2017,
abstract = {Collaborative robots represent a clear added value to manufacturing, as they promise to increase productivity and improve working conditions of such environments. Although modern robotic systems have become safe and reliable enough to operate close to human workers on a day-to-day basis, the workload is still skewed in favor of a limited contribution from the robot's side, and a significant cognitive load is allotted to the human. We believe the transition from robots as recipients of human instruction to robots as capable collaborators hinges around the implementation of transparent systems, where mental models about the task are shared between peers, and the human partner is freed from the responsibility of taking care of both actors. In this work, we implement a transparent task planner able to be deployed in realistic, near-future applications. The proposed framework is capable of basic reasoning capabilities for what concerns role assignment and task allocation, and it interfaces with the human partner at the level of abstraction he is most comfortable with. The system is readily available to non-expert users, and programmable with high-level commands in an intuitive interface. Our results demonstrate an overall improvement in terms of completion time, as well as a reduced cognitive load for the human partner.},
author = {Roncone, Alessandro and Mangin, Olivier and Scassellati, Brian},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2017.7989122},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings - IEEE International Conference on Robotics and Automation/Roncone, Mangin, Scassellati/Roncone, Mangin, Scassellati - 2017 - Transparent role assignment and task allocation in human robot collabo.pdf:pdf},
isbn = {9781509046331},
issn = {10504729},
keywords = {Human Factors and H,Social Human-Robot Interaction},
pages = {1014--1021},
publisher = {IEEE},
title = {{Transparent role assignment and task allocation in human robot collaboration}},
year = {2017}
}
@article{Abel2018,
author = {Abel, David and Arumugam, Dilip and Lehnert, Lucas and Littman, Michael L.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 35th International Conference on Machine Learning/Abel et al/Abel et al. - 2018 - State Abstractions for Lifelong Reinforcement Learning.jpg:jpg;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 35th International Conference on Machine Learning/Abel et al/Abel et al. - 2018 - State Abstractions for Lifelong Reinforcement Learning.pdf:pdf},
journal = {Proceedings of the 35th International Conference on Machine Learning},
title = {{State Abstractions for Lifelong Reinforcement Learning}},
year = {2018}
}
@article{Dehaene2017,
author = {Dehaene, Stanislas and Lau, Hakwan and Kouider, Sid},
doi = {10.1126/science.aan8871},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Science/Dehaene, Lau, Kouider/Dehaene, Lau, Kouider - 2017 - What is consciousness, and could machines have it.pdf:pdf},
issn = {0036-8075},
journal = {Science},
month = {oct},
number = {6362},
pages = {486--492},
pmid = {29074769},
title = {{What is consciousness, and could machines have it?}},
url = {http://www.sciencemag.org/lookup/doi/10.1126/science.aan8871},
volume = {358},
year = {2017}
}
@article{Oakley2017,
author = {Oakley, David A. and Halligan, Peter W.},
doi = {10.3389/fpsyg.2017.01924},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Frontiers in Psychology/Oakley, Halligan/Oakley, Halligan - 2017 - Chasing the Rainbow The Non-conscious Nature of Being.pdf:pdf},
isbn = {1664-1078},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {agency,consciousness,evolu,evolution,social,suggestion,volition},
month = {nov},
pmid = {29184516},
title = {{Chasing the Rainbow: The Non-conscious Nature of Being}},
url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2017.01924/full},
volume = {8},
year = {2017}
}
@article{Co-Reyes2018,
abstract = {In this work, we take a representation learning perspective on hierarchical reinforcement learning, where the problem of learning lower layers in a hierarchy is transformed into the problem of learning trajectory-level generative models. We show that we can learn continuous latent representations of trajectories, which are effective in solving temporally extended and multi-stage problems. Our proposed model, SeCTAR, draws inspiration from variational autoencoders, and learns latent representations of trajectories. A key component of this method is to learn both a latent-conditioned policy and a latent-conditioned model which are consistent with each other. Given the same latent, the policy generates a trajectory which should match the trajectory predicted by the model. This model provides a built-in prediction mechanism, by predicting the outcome of closed loop policy behavior. We propose a novel algorithm for performing hierarchical RL with this model, combining model-based planning in the learned latent space with an unsupervised exploration objective. We show that our model is effective at reasoning over long horizons with sparse rewards for several simulated tasks, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration.},
archivePrefix = {arXiv},
arxivId = {1806.02813},
author = {Co-Reyes, John D. and Liu, YuXuan and Gupta, Abhishek and Eysenbach, Benjamin and Abbeel, Pieter and Levine, Sergey},
eprint = {1806.02813},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Co-Reyes et al/Co-Reyes et al. - 2018 - Self-Consistent Trajectory Autoencoder Hierarchical Reinforcement Learning with Trajectory Embeddings.jpg:jpg;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Co-Reyes et al/Co-Reyes et al. - 2018 - Self-Consistent Trajectory Autoencoder Hierarchical Reinforcement Learning with Trajectory Embeddings.pdf:pdf},
issn = {1938-7228},
title = {{Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings}},
url = {http://arxiv.org/abs/1806.02813},
year = {2018}
}
@inproceedings{Vo2009,
abstract = {Despite the large body of work in both motion planning and multi-agent simulation, little work has focused on the problem of planning motion for groups of robots using external ¿controller¿ agents. We call this problem the group control problem. This problem is complex because it is highly underactuated, dynamic, and requires multi-agent cooperation. In this paper, we present a variety of new motion planning algorithms based on EST, RRT, and PRM methods for shepherds to guide flocks of robots through obstacle-filled environments. We show using simulation on several environments that under certain circumstances, motion planning can find paths that are too complicated for nai¿ve ¿simulation only¿ approaches. However, inconsistent results indicate that this problem is still in need of additional study.},
author = {Vo, Christopher and Harrison, Joseph F. and Lien, Jyh Ming},
booktitle = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2009},
doi = {10.1109/IROS.2009.5354032},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/2009 IEEERSJ International Conference on Intelligent Robots and Systems, IROS 2009/Vo, Harrison, Lien/Vo, Harrison, Lien - 2009 - Behavior-based motion planning for group control.pdf:pdf},
isbn = {9781424438044},
pages = {3768--3773},
title = {{Behavior-based motion planning for group control}},
year = {2009}
}
@inproceedings{Elkawkagy2010,
author = {Elkawkagy, Mohamed and Schattenberg, Bernd and Biundo, Susanne},
booktitle = {ECAI 2010: 19th European Conference on Artificial Intelligence, 16-20 August 2010, Lisbon, Portugal : Including Prestigious Applications of Artificial Intelligence (PAIS-2010) : Proceedings},
doi = {10.3233/978-1-60750-606-5-229},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/ECAI 2010 19th European Conference on Artificial Intelligence, 16-20 August 2010, Lisbon, Portugal Including Prestigious Applications of Ar./Elkawkagy, Schattenberg, Biundo/Elkawkagy, Schattenberg, Biundo -.pdf:pdf},
isbn = {9781607506065},
pages = {229--234},
title = {{Landmarks in Hierarchical Planning}},
year = {2010}
}
@inproceedings{Davoodabadi2011a,
abstract = {In this paper the problem of automatically discovering subtasks and hierarchies in reinforcement learning is considered. We present a novel method that allows an agent to autonomously discover subgoals and create a hierarchy from actions. Our method identifies subgoals by partitioning local state transition graphs. Options constructed for reaching these subgoals are added to action choices and used for accelerating the Q-Learning algorithm. Experimental results show significant performance improvements, especially in the initial learning phase.},
author = {Davoodabadi, M and Beigy, H},
booktitle = {Proceedings of the 5th Indian International Conference on Artificial Intelligence, IICAI 2011},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 5th Indian International Conference on Artificial Intelligence, IICAI 2011/Davoodabadi, Beigy/Davoodabadi, Beigy - 2011 - A new method for discovering subgoals and constructing options in r.pdf:pdf},
isbn = {9780972741286},
keywords = {Artificial intelligence,Autonomously discovering subgoals,Community detection,Hierarchical reinforcement learning,Learning algorithms,Learning phase,Local state,Option,Performance improvements,Q-learning algorithms,Reinforcement learning,Subgoals,Subtasks},
pages = {441--450},
title = {{A new method for discovering subgoals and constructing options in reinforcement learning}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84872195092{\&}partnerID=40{\&}md5=9bf17b3f8f09d77ec8cd05285124028d},
year = {2011}
}
@article{Spratling2017,
author = {Spratling, M. W.},
doi = {10.1007/s12559-016-9445-1},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Cognitive Computation/Spratling/Spratling - 2017 - A Hierarchical Predictive Coding Model of Object Recognition in Natural Images.pdf:pdf},
issn = {18669964},
journal = {Cognitive Computation},
keywords = {Deep neural networks,Implicit shape model,Neural networks,Object recognition,Predictive coding,Sparse coding},
number = {2},
pages = {151--167},
publisher = {Cognitive Computation},
title = {{A Hierarchical Predictive Coding Model of Object Recognition in Natural Images}},
volume = {9},
year = {2017}
}
@inproceedings{Jonsson2001a,
abstract = {Learning a complex task can be significantly facilitated by defining a hierarchy of subtasks. An agent can learn to choose between various temporally abstract actions, each solving an assigned subtask, to accomplish the overall task. In this paper, we study hierarchical learning using the framework of options. We argue that to take full advantage of hierarchical structure, one should perform option-specific state abstraction, and that if this is to scale to larger tasks, state abstraction ...},
author = {Jonsson, Anders and Barto, Andrew G.},
booktitle = {Proceedings of NIPS 2001},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of NIPS 2001/Jonsson, Barto/Jonsson, Barto - 2001 - Automated State Abstraction for Options using the U-Tree Algorithm.pdf:pdf},
isbn = {0262122413},
issn = {1049-5258},
pages = {1054--1060},
title = {{Automated State Abstraction for Options using the U-Tree Algorithm}},
year = {2001}
}
@inproceedings{Paxton2017a,
abstract = {We consider task and motion planning in complex dynamic environments for problems expressed in terms of a set of Linear Temporal Logic (LTL) constraints, and a reward function. We propose a methodology based on reinforcement learning that employs deep neural networks to learn low-level control policies as well as task-level option policies. A major challenge in this setting, both for neural network approaches and classical planning, is the need to explore future worlds of a complex and interactive environment. To this end, we integrate Monte Carlo Tree Search with hierarchical neural net control policies trained on expressive LTL specifications. This paper investigates the ability of neural networks to learn both LTL constraints and control policies in order to generate task plans in complex environments. We demonstrate our approach in a simulated autonomous driving setting, where a vehicle must drive down a road in traffic, avoid collisions, and navigate an intersection, all while obeying given rules of the road.},
archivePrefix = {arXiv},
arxivId = {1703.07887},
author = {Paxton, Chris and Raman, Vasumathi and Hager, Gregory D. and Kobilarov, Marin},
booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2017.8206505},
eprint = {1703.07887},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/2017 IEEERSJ International Conference on Intelligent Robots and Systems (IROS)/Paxton et al/Paxton et al. - 2017 - Combining Neural Networks and Tree Search for Task and Motion Planning in Challenging Environ.pdf:pdf},
isbn = {978-0-12-084802-7},
keywords = {Deep Learning in Robotics and Automa,Task Planning},
pages = {6059--6066},
publisher = {IEEE},
title = {{Combining Neural Networks and Tree Search for Task and Motion Planning in Challenging Environments}},
url = {http://ieeexplore.ieee.org/document/8206505/{\%}0Ahttp://arxiv.org/abs/1703.07887},
year = {2017}
}
@inproceedings{Serrano2012,
abstract = {We propose a novel method for assessing the reputation of agents in multiagent systems that is capable of exploiting the structure and semantics of rich agent interaction protocols and agent communication languages. Our method is based on using so-called conversation models, i.e. succinct, qualitative models of agents' behaviours derived from the application of data mining techniques on protocol execution data in a way that takes advantage of the semantics of interagent communication available in many multiagent systems. Contrary to existing systems, which only allow for querying agents regarding their assessment of others' reputation in an outcome-based way (often limited to distinguishing between "successful" and "unsuccessful" interactions), our method allows for contextualised queries regarding the structure of past interactions, the values of content variables, and the behaviour of agents across different protocols. Moreover, this is achieved while preserving maximum privacy for the reputation querying agent and the witnesses queried, and without requiring a common definition of reputation, trust or reliability among the agents exchanging reputation information. A case study shows that, even with relatively simple reputation measures, our qualitative method outperforms quantitative approaches, proving that we can meaningfully exploit the additional information afforded by rich interaction protocols and agent communication semantics. Copyright {\textcopyright} 2012, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.},
author = {Serrano, Emilio and Rovatsos, Michael and Botia, Juan},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems/Serrano, Rovatsos, Botia/Serrano, Rovatsos, Botia - 2012 - A qualitative reputation system for multiagent systems w.pdf:pdf},
keywords = {agent communication,data mining,trust and reputation},
pages = {307--314},
title = {{A qualitative reputation system for multiagent systems with protocol-based communication}},
volume = {1},
year = {2012}
}
@article{Precup1998,
abstract = {Planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence. In this paper we summarize an ap-proach to this problem based on the mathematical framework of Markov decision processes and reinforcement learning. Current model-based re-inforcement learning is based on one-step models that cannot represent common-sense higher-level actions, such as going to lunch, grasping an object, or flying to Denver. This paper generalizes prior work on tem-porally abstract models [Sutton, 1995] and extends it from the prediction setting to include actions, control, and planning. We introduce a more general form of temporally abstract model, the multi-time model, and es-tablish its suitability for planning and learning by virtue of its relationship to the Bellman equations. This paper summarizes the theoretical frame-work of multi-time models and illustrates their potential advantages in a grid world planning task. The need for hierarchical and abstract planning is a fundamental problem in AI (see, e.g., Sacerdoti, 1977; Laird et aI., 1986; Korf, 1985; Kaelbling, 1993; Dayan {\&} Hinton, 1993). Model-based reinforcement learning offers a possible solution to the problem of integrating planning with real-time learning and decision-making (Peng {\&} Williams, 1993, Moore {\&} Atkeson, 1993; Sutton and Barto, 1998). However, current model-based reinforcement learning is based on one-step models that cannot represent common-sense, higher-level actions. Modeling such actions requires the ability to handle different, interrelated levels of temporal abstraction.},
author = {Precup, Doina and Sutton, Rs and Singh, S},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Advances in Neural Information Processing Systems/Precup, Sutton, Singh/Precup, Sutton, Singh - 1998 - Multi-time models for temporally abstract planning.pdf:pdf},
isbn = {0-262-10076-2},
issn = {1049-5258},
journal = {Advances in Neural Information Processing Systems},
number = {1995},
pages = {1050--1056},
title = {{Multi-time models for temporally abstract planning}},
url = {http://www.researchgate.net/publication/221619262{\_}Multi-time{\_}Models{\_}for{\_}Temporally{\_}Abstract{\_}Planning/file/d912f506316e202652.pdf},
volume = {10},
year = {1998}
}
@article{Held2017,
abstract = {Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.},
archivePrefix = {arXiv},
arxivId = {1705.06366},
author = {Held, David and Geng, Xinyang and Florensa, Carlos and Abbeel, Pieter},
doi = {arXiv:1705.06366v3},
eprint = {1705.06366},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Held et al/Held et al. - 2017 - Automatic Goal Generation for Reinforcement Learning Agents.pdf:pdf},
isbn = {076453601X},
keywords = {Reinforcement Learning, Automatic Curriculum, Mult},
title = {{Automatic Goal Generation for Reinforcement Learning Agents}},
url = {http://arxiv.org/abs/1705.06366},
year = {2017}
}
@article{Khardon1999,
abstract = {We introduce a new framework for the study of reasoning. The Learning (in order) to Reason approach developed here views learning as an integral part of the inference process, and suggests that learning and reasoning should be studied together. The Learning to Reason framework combines the interfaces to the world used by known learning models with the reasoning task and a performance criterion suitable for it. In this framework, the intelligent agent is given access to its favorite learning interface, and is also given a grace period in which it can interact with this interface and construct a representation KB of the world W. The reasoning performance is measured only after this period, when the agent is presented with queries a from some query language, relevant to the world, and has to answer whether W implies a. The approach is meant to overcome the main computational difficulties in the traditional treatment of reasoning which stem from its separation from the “world”. Since the agent interacts with the world when constructing its knowledge representation it can choose a representation that is useful for the task at hand. Moreover, we can now make explicit the dependence of the reasoning performance on the environment the agent interacts with. We show how previous results from learning theory and reasoning fit into this framework and illustrate the usefulness of the Learning to Reason approach by exhibiting new results that are not possible in the traditional setting. First, we give Learning to Reason algorithms for classes of propositional languages for which there are no efficient reasoning algorithms, when represented as a traditional (formula-based) knowledge base. Second, we exhibit a Learning to Reason algorithm for a class of propositional languages that is not known to be learnable in the traditional sense.},
author = {Khardon, Roni and Roth, Dan},
doi = {10.1023/A:1007581123604},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Machine Learning/Khardon, Roth/Khardon, Roth - 1999 - Learning to reason with a restricted view.pdf:pdf},
isbn = {0-262-61102-3},
issn = {08856125},
journal = {Machine Learning},
keywords = {computational learning theory,partial assignments,reasoning},
number = {2},
pages = {95--116},
title = {{Learning to reason with a restricted view}},
volume = {35},
year = {1999}
}
@article{Huynh2006,
abstract = {Abstract Trust and reputation are central to effective interactions in open multi-agent systems (MAS) in which agents, that are owned by a variety of stakeholders, continuously enter and leave the system. This openness means existing trust and reputation models cannot readily be used since their performance suffers when there are various (unforseen) changes in the environment. To this end, this paper presents FIRE, a trust and reputation model that integrates a number of information sources to produce a comprehensive assessment of an agents likely performance in open systems. Specifically, FIRE incorporates interaction trust, role-based trust, witness reputation, and certified reputation to provide trust metrics in most circumstances. FIRE is empirically evaluated and is shown to help agents gain better utility (by effectively selecting appropriate interaction partners) than our benchmarks in a variety of agent populations. It is also shown that FIRE is able to effectively respond to changes that occur in an agents environment.},
author = {Huynh, Trung Dong and Jennings, Nicholas R. and Shadbolt, Nigel R.},
doi = {10.1007/s10458-005-6825-4},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Autonomous Agents and Multi-Agent Systems/Huynh, Jennings, Shadbolt/Huynh, Jennings, Shadbolt - 2006 - An integrated trust and reputation model for open multi-agent systems.pdf:pdf},
isbn = {1387-2532},
issn = {1387-2532},
journal = {Autonomous Agents and Multi-Agent Systems},
keywords = {Multi-agent systems,Reputation,Trust},
month = {sep},
number = {2},
pages = {119--154},
pmid = {8581365399728019542},
title = {{An integrated trust and reputation model for open multi-agent systems}},
url = {http://link.springer.com/10.1007/s10458-005-6825-4},
volume = {13},
year = {2006}
}
@inproceedings{Shu2017a,
abstract = {Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task corresponds to a human language description. Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices. In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills. We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.},
archivePrefix = {arXiv},
arxivId = {1712.07294},
author = {Shu, Tianmin and Xiong, Caiming and Socher, Richard},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
doi = {10.1051/0004-6361/201527329},
eprint = {1712.07294},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Shu, Xiong, Socher/Shu, Xiong, Socher - 2017 - Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pmid = {23459267},
title = {{Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning}},
url = {http://arxiv.org/abs/1712.07294},
year = {2017}
}
@article{DeWolf2016,
abstract = {We present a spiking neuron model of the motor cortices and cerebellum of the motor control system. The model consists of anatomically organized spiking neurons encompassing premotor, primary motor, and cerebellar cortices. The model proposes novel neural computations within these areas to control a nonlinear three-link arm model that can adapt to unknown changes in arm dynamics and kinematic structure. We demonstrate the mathematical stability of both forms of adaptation, suggesting that this is a robust approach for common biological problems of changing body size (e.g. during growth), and unexpected dynamic perturbations (e.g. when moving through different media, such as water or mud). To demonstrate the plausibility of the proposed neural mechanisms, we show that the model accounts for data across 19 studies of the motor control system. These data include a mix of behavioural and neural spiking activity, across subjects performing adaptive and static tasks. Given this proposed characterization of the biological processes involved in motor control of the arm, we provide several experimentally testable predictions that distinguish our model from previous work.},
author = {DeWolf, Travis and Stewart, Terrence C. and Slotine, Jean-Jacques and Eliasmith, Chris},
doi = {10.1098/rspb.2016.2134},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the Royal Society B Biological Sciences/DeWolf et al/DeWolf et al. - 2016 - A spiking neural model of adaptive arm control.pdf:pdf},
issn = {0962-8452},
journal = {Proceedings of the Royal Society B: Biological Sciences},
keywords = {computational biology,neuroscience},
month = {nov},
number = {1843},
pages = {20162134},
pmid = {27903878},
title = {{A spiking neural model of adaptive arm control}},
url = {http://rspb.royalsocietypublishing.org/lookup/doi/10.1098/rspb.2016.2134},
volume = {283},
year = {2016}
}
@inproceedings{Kuzmin2018a,
address = {Ростов-на-Дону},
author = {Кузьмин, В. Ф.},
booktitle = {Информатика, управление и системный анализ: Труды V Всероссийской научной конференции молодых учёных с международным участием},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Информатика, управление и системный анализ Труды V Всероссийской научной конференции молодых учёных с международным участием/Кузьмин/Кузьмин - 2018 - Нейронные сети в иерархическом обучении с подкреплением.pdf:pdf},
keywords = {17-29-07079},
language = {russian},
mendeley-tags = {17-29-07079},
pages = {179--188},
publisher = {Мини-Тайп},
title = {{Нейронные сети в иерархическом обучении с подкреплением}},
year = {2018}
}
@article{Smolensky2014,
abstract = {Mental representations have continuous as well as discrete, combinatorial aspects. For example, while predominantly discrete, phonological representations also vary continuously, as evidenced by instrumental studies of both grammatically-­‐‑induced sound alternations and speech errors. Can an integrated theoretical framework address both aspects of structure? The framework we introduce here, Gradient Symbol Processing, characterizes the emergence of grammatical macrostructure from the Parallel Distributed Processing microstructure (McClelland {\&} Rumelhart, 1986) of language processing. The mental representations that emerge, Distributed Symbol Systems, have both combinatorial and gradient structure. They are processed through Subsymbolic Optimization-­‐‑Quantization, in which an optimization process favoring representations that satisfy well-­‐‑formedness constraints operates in parallel with a distributed quantization process favoring discrete symbolic structures. We apply a particular instantiation of this framework, $\lambda$-­‐‑ Diffusion Theory, to phonological production. Simulations of the resulting model suggest that Gradient Symbol Processing offers a way to unify accounts of discrete grammatical competence with both discrete and continuous patterns in language performance.},
author = {Smolensky, Paul and Goldrick, Matthew and Mathis, Donald},
doi = {10.1111/cogs.12047},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Cognitive Science/Smolensky, Goldrick, Mathis/Smolensky, Goldrick, Mathis - 2014 - Optimization and quantization in gradient symbol systems A framework for integrating the continuous.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Combinatorial structure,Distributed representation,Harmonic grammar,Optimization,Selection,Speech errors},
number = {6},
pages = {1102--1138},
pmid = {23802807},
title = {{Optimization and quantization in gradient symbol systems: A framework for integrating the continuous and the discrete in cognition}},
volume = {38},
year = {2014}
}
@unpublished{Sutton2016,
author = {Sutton, Richard S and Barto, Andrew G},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Sutton, Barto/Sutton, Barto - 2017 - Reinforcement learning An introduction.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Sutton, Barto/Sutton, Barto - 2017 - Reinforcement learning An introduction(2).pdf:pdf},
pages = {445},
title = {{Reinforcement learning: An introduction}},
year = {2017}
}
@article{Cao2012a,
abstract = {We describe an approach to incorporating Bayesian priors in the MAXQ framework for hierarchical reinforcement learning (HRL). We define priors on the primitive environment model and on task pseudo-rewards. Since models for composite tasks can be complex, we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy. We show empirically that (i) our approach results in improved convergence over non-Bayesian baselines, (ii) using both task hierarchies and Bayesian priors is better than either alone, (iii) taking advantage of the task hierarchy reduces the computational cost of Bayesian reinforcement learning and (iv) in this framework, task pseudo-rewards can be learned instead of being manually specified, leading to hierarchically optimal rather than recursively optimal policies.},
author = {Cao, F and Ray, Soumya},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Advances in Neural Information Processing Systems/Cao, Ray/Cao, Ray - 2012 - Bayesian hierarchical reinforcement learning.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {73--81},
title = {{Bayesian hierarchical reinforcement learning}},
url = {http://papers.nips.cc/paper/4752-bay},
year = {2012}
}
@article{Dayan1993,
abstract = {One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time. This paper shows how to create a Q-learning managerial hierarchy in which high level managers learn how to set tasks to their sub-managers who, in turn, learn how to satisfy them. Sub-managers need not initially understand their managers' commands. They simply learn to maximise their reinforcement in the context of the current command. We illustrate the system using a simple maze task.. As the system learns how to get around, satisfying commands at the multiple levels, it explores more efficiently than standard, flat, Q-learning and builds a more comprehensive map.},
author = {Dayan, Peter and Hinton, Geoffrey},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Advances in neural information processing systems/Dayan, Hinton/Dayan, Hinton - 1993 - Feudal Reinforcement Learning.pdf:pdf},
isbn = {1-55860-274-7},
issn = {1049-5258},
journal = {Advances in neural information processing systems},
keywords = {Reinforcement Learning},
pages = {271--278},
title = {{Feudal Reinforcement Learning}},
url = {http://www.cs.utoronto.ca/{~}hinton/absps/dh93.pdf},
year = {1993}
}
@article{Hayes2016b,
abstract = {Collaboration between humans and robots requires solutions to an array of challenging problems, including multi-agent planning, state estimation, and goal inference. There already exist feasible solutions for many of these challenges, but they depend upon having rich task models. In this work we detail a novel type of Hierarchical Task Network we call a Clique/Chain HTN (CC-HTN), alongside an algorithm for autonomously constructing them from topological properties derived from graphical task representations. As the presented method relies on the structure of the task itself, our work imposes no particular type of symbolic insight into motor primitives or environmental representation, making it applicable to a wide variety of use cases critical to human-robot interaction. We present evaluations within a multi-resolution goal inference task and a transfer learning application showing the utility of our approach.},
author = {Hayes, Bradley and Scassellati, Brian},
doi = {10.1109/ICRA.2016.7487760},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings - IEEE International Conference on Robotics and Automation/Hayes, Scassellati/Hayes, Scassellati - 2016 - Autonomously constructing hierarchical task networks for planning and human-robot collabor.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {5469--5476},
title = {{Autonomously constructing hierarchical task networks for planning and human-robot collaboration}},
volume = {2016-June},
year = {2016}
}
@inproceedings{MacGlashan2010a,
abstract = {We present skill bootstrapping, a proposed new research direction for agent learning and planning that allows an agent to start with low-level primitive actions, and develop skills that can be used for higher-level planning. Skills are developed over the course of solving many different problems in a domain, using reinforcement learning techniques to complement the benefits and disadvantages of heuristic-search planning. We describe the overall architecture of the proposed approach, discuss how it relates to other work, and give motivating examples for why this approach would be successful.},
author = {MacGlashan, James and DesJArdins, Marie},
booktitle = {24th AAAI Conference on Artificial Intelligence},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/24th AAAI Conference on Artificial Intelligence/MacGlashan, DesJArdins/MacGlashan, DesJArdins - 2010 - Hierarchical Skill Learning for High-Level Planning.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/24th AAAI Conference on Artificial Intelligence/MacGlashan, DesJArdins/MacGlashan, DesJArdins - 2010 - Hierarchical Skill Learning for High-Level Planning.pdf:pdf},
isbn = {9781577354666},
keywords = {abstraction,approximation,planning,reinforcement learning},
pages = {1988--1989},
title = {{Hierarchical Skill Learning for High-Level Planning}},
volume = {3},
year = {2010}
}
@incollection{Vezhnevets2017,
abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.},
archivePrefix = {arXiv},
arxivId = {1703.01161},
author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
booktitle = {Proceedings of the 34th International Conference on Machine Learning},
editor = {Precup, Doina and Teh, Yee Whye},
eprint = {1703.01161},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 34th International Conference on Machine Learning/Vezhnevets et al/Vezhnevets et al. - 2017 - FeUdal Networks for Hierarchical Reinforcement Learning.pdf:pdf},
issn = {1938-7228},
pages = {3540--3549},
publisher = {PMLR},
title = {{FeUdal Networks for Hierarchical Reinforcement Learning}},
url = {http://arxiv.org/abs/1703.01161},
year = {2017}
}
@article{Donadello2017,
abstract = {Semantic Image Interpretation (SII) is the task of extracting structured semantic descriptions from images. It is widely agreed that the combined use of visual data and background knowledge is of great importance for SII. Recently, Statistical Relational Learning (SRL) approaches have been developed for reasoning under uncertainty and learning in the presence of data and rich knowledge. Logic Tensor Networks (LTNs) are an SRL framework which integrates neural networks with first-order fuzzy logic to allow (i) efficient learning from noisy data in the presence of logical constraints, and (ii) reasoning with logical formulas describing general properties of the data. In this paper, we develop and apply LTNs to two of the main tasks of SII, namely, the classification of an image's bounding boxes and the detection of the relevant part-of relations between objects. To the best of our knowledge, this is the first successful application of SRL to such SII tasks. The proposed approach is evaluated on a standard image processing benchmark. Experiments show that the use of background knowledge in the form of logical constraints can improve the performance of purely data-driven approaches, including the state-of-the-art Fast Region-based Convolutional Neural Networks (Fast R-CNN). Moreover, we show that the use of logical background knowledge adds robustness to the learning system when errors are present in the labels of the training data.},
archivePrefix = {arXiv},
arxivId = {1705.08968},
author = {Donadello, Ivan and Serafini, Luciano and d'Avila Garcez, Artur},
eprint = {1705.08968},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI'17)/Donadello, Serafini, Garcez/Donadello, Serafini, Garcez - 2017 - Logic Tensor Networks for Semantic Image Interpret.pdf:pdf},
journal = {Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI'17)},
keywords = {Machine Learning: Knowledge-based Learning,Robotics and Vision: Vision and Perception,Uncertainty in AI: Uncertainty in AI},
pages = {1596--1602},
title = {{Logic Tensor Networks for Semantic Image Interpretation}},
url = {http://arxiv.org/abs/1705.08968},
year = {2017}
}
@article{George2017,
author = {George, D. and Lehrach, W. and Kansky, K. and L{\'{a}}zaro-Gredilla, M. and Laan, C. and Marthi, B. and Lou, X. and Meng, Z. and Liu, Y. and Wang, H. and Lavin, A. and Phoenix, D. S.},
doi = {10.1126/science.aag2612},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Science/George et al/George et al. - 2017 - A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Science/George et al/George et al. - 2017 - A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs(2).pdf:pdf},
issn = {0036-8075},
journal = {Science},
month = {oct},
number = {October},
pages = {eaag2612},
pmid = {29074582},
title = {{A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs}},
url = {http://www.sciencemag.org/lookup/doi/10.1126/science.aag2612},
volume = {10},
year = {2017}
}
@inproceedings{Eckstein2017,
author = {Eckstein, Maria K and Collins, Anne G E},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Eckstein, Collins/Eckstein, Collins - 2017 - Combining intrinsic motivation and hierarchical reinforcement learning.pdf:pdf},
title = {{Combining intrinsic motivation and hierarchical reinforcement learning}},
url = {https://drive.google.com/file/d/1zjc6q0i2J4JoJ8fnVnQslPRoiiQku3Gl/view},
year = {2017}
}
@article{Botvinick2009a,
abstract = {Research on human and animal behavior has long emphasized its hierarchical structure-the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior.},
author = {Botvinick, Matthew M. and Niv, Yael and Barto, Andrew C.},
doi = {10.1016/j.cognition.2008.08.011},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Cognition/Botvinick, Niv, Barto/Botvinick, Niv, Barto - 2009 - Hierarchically organized behavior and its neural foundations a reinforcement learning perspective.pdf:pdf},
issn = {1873-7838},
journal = {Cognition},
keywords = {Animals,Humans,Models,Nerve Net,Nerve Net: physiology,Prefrontal Cortex,Prefrontal Cortex: physiology,Problem Solving,Problem Solving: physiology,Psychological,Reinforcement (Psychology)},
number = {3},
pages = {262--80},
pmid = {18926527},
title = {{Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2783353{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {113},
year = {2009}
}
@article{Daniel2016a,
abstract = {Tasks that require many sequential decisions are hard to solve using conventional rein- forcement learning algorithms. Based on the option framework, we propose a model which aims to alleviate these concerns. Instead of learning a single monolithic policy, the agent learns a set of simpler sub-policies as well as the initiation and termination probabili- ties for each of those sub-policies. While ex- isting option learning algorithms frequently require manual specification of components such as the sub-policies, we present an algo- rithm which infers all relevant components of the option framework from data. We present results on SMDPs with discrete as well as continuous state-action spaces. The results show that the presented algorithm can combine simple sub-policies to solve complex tasks and can improve learning performance on simpler tasks.},
author = {Daniel, Christian and van Hoof, Herke and Peters, Jan and Neumann, Gerhard},
doi = {10.1007/s10994-016-5580-x},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Machine Learning/Daniel et al/Daniel et al. - 2016 - Probabilistic inference for determining options in reinforcement learning.pdf:pdf},
issn = {15730565},
journal = {Machine Learning},
keywords = {Options,Reinforcement learning,Robot learning,Semi Markov decision process},
number = {2-3},
pages = {337--357},
publisher = {Springer US},
title = {{Probabilistic inference for determining options in reinforcement learning}},
volume = {104},
year = {2016}
}
@inproceedings{Singh2005a,
author = {Singh, Satinder and Barto, Andrew G. and Chentanez, Nuttapong},
booktitle = {Proceeding of NIPS 2005},
doi = {10.1.1.123.395},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceeding of NIPS 2005/Singh, Barto, Chentanez/Singh, Barto, Chentanez - 2005 - Intrinsically motivated reinforcement learning.pdf:pdf},
pages = {1281--1288},
title = {{Intrinsically motivated reinforcement learning}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2005{\_}724.pdf},
year = {2005}
}
@incollection{2018d,
address = {Ростов-на-Дону},
author = {Дайлиденок, И. Д. and Фроленкова, А. И.},
booktitle = {Информатика, управление и системный анализ: Труды V Всероссийской научной конференции молодых учёных с международным участием},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Информатика, управление и системный анализ Труды V Всероссийской научной конференции молодых учёных с международным участием/Дайлиденок, Фроленкова/Дайлиденок, Фроленкова - 2018 - Биологически правдоподобные.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Информатика, управление и системный анализ Труды V Всероссийской научной конференции молодых учёных с международным участием/Дайлиденок, Фроленкова/Дайлиденок, Фроленкова - 2018 - Биологически правдоподоб(2).docx:docx},
keywords = {17-29-07051},
language = {russian},
mendeley-tags = {17-29-07051},
pages = {169--178},
publisher = {Мини-Тайп},
title = {{Биологически правдоподобные нейронные сети для выявления аномалий во временных последовательностях}},
year = {2018}
}
@incollection{Hengst2012a,
abstract = {Hierarchical decomposition tackles complex problems by reducing them to a smaller set of interrelated problems. The smaller problems are solved separately and the results re-combined to find a solution to the original problem. It is well known that the na{\"{i}}ve application of reinforcement learning (RL) techniques fails to scale to more complex domains. This Chapter introduces hierarchical approaches to reinforcement learning that hold out the promise of reducing a reinforcement learning problems to a manageable size. Hierarchical Reinforcement Learning (HRL) rests on finding good re-usable temporally extended actions that may also provide opportunities for state abstraction. Methods for reinforcement learning can be extended to work with abstract states and actions over a hierarchy of subtasks that decompose the original problem, potentially reducing its computational complexity. We use a four-room task as a running example to illustrate the various concepts and approaches, including algorithms that can automatically learn the hierarchical structure from interactions with the domain.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.02971},
author = {Hengst, Bernhard},
booktitle = {Reinforcement Learning},
doi = {10.1007/978-3-642-27645-3_9},
eprint = {arXiv:1509.02971},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Reinforcement Learning/Hengst/Hengst - 2012 - Hierarchical Approaches.pdf:pdf},
isbn = {978-3-642-27644-6},
issn = {18726240},
pages = {293--323},
title = {{Hierarchical Approaches}},
url = {http://dx.doi.org/10.1007/978-3-642-27645-3{\_}9 http://link.springer.com/10.1007/978-3-642-27645-3{\_}9},
year = {2012}
}
@inproceedings{Kumar2017a,
abstract = {We present a framework combining hierarchical and multi-agent deep reinforcement learning approaches to solve coordination problems among a multitude of agents using a semi-decentralized model. The framework extends the multi-agent learning setup by introducing a meta-controller that guides the communication between agent pairs, enabling agents to focus on communicating with only one other agent at any step. This hierarchical decomposition of the task allows for efficient exploration to learn policies that identify globally optimal solutions even as the number of collaborating agents increases. We show promising initial experimental results on a simulated distributed scheduling problem.},
archivePrefix = {arXiv},
arxivId = {1712.08266},
author = {Kumar, Saurabh and Shah, Pararth and Hakkani-Tur, Dilek and Heck, Larry},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
eprint = {1712.08266},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Kumar et al/Kumar et al. - 2017 - Federated Control with Hierarchical Multi-Agent Deep Reinforcement Learning.pdf:pdf},
title = {{Federated Control with Hierarchical Multi-Agent Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1712.08266},
year = {2017}
}
@article{Wang2012b,
abstract = {Reinforcement learning has been an important category of machine learning approaches exhibiting self-learning and online learning characteristics. Using reinforcement learning, an agent can learn its behaviors through trial-and-error interactions with a dynamic environment and finally come up with an optimal strategy. Reinforcement learning suffers the curse of dimensionality, though there has been significant progress to overcome this issue in recent years. MAXQ is one of the most common approaches for reinforcement learning. To function properly, MAXQ requires a decomposition of the agent's task into a task hierarchy. Previously, the decomposition can only be done manually. In this paper, we propose a mechanism for automatic subtask discovery. The mechanism applies clustering to automatically construct task hierarchy required by MAXQ, such that MAXQ can be fully automated. We present the design of our mechanism, and demonstrate its effectiveness through theoretical analysis and an extensive experimental evaluation. {\textcopyright} 2012 IEEE.},
author = {Wang, Hongbing and Li, Wenya and Zhou, Xuan},
doi = {10.1109/ICTAI.2012.165},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of International Conference on Tools with Artificial Intelligence, ICTAI/Wang, Li, Zhou/Wang, Li, Zhou - 2012 - Automatic discovery and transfer of MAXQ hierarchies in a complex system.pdf:pdf},
isbn = {9780769549156},
issn = {10823409},
journal = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
keywords = {Clustering,MAXQ,Reinforcement Learning,System of Systems},
pages = {1157--1162},
title = {{Automatic discovery and transfer of MAXQ hierarchies in a complex system}},
volume = {1},
year = {2012}
}
@book{2013b,
author = {Новиков, Д. А. and Чхартишвили, А. Г.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Новиков, Чхартишвили/Новиков, Чхартишвили - 2013 - Рефлексия и управление математические модели.pdf:pdf},
isbn = {9785940522263},
language = {russian},
pages = {412},
publisher = {Физматлит},
title = {{Рефлексия и управление: математические модели}},
year = {2013}
}
@inproceedings{Mehta2008b,
abstract = {We present an algorithm, HI-MAT (Hierar- chy Induction via Models And Trajectories), that discovers MAXQ task hierarchies by ap- plying dynamic Bayesian network models to a successful trajectory from a source rein- forcement learning task. HI-MAT discovers subtasks by analyzing the causal and tem- poral relationships among the actions in the trajectory. Under appropriate assumptions, HI-MAT induces hierarchies that are consis- tent with the observed trajectory and have compact value-function tables employing safe state abstractions. We demonstrate empir- ically that HI-MAT constructs compact hi- erarchies that are comparable to manually- engineered hierarchies and facilitate signifi- cant speedup in learning when transferred to a target task.},
address = {New York, New York, USA},
author = {Mehta, Neville and Ray, Soumya and Tadepalli, Prasad and Dietterich, Thomas},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390238},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 25th international conference on Machine learning - ICML '08/Mehta et al/Mehta et al. - 2008 - Automatic discovery and transfer of MAXQ hierarchies.pdf:pdf},
isbn = {9781605582054},
pages = {648--655},
publisher = {ACM Press},
title = {{Automatic discovery and transfer of MAXQ hierarchies}},
url = {http://portal.acm.org/citation.cfm?id=1390238 http://portal.acm.org/citation.cfm?doid=1390156.1390238},
year = {2008}
}
@article{Rasmussen2017a,
abstract = {We present the first model capable of performing hierarchical reinforcement learning in a general, neurally detailed imple-mentation. We show that this model is able to learn a spatial pickup and delivery task more quickly than one without hier-archical abilities. In addition, we show that this model is able to leverage its hierarchical structure to transfer learned knowl-edge between related tasks. These results point towards the advantages to be gained by using a hierarchical RL framework to understand the brain's powerful learning ability.},
author = {Rasmussen, Daniel and Voelker, Aaron and Eliasmith, Chris},
doi = {10.1371/journal.pone.0180234},
editor = {Cymbalyuk, Gennady},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/PLOS ONE/Rasmussen, Voelker, Eliasmith/Rasmussen, Voelker, Eliasmith - 2017 - A neural model of hierarchical reinforcement learning(2).pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/PLOS ONE/Rasmussen, Voelker, Eliasmith/Rasmussen, Voelker, Eliasmith - 2017 - A neural model of hierarchical reinforcement learning.pdf:pdf},
isbn = {1111111111},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {brain could achieve its,hierarchical,hrl into a theory,neural engineering framework,neural model,of neural function,providing a hypothe-,reinforcement learning,sis for how the,strengths in scaling},
month = {jul},
number = {7},
pages = {e0180234},
title = {{A neural model of hierarchical reinforcement learning}},
url = {http://dx.plos.org/10.1371/journal.pone.0180234},
volume = {12},
year = {2017}
}
@article{Konidaris2016a,
abstract = {We describe a framework for building abstraction hierarchies whereby an agent alternates skill- and representation-acquisition phases to construct a sequence of increasingly abstract Markov decision processes. Our formulation builds on recent results showing that the appropriate abstract representation of a problem is specified by the agent's skills. We describe how such a hierarchy can be used for fast planning, and illustrate the construction of an appropriate hierarchy for the Taxi domain.},
archivePrefix = {arXiv},
arxivId = {1509.07582},
author = {Konidaris, George},
doi = {10.1016/j.surg.2015.03.052.Rate},
eprint = {1509.07582},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IJCAI International Joint Conference on Artificial Intelligence/Konidaris/Konidaris - 2016 - Constructing abstraction hierarchies using a skill-symbol loop.pdf:pdf},
isbn = {6176366127},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning},
pages = {1648--1654},
pmid = {28579718},
title = {{Constructing abstraction hierarchies using a skill-symbol loop}},
volume = {2016-Janua},
year = {2016}
}
@article{Gorodetsky2007,
abstract = {Данная работа представляет реализованную авторами P2P агентскую плат- форму, экземпляры которой, установленные в узлах сети поверх стандартного P2P серви- са, образуют распределенную базу знаний, предназначенную для организации семантическо- го P2P взаимодействия агентов. Прикладные агенты, в свою очередь, устанавливаются в узлах сети поверх экземпляров агентской платформы. В основу разработки положены функциональная архитектура, разработанная рабочей группой FIPA в качестве предложе- ния для последующей программной реализации и стандартизации. Разработанная про- граммная реализация платформы поддерживается также механизмом парных взаимодейст- вий агентов на основе сообщений разработанных форматов, а также парных коммуникаций узлов сети. Такой механизмом парных взаимодействий агентов также разработан автора- ми. Роль, функции и существо процессов функционирования этой платформы поясняется на примерах двух приложений, которые сами по себе являются достаточно важными с практи- ческой точки зрения. Эти же приложения использованы для верификации основных решений, предложенных в работе.},
author = {Городецкий, В. И. and Карсаев, О. В. and Самойлов, В. В. and Серебряков, С. В.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Труды СПИИРАН/Городецкий et al/Городецкий et al. - 2007 - Открытые сети агентов.pdf:pdf},
journal = {Труды СПИИРАН},
language = {russian},
number = {4},
pages = {11--35},
title = {{Открытые сети агентов}},
year = {2007}
}
@inproceedings{Harutyunyan2017,
abstract = {A temporally abstract action, or an option, is specified by a policy and a termination condition: the policy guides option behavior, and the termination condition roughly determines its length. Generally, learning with longer options (like learning with multi-step returns) is known to be more efficient. However, if the option set for the task is not ideal, and cannot express the primitive optimal policy exactly, shorter options offer more flexibility and can yield a better solution. Thus, the termination condition puts learning efficiency at odds with solution quality. We propose to resolve this dilemma by decoupling the behavior and target terminations, just like it is done with policies in off-policy learning. To this end, we give a new algorithm, Q($\backslash$beta), that learns the solution with respect to any termination condition, regardless of how the options actually terminate. We derive Q($\backslash$beta) by casting learning with options into a common framework with well-studied multi-step off-policy learning. We validate our algorithm empirically, and show that it holds up to its motivating claims.},
archivePrefix = {arXiv},
arxivId = {1711.03817},
author = {Harutyunyan, Anna and Vrancx, Peter and Bacon, Pierre-Luc and Precup, Doina and Nowe, Ann},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
eprint = {1711.03817},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Harutyunyan et al/Harutyunyan et al. - 2017 - Learning with Options that Terminate Off-Policy.pdf:pdf},
title = {{Learning with Options that Terminate Off-Policy}},
url = {http://arxiv.org/abs/1711.03817},
year = {2017}
}
@article{2017a,
abstract = {Системы комического наблюдения являются важным источником информации, необ- ходимой для оперативного решения широкого круга задач гражданского и военного назначе- ния. Эффективность функционирования таких систем зависит от многих факторов. Наряду с множеством технических, информационных, технологических, инфраструктурных, эконо- мических и других факторов, их эффективность в значительной степени зависит от качест- ва планирования миссий космических аппаратов и управления в реальном времени процессом исполнения миссии. В работе сформулирована проблема построения самоорганизующейся системы группового управления поведением кластера малых спутников, реализующего авто- номное исполнение заявок на сервис по добыванию информации о наземных объектах средст- вами космического наблюдения. В ней предлагается новая концепция группового управления в системе космического наблюдения. В основу этой концепции положен принцип самоорганиза- ции группового поведения кластера спутников. Такая система управления оказывается в со- стоянии реализовать автономное планирование и оперативное управления космической груп- пировкой, в которой все базовые функции процесса управления реализуются ее бортовыми средствами. Теоретический фундамент этой концепции строится на моделях коллективной робототехники, которые в настоящее время активно развиваются в области многоагент- ных систем. В работе приведен краткий обзор современного состояния исследований в об- ласти систем управления кластерами малых спутников, дана достаточно общая постановка задачи, в которой кластер малых спутников рассматривается как полностью автономная система, предназначенная для выполнения заказов на сбор и доставку космической информа- ции о наземных объектах. При этом полагается, что система управления самостоятельно в реальном времени распределяет задачи наблюдения на множестве спутников группировки, планирует и составляет расписание выполнения наблюдений в соответствии с пространст- венно-временными требованиями заказчика, выполняет оперативное управление распреде- ленным исполнением построенного расписания и выполняет коррекцию распределения задач и расписания их выполнения при возникновении нештатных ситуаций. В работе дано деталь- ное описание разработанной концепции самоорганизующейся системы группового управления кластером малых спутников, и архитектуры ее программной реализации.},
author = {Городецкий, В. И. and Карасев, О. В.},
doi = {10.18522/2311-3103-2017-1-234247},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Известия ЮФУ. Технические науки/Городецкий, Карасев/Городецкий, Карасев - 2017 - Самоорганизация группового поведения кластера малых спутников распределенной системы наблюдения.pdf:pdf},
issn = {23113103},
journal = {Известия ЮФУ. Технические науки},
keywords = {Малый спутник,автономная миссия,блюдения,динамическая коммуникационная сеть.,задача наблюдения,коллективное поведение,парное взаимо- действие спутников,распределенная система на-,ресурсы спутника,самоорганизация},
language = {russian},
month = {jan},
number = {2},
pages = {234--247},
title = {{Самоорганизация группового поведения кластера малых спутников распределенной системы наблюдения}},
url = {http://izv-tn.tti.sfedu.ru/wp-content/uploads/2017/1/19.pdf},
volume = {187},
year = {2017}
}
@book{Szepesvari2010,
abstract = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective.What distin- guishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the al- gorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book,we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming.We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.},
author = {Szepesv{\'{a}}ri, Csaba},
booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
doi = {10.2200/S00268ED1V01Y201005AIM009},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Synthesis Lectures on Artificial Intelligence and Machine Learning/Szepesv{\'{a}}ri/Szepesv{\'{a}}ri - 2010 - Algorithms for Reinforcement Learning.pdf:pdf},
isbn = {9781608454921},
issn = {1939-4608},
number = {1},
pages = {1--103},
title = {{Algorithms for Reinforcement Learning}},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009},
volume = {4},
year = {2010}
}
@article{Singh2010a,
abstract = {There is great interest in building intrinsic motivation into artificial systems using the reinforcement learning framework. Yet, what intrinsic motivation may mean computationally, and how it may differ from extrinsic motivation, remains a murky and controversial subject. In this paper, we adopt an evolutionary perspective and define a new optimal reward framework that captures the pressure to design good primary reward functions that lead to evolutionary success across environments. The results of two computational experiments show that optimal primary reward signals may yield both emergent intrinsic and extrinsic motivation. The evolutionary perspective and the associated optimal reward framework thus lead to the conclusion that there are no hard and fast features distinguishing intrinsic and extrinsic reward computationally. Rather, the directness of the relationship between rewarding behavior and evolutionary success varies along a continuum.},
author = {Singh, Satinder and Lewis, Richard L. and Barto, Andrew G. and Sorg, Jonathan},
doi = {10.1109/TAMD.2010.2051031},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IEEE Transactions on Autonomous Mental Development/Singh et al/Singh et al. - 2010 - Intrinsically Motivated Reinforcement Learning An Evolutionary Perspective.pdf:pdf},
isbn = {0262195348, 9780262195348},
issn = {1943-0604},
journal = {IEEE Transactions on Autonomous Mental Development},
number = {2},
pages = {70--82},
title = {{Intrinsically Motivated Reinforcement Learning: An Evolutionary Perspective}},
url = {http://ieeexplore.ieee.org/document/5471106/},
volume = {2},
year = {2010}
}
@incollection{Lorini2010,
author = {Lorini, Emiliano and Verdicchio, Mario},
booktitle = {Coordination, Organizations, Institutions and Norms in Agent Systems V},
doi = {10.1007/978-3-642-14962-7_10},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Coordination, Organizations, Institutions and Norms in Agent Systems V/Lorini, Verdicchio/Lorini, Verdicchio - 2010 - Towards a Logical Model of Social Agreement for Agent Societies.pdf:pdf},
isbn = {3642149618},
issn = {03029743},
pages = {147--162},
title = {{Towards a Logical Model of Social Agreement for Agent Societies}},
url = {http://link.springer.com/10.1007/978-3-642-14962-7{\_}10},
year = {2010}
}
@inproceedings{Cacace2015,
abstract = {In human-robot interactive scenarios communication and col- laboration during task execution are crucial issues. Since the human be- havior is unpredictable and ambiguous, an interactive robotic system is to continuously interpret intentions and goals adapting its executive and communicative processes according to the users behaviors. In this work, we propose an integrated system that exploits attentional mechanisms to flexibly adapt planning and executive processes to the multimodal human-robot interaction.},
author = {Cacace, Jonathan and Caccavale, Riccardo and Fiore, Michelangelo and Alam, Rachid},
booktitle = {Proceedings of the 2nd Italian Workshop on Artificial Intelligence and Robotics},
editor = {Alberto, Finzi and Mastrogiovanni, Fulvio and Orlandini, Andrea and Sgorbissa, Antonio},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 2nd Italian Workshop on Artificial Intelligence and Robotics/Cacace et al/Cacace et al. - 2015 - Attentional Plan Execution for Human-Robot Cooperation.pdf:pdf},
pages = {19--28},
series = {CEUR Workshop Proceedings},
title = {{Attentional Plan Execution for Human-Robot Cooperation}},
year = {2015}
}
@article{Huang2017,
abstract = {Our ultimate goal is to efficiently enable end-users to correctly anticipate a robot's behavior in novel situations. This behavior is often a direct result of the robot's underlying objective function. Our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, and then selecting those behaviors that are maximally informative. The problem of computing a posterior over objectives from observed behavior is known as Inverse Reinforcement Learning (IRL), and has been applied to robots learning human objectives. We consider the problem where the roles of human and robot are swapped. Our main contribution is to recognize that unlike robots, humans will not be $\backslash$emph{\{}exact{\}} in their IRL inference. We thus introduce two factors to define candidate approximate-inference models for human learning in this setting, and analyze them in a user study in the autonomous driving domain. We show that certain approximate-inference models lead to the robot generating example behaviors that better enable users to anticipate what the robot will do in test situations. Our results also suggest, however, that additional research is needed in modeling how humans extrapolate from examples of robot behavior.},
archivePrefix = {arXiv},
arxivId = {1702.03465},
author = {Huang, Sandy H. and Held, David and Abbeel, Pieter and Dragan, Anca D.},
doi = {10.15607/RSS.2017.XIII.059},
eprint = {1702.03465},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Huang et al/Huang et al. - 2017 - Enabling Robots to Communicate their Objectives.pdf:pdf},
isbn = {1234567245},
issn = {15737527},
month = {feb},
title = {{Enabling Robots to Communicate their Objectives}},
url = {http://arxiv.org/abs/1702.03465},
year = {2017}
}
@article{James2018,
author = {James, Steven and Rosman, Benjamin and Africa, South and Konidaris, George},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/James et al/James et al. - 2018 - Learning to Plan with Portable Symbols.pdf:pdf},
number = {July},
title = {{Learning to Plan with Portable Symbols}},
year = {2018}
}
@phdthesis{Mehta2011a,
abstract = {Acting intelligently to efficiently solve sequential decision problems requires the ability to extract hierarchical structure from the underlying domain dynamics, exploit it for optimal or near-optimal decision-making, and transfer it to related problems instead of solving every problem in isolation. This dissertation makes three contributions toward this goal. The first contribution is the introduction of two frameworks for the transfer of hi- erarchical structure in sequential decision problems. The MASH framework facilitates transfer among multiple agents coordinating within a domain. The VRHRL framework allows an agent to transfer its knowledge across a family of domains that share the same transition dynamics but have differing reward dynamics. Both MASH and VRHRL are validated empirically in large domains and the results demonstrate significant speedup in the solutions due to transfer. The second contribution is a new approach to the discovery of hierarchical structure in sequential decision problems. HI-MAT leverages action models to analyze the relevant dependencies in a hierarchically-generated trajectory and it discovers hierarchical struc- ture that transfers to all problems whose actions share the same relevant dependencies as the single source problem. HierGen advances HI-MAT by learning simple action models, leveraging these models to analyze non-hierarchically-generated trajectories from mul- tiple source problems in a robust causal fashion, and discovering hierarchical structure that transfers to all problems whose actions share the same causal dependencies as those in the source problems. Empirical evaluations in multiple domains demonstrate that the discovered hierarchical structures are comparable to manually-designed structures in quality and performance. Action models are essential to hierarchical structure discovery and other aspects of intelligent behavior. The third contribution of this dissertation is the introduction of two general frameworks for learning action models in sequential decision problems. In the MBP framework, learning is user-driven; in the PLEX framework, the learner generates its own problems. The frameworks are formally analyzed and reduced to concept learning with one-sided error. A general action-modeling language is shown to be efficiently learnable in both frameworks.},
author = {Mehta, Neville},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Mehta/Mehta - 2011 - Hierarchical Structure Discovery and Transfer in Sequential Decision Problems.pdf:pdf},
pages = {180},
title = {{Hierarchical Structure Discovery and Transfer in Sequential Decision Problems}},
year = {2011}
}
@article{Botvinick2012,
abstract = {The hierarchical structure of human and animal behavior has been of critical interest in neuroscience for many years. Yet understanding the neural processes that give rise to such structure remains an open challenge. In recent research, a new perspective on hierarchical behavior has begun to take shape, inspired by ideas from machine learning, and in particular the framework of hierarchical reinforcement learning. Hierarchical reinforcement learning builds on traditional reinforcement learning mechanisms, extending them to accommodate temporally extended behaviors or subroutines. The resulting computational paradigm has begun to influence both theoretical and empirical work in neuroscience, conceptually aligning the study of hierarchical behavior with research on other aspects of learning and decision making, and giving rise to some thought-provoking new findings. ?? 2012.},
author = {Botvinick, Matthew Michael},
doi = {10.1016/j.conb.2012.05.008},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Current Opinion in Neurobiology/Botvinick/Botvinick - 2012 - Hierarchical reinforcement learning and decision making.pdf:pdf},
isbn = {0818653302},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
number = {6},
pages = {956--962},
pmid = {22695048},
publisher = {Elsevier Ltd},
title = {{Hierarchical reinforcement learning and decision making}},
url = {http://dx.doi.org/10.1016/j.conb.2012.05.008},
volume = {22},
year = {2012}
}
@article{Jong2008a,
abstract = {Hierarchical decomposition promises to help scale reinforcement learning algorithms naturally to real-world problems by exploiting their under- lying structure. Model-based algorithms, which provided the first finite-time convergence guaran- tees for reinforcement learning, may also play an important role in copingwith the relative scarcity of data in large environments. In this paper, we introduce an algorithm that fully integrates mod- ern hierarchical and model-learning methods in the standard reinforcement learning setting. Our algorithm, R-MAXQ, inherits the efficientmodel- based exploration of the R-MAX algorithm and the opportunities for abstraction provided by the MAXQframework. We analyze the sample com- plexity of our algorithm, and our experiments in a standard simulation environment illustrate the advantages of combining hierarchies andmodels},
author = {Jong, Nicholas K and Stone, Peter},
doi = {10.1145/1390156.1390211},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/25th International Conference on Machine Learning/Jong, Stone/Jong, Stone - 2008 - Hierarchical Model-Based Reinforcement Learning R- MAX MAXQ.pdf:pdf},
isbn = {9781605582054},
journal = {25th International Conference on Machine Learning},
title = {{Hierarchical Model-Based Reinforcement Learning : R- MAX + MAXQ}},
year = {2008}
}
@incollection{Castro2012a,
abstract = {Abstract. Temporally extended actions are usually effective in speeding up reinforcement learning. In this paper we present a mechanism for automatically constructing such actions, expressed as options [Sutton et al., 1999], in a finite Markov Decision Process (MDP). To do this, we compute a bisimulation metric [Ferns et al., 2004] between the states in a small MDP and the states in a large MDP, which we want to solve. The shape of this metric is then used to completely define a set of options for the large MDP. We demonstrate empirically that our approach is able to improve the speed of reinforcement learning, and is generally not sensitive to parameter tuning.},
author = {Castro, Pablo Samuel and Precup, Doina},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-29946-9_16},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Castro, Precup/Castro, Precup - 2012 - Automatic Construction of Temporally.pdf:pdf},
isbn = {9783642299452},
issn = {03029743},
pages = {140--152},
title = {{Automatic Construction of Temporally Extended Actions for MDPs Using Bisimulation Metrics}},
url = {http://link.springer.com/10.1007/978-3-642-29946-9{\_}16},
volume = {7188 LNAI},
year = {2012}
}
@article{Andre2002a,
abstract = {Hierarchical reinforcement learning ALisp - a language to support represent hierarchical model. The authors present way to organize information in hierarchical fashion. In which, at each level, each state contains only necessary information for RL, and speed up learning process then. That state is called abstract state in the paper.},
author = {Andre, David and Russell, Stuart J},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/AAAI Conference on Artificial Intelligence/Andre, Russell/Andre, Russell - 2002 - State Abstraction for Programmable Reinforcement Learning Agents.pdf:pdf},
isbn = {0-262-51129-0},
issn = {1049-5258},
journal = {AAAI Conference on Artificial Intelligence},
pages = {119--125},
title = {{State Abstraction for Programmable Reinforcement Learning Agents}},
url = {http://www.aaai.org/Papers/AAAI/2002/AAAI02-019.pdf},
year = {2002}
}
@article{Ghallab2014,
abstract = {Planning is motivated by acting. Most of the existing work on automated planning underestimates the reasoning and deliberation needed for acting; it is instead biased towards path-finding methods in a compactly specified state-transition system. Researchers in this AI field have developed many planners, but very few actors. We believe this is one of the main causes of the relatively low deployment of automated planning applications. In this paper, we advocate a change in focus to actors as the primary topic of investigation. Actors are not mere plan executors: they may use planning and other deliberation tools, before and during acting. This change in focus entails two interconnected principles: a hierarchical structure to integrate the actor's deliberation functions, and continual online planning and reasoning throughout the acting process. In the paper, we discuss open problems and research directions toward that objective in knowledge representations, model acquisition and verification, synthesis and refinement, monitoring, goal reasoning, and integration. {\textcopyright} 2013 Elsevier B.V.},
author = {Ghallab, Malik and Nau, Dana and Traverso, Paolo},
doi = {10.1016/j.artint.2013.11.002},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Artificial Intelligence/Ghallab, Nau, Traverso/Ghallab, Nau, Traverso - 2014 - The actor's view of automated planning and acting A position paper.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Automated planning and acting},
number = {1},
pages = {1--17},
publisher = {Elsevier B.V.},
title = {{The actor's view of automated planning and acting: A position paper}},
url = {http://dx.doi.org/10.1016/j.artint.2013.11.002},
volume = {208},
year = {2014}
}
@inproceedings{Kulkarni2016,
abstract = {Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.},
archivePrefix = {arXiv},
arxivId = {1604.06057},
author = {Kulkarni, Tejas D. and Narasimhan, Karthik R. and Saeedi, Ardavan and Tenenbaum, Joshua B.},
booktitle = {Proceeding of NIPS 2016},
doi = {10.1023/A:1025696116075},
eprint = {1604.06057},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceeding of NIPS 2016/Kulkarni et al/Kulkarni et al. - 2016 - Hierarchical Deep Reinforcement Learning Integrating Temporal Abstraction and Intrinsic Motivation.pdf:pdf},
isbn = {0924-6703},
issn = {1573-7594},
pages = {3682--3690},
title = {{Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation}},
url = {http://arxiv.org/abs/1604.06057},
year = {2016}
}
@article{Simsek2005a,
abstract = {We present a new subgoal-based method for automatically creating useful skills in reinforcement learning. Our method identifies subgoals by partitioning local state transition graphs—those that are constructed using only the most recent experiences of the agent. The local scope of our subgoal discovery method allows it to successfully identify the type of subgoals we seek—states that lie between two densely-connected regions of the state space—while producing an algorithm with low computational cost.},
author = {Şimşek, {\"{O}}zg{\"{u}}r and Wolfe, Alicia P. and Barto, Andrew G.},
doi = {10.1145/1102351.1102454},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 22nd international conference on Machine learning - ICML '05/Şimşek, Wolfe, Barto/Şimşek, Wolfe, Barto - 2005 - Identifying useful subgoals in reinforcement learning by local graph partitio.pdf:pdf},
isbn = {1595931805},
journal = {Proceedings of the 22nd international conference on Machine learning  - ICML '05},
pages = {816--823},
title = {{Identifying useful subgoals in reinforcement learning by local graph partitioning}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102454},
year = {2005}
}
@article{Auslander2014a,
abstract = {Executing complex plans for coordinating the behaviors of multiple heterogeneous agents often requires setting several parameters. For example, we are developing a decision aid for deploying a set of autonomous vehicles to perform situation assessment in a disaster relief operation. Our system, the Situated Decision Process (SDP), uses parameterized plans to coordinate these vehicles. However, no model exists for setting the values of these parameters. We describe a case-based reasoning solution for this problem and report on its utility in simulated scenarios, given a case library that represents only a small percentage of the problem space. We found that our agents, when executing plans generated using our case-based algorithm on problems with high uncertainty, performed significantly better than when executing plans using baseline approaches.},
author = {Auslander, Brian and Apker, Thomas and Aha, David W},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the Twenty-Second International Conference on Case-Based Reasoning/Auslander, Apker, Aha/Auslander, Apker, Aha - 2014 - Case-Based Parameter Selection for Plans Coordinating Autonomous Vehicle.pdf:pdf},
isbn = {03029743 (ISSN)},
issn = {16113349},
journal = {Proceedings of the Twenty-Second International Conference on Case-Based Reasoning},
keywords = {case-based reasoning,parameter selection,robotic control},
pages = {32--47},
title = {{Case-Based Parameter Selection for Plans: Coordinating Autonomous Vehicle Teams}},
volume = {2},
year = {2014}
}
@book{Sutton2011,
address = {М.},
author = {Саттон, Р.С. and Барто, Э. Г.},
edition = {2-е},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Саттон, Барто/Саттон, Барто - 2011 - Обучение с подкреплением.pdf:pdf},
language = {russian},
pages = {399},
publisher = {БИНОМ. Лаборатория знаний},
title = {{Обучение с подкреплением}},
year = {2011}
}
@inproceedings{Stulp2011,
abstract = {Temporal abstraction and task decomposition drastically reduce the search space for planning and control, and are fundamental to making complex tasks amenable to learning. In the context of reinforcement learning, temporal abstractions are studied within the paradigm of hierarchical reinforcement learning.},
author = {Stulp, Freek and Schaal, Stefan},
booktitle = {2011 11th IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/Humanoids.2011.6100841},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/2011 11th IEEE-RAS International Conference on Humanoid Robots/Stulp, Schaal/Stulp, Schaal - 2011 - Hierarchical reinforcement learning with movement primitives.pdf:pdf},
isbn = {978-1-61284-868-6},
issn = {21640572},
month = {oct},
pages = {231--238},
publisher = {IEEE},
title = {{Hierarchical reinforcement learning with movement primitives}},
url = {http://ieeexplore.ieee.org/document/6100841/},
year = {2011}
}
@inproceedings{Pitis2017,
abstract = {This paper presents preliminary work on a framework for reasoning over multiple competing representations of the value function during reinforcement learning, with a focus on landmark and factor-based reasoning. This approach enables consistency-based learning, explicit justification of actions and learning by partial supervision, and has the potential to improve agent performance.},
author = {Pitis, Silviu},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Pitis/Pitis - 2017 - Reasoning for Reinforcement Learning.pdf:pdf},
title = {{Reasoning for Reinforcement Learning}},
url = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-51.html},
year = {2017}
}
@article{2007a,
author = {Лукьянова, Л. М.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Известия РАН. Теория и системы управления/Лукьянова/Лукьянова - 2007 - Целеполагание, анализ и синтез целей в сложных системах модели и методы моделирования.pdf:pdf},
journal = {Известия РАН. Теория и системы управления},
language = {russian},
number = {5},
pages = {100--113},
title = {{Целеполагание, анализ и синтез целей в сложных системах: модели и методы моделирования}},
year = {2007}
}
@book{Maes1988,
abstract = {The papers in the first part shed some light on the concept of reflection or its origins. Important questions treated in these papers are: What are the issues in computational reflection. How does it relate to the notion of reflection in logic and meta-mathematics. How can reflective systems be categorized. Why is meaning an important issue in reflection and reflection an interesting domain to study meaning. A number of practical realisations of reflective systems are presented in the second part. The papers investigate the problems arising in the construction of reflective systems, and present techniques to solve these. The implementations range from specific systems exhibiting a reflective behaviour e.g. logic-based reasoning system, rule-based systems, etc., to programming languages providing facilities for the construction of reflective systems (logic programming languages, description-based languages, functional languages, object-oriented and actor languages). Finally, various applications of meta-level architectures and reflection are described.},
author = {Maes, P. and Nardi, D.},
doi = {10.1007/3-540-48443-4},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Maes, Nardi/Maes, Nardi - 1988 - Meta-level architectures and reflection.pdf:pdf},
isbn = {0444703438 (U.S.)},
keywords = {-- mathematics {\&} mathematical models-- (1987-1989),abstracts,and information science,computer architecture,computing,data processing,document types,general and miscellaneous//mathematics,leading abstract,mathematical logic,mathematics,processing 990210*  -- supercomputers-- (1987-1989,programming,programming languages,reflection},
title = {{Meta-level architectures and reflection}},
year = {1988}
}
@article{Florensa2017,
abstract = {Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.},
archivePrefix = {arXiv},
arxivId = {1704.03012},
author = {Florensa, Carlos and Duan, Yan and Abbeel, Pieter},
eprint = {1704.03012},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/ArXiv/Florensa, Duan, Abbeel/Florensa, Duan, Abbeel - 2017 - Stochastic Neural Networks for Hierarchical Reinforcement Learning.pdf:pdf},
isbn = {9781613242643},
journal = {ArXiv},
month = {apr},
title = {{Stochastic Neural Networks for Hierarchical Reinforcement Learning}},
url = {https://openreview.net/pdf?id=B1oK8aoxe http://arxiv.org/abs/1704.03012},
year = {2017}
}
@unpublished{Tessler2016,
abstract = {We propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base. Knowledge is transferred by learning reusable skills to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem. These reusable skills, which we refer to as Deep Skill Networks, are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture using two techniques: (1) a deep skill array and (2) skill distillation, our novel variation of policy distillation (Rusu et. al. 2015) for learning skills. Skill distillation enables the HDRLN to efficiently retain knowledge and therefore scale in lifelong learning, by accumulating knowledge and encapsulating multiple reusable skills into a single distilled network. The H-DRLN exhibits superior performance and lower learning sample complexity compared to the regular Deep Q Network (Mnih et. al. 2015) in sub-domains of Minecraft.},
archivePrefix = {arXiv},
arxivId = {1604.07255},
author = {Tessler, Chen and Givony, Shahar and Zahavy, Tom and Mankowitz, Daniel J. and Mannor, Shie},
eprint = {1604.07255},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Tessler et al/Tessler et al. - 2016 - A Deep Hierarchical Approach to Lifelong Learning in Minecraft.pdf:pdf},
title = {{A Deep Hierarchical Approach to Lifelong Learning in Minecraft}},
url = {http://arxiv.org/abs/1604.07255},
year = {2016}
}
@article{Dann2017,
abstract = {In platform videogames, players are frequently tasked with solving medium-term navigation prob-lems in order to gather items or powerups. Arti-ficial agents must generally obtain some form of direct experience before they can solve such tasks. Experience is gained either through training runs, or by exploiting knowledge of the game's physics to generate detailed simulations. Human players, on the other hand, seem to look ahead in high-level, abstract steps. Motivated by human play, we intro-duce an approach that leverages not only abstract " skills " , but also knowledge of what those skills can and cannot achieve. We apply this approach to Infinite Mario, where despite facing randomly generated, maze-like levels, our agent is capable of deriving complex plans in real-time, without rely-ing on perfect knowledge of the game's physics.},
author = {Dann, Michael and Zambetta, Fabio and Thangarajah, John},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IJCAI International Joint Conference on Artificial Intelligence/Dann, Zambetta, Thangarajah/Dann, Zambetta, Thangarajah - 2017 - Real-time navigation in classical platform games via skill reuse.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning: Reinforcement Learning,Multidisciplinary Topics and Applications: Compute},
pages = {1582--1588},
title = {{Real-time navigation in classical platform games via skill reuse}},
year = {2017}
}
@article{Dietterich2000,
abstract = {This paper presents the MAXQ approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges wih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {cs/9905014},
author = {Dietterich, Thomas G.},
doi = {10.1613/jair.639},
eprint = {9905014},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Journal of Artificial Intelligence Research/Dietterich/Dietterich - 2000 - Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition.pdf:pdf},
isbn = {978-3-540-67839-7},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {227--303},
primaryClass = {cs},
title = {{Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition}},
volume = {13},
year = {2000}
}
@article{VanHarmelen1992,
author = {van Harmelen, Frank and Wielinga, Bob and Bredeweg, Bert and Schreiber, Guus and Karbach, Werner and Reinders, Martin and Vo{\ss}, Angi and Akkermans, Hans and Bartsch-Sp{\"{o}}rl, Brigitte and Vinkhuyzen, Erik},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Enhancing the Knowledge Engineering Process -- Contributions from {\{}ESPRIT{\}}/van Harmelen et al/van Harmelen et al. - 1992 - Knowledge-Level Reflection.pdf:pdf},
journal = {Enhancing the Knowledge Engineering Process -- Contributions from {\{}ESPRIT{\}}},
keywords = {knowledge-based systems,meta-reasoning,reflection},
number = {4178},
pages = {175--204},
title = {{Knowledge-Level Reflection}},
year = {1992}
}
@incollection{Menache2002a,
abstract = {We present the Q-Cut algorithm, a graph theoretic approach for automatic detection of sub-goals in a dynamic environment, which is used for acceleration of the Q-Learning algorithm. The learning agent creates an on-line map of the process history, and uses an efficient Max-Flow/Min-Cut algorithm for identifying bottlenecks. The policies for reaching bottlenecks are separately learned and added to the model in a form of options (macro-actions). We then extend the basic Q-Cut algorithm to the Segmented Q-Cut algorithm, which uses previously identified bottlenecks for state space partitioning, necessary for finding additional bottlenecks in complex environments. Experiments show significant performance improvements, particulary in the initial learning phase.},
author = {Menache, Ishai and Mannor, Shie and Shimkin, Nahum},
booktitle = {ECML 2002: Machine Learning: ECML 2002},
doi = {10.1007/3-540-36755-1_25},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/ECML 2002 Machine Learning ECML 2002/Menache, Mannor, Shimkin/Menache, Mannor, Shimkin - 2002 - Q-Cut—Dynamic Discovery of Sub-goals in Reinforcement Learning.pdf:pdf},
isbn = {978-3-540-44036-9, 978-3-540-36755-0},
issn = {16113349},
pages = {295--306},
title = {{Q-Cut—Dynamic Discovery of Sub-goals in Reinforcement Learning}},
url = {http://link.springer.com/10.1007/3-540-36755-1{\_}25},
year = {2002}
}
@inproceedings{Roderick2017a,
abstract = {We examine the problem of learning and planning on high-dimensional domains with long horizons and sparse rewards. Recent approaches have shown great successes in many Atari 2600 domains. However, domains with long horizons and sparse rewards, such as Montezuma's Revenge and Venture, remain challenging for existing methods. Methods using abstraction (Dietterich 2000; Sutton, Precup, and Singh 1999) have shown to be useful in tackling long-horizon problems. We combine recent techniques of deep reinforcement learning with existing model-based approaches using an expert-provided state abstraction. We construct toy domains that elucidate the problem of long horizons, sparse rewards and high-dimensional inputs, and show that our algorithm significantly outperforms previous methods on these domains. Our abstraction-based approach outperforms Deep Q-Networks (Mnih et al. 2015) on Montezuma's Revenge and Venture, and exhibits backtracking behavior that is absent from previous methods.},
archivePrefix = {arXiv},
arxivId = {1710.00459},
author = {Roderick, Melrose and Grimm, Christopher and Tellex, Stefanie},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
eprint = {1710.00459},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Roderick, Grimm, Tellex/Roderick, Grimm, Tellex - 2017 - Deep Abstract Q-Networks.pdf:pdf},
title = {{Deep Abstract Q-Networks}},
url = {http://arxiv.org/abs/1710.00459},
year = {2017}
}
@article{Garcez2015,
abstract = {The goal of neural-symbolic computation is to integrate ro- bust connectionist learning and sound symbolic reasoning. With the recent advances in connectionist learning, in par- ticular deep neural networks, forms of representation learn- ing have emerged. However, such representations have not become useful for reasoning. Results from neural-symbolic computation have shown to offer powerful alternatives for knowledge representation, learning and reasoning in neural computation. This paper recalls the main contributions and discusses key challenges for neural-symbolic integration which have been identified at a recent Dagstuhl seminar.},
author = {Garcez, Avila and Besold, Tarek R and Raedt, Luc De and F{\"{o}}ldiak, Peter and Hitzler, Pascal and Icard, Thomas and K{\"{u}}hnberger, Kai-uwe and Lamb, Luis C and Miikkulainen, Risto and Silver, Daniel L},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Knowledge Representation and Reasoning Integrating Symbolic and Neural Approaches Papers from the 2015 AAAI Spring Symposium/Garcez et al/Garcez et al. - 2015 - Neural-Symbolic Learning and Reasoning Contrib.pdf:pdf},
journal = {Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches: Papers from the 2015 AAAI Spring Symposium},
pages = {18--21},
title = {{Neural-Symbolic Learning and Reasoning : Contributions and Challenges}},
year = {2015}
}
@article{Cox2007,
author = {Cox, Michael T},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/AI Magazine/Cox/Cox - 2007 - Perpetual Self-Aware Cognitive Agents.pdf:pdf},
journal = {AI Magazine},
number = {1},
pages = {32--45},
title = {{Perpetual Self-Aware Cognitive Agents}},
volume = {28},
year = {2007}
}
@article{Botvinick2008,
abstract = {The recognition of hierarchical structure in human behavior was one of the founding insights of the cognitive revolution. Despite decades of research, however, the computational mechanisms underlying hierarchically organized behavior are still not fully understood. Recent findings from behavioral and neuroscientific research have fueled a resurgence of interest in the problem, inspiring a new generation of computational models. In addition to developing some classic proposals, these models also break fresh ground, teasing apart different forms of hierarchical structure, placing a new focus on the issue of learning and addressing recent findings concerning the representation of behavioral hierarchies within the prefrontal cortex. In addition to offering explanations for some key aspects of behavior and functional neuroanatomy, the latest models also pose new questions for empirical research. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
author = {Botvinick, Matthew M.},
doi = {10.1016/j.tics.2008.02.009},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Trends in Cognitive Sciences/Botvinick/Botvinick - 2008 - Hierarchical models of behavior and prefrontal function.pdf:pdf},
isbn = {1364-6613 (Print)$\backslash$n1364-6613 (Linking)},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {5},
pages = {201--208},
pmid = {18420448},
title = {{Hierarchical models of behavior and prefrontal function}},
volume = {12},
year = {2008}
}
@book{Cox2011,
abstract = {The capacity to think about our own thinking may lie at the heart of what it means to be both human and intelligent. Philosophers and cognitive scientists have investigated these matters for many years. Researchers in artificial intelligence have gone further, attempting to implement actual machines that mimic, simulate, and perhaps even replicate this capacity, called metareasoning. In this volume, leading authorities offer a variety of perspectives--drawn from philosophy, cognitive psychology, and computer science--on reasoning about the reasoning process. The book offers a simple model of reasoning about reason as a framework for its discussions. Following this framework, the contributors consider metalevel control of computational activities, introspective monitoring, distributed metareasoning, and, putting all these aspects of metareasoning together, models of the self. Taken together, the chapters offer an integrated narrative on metareasoning themes from both artificial intelligence and cognitive science perspectives.},
doi = {10.7551/mitpress/9780262014809.001.0001},
editor = {Cox, Michael T. and Raja, Anita},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Unknown/Unknown - 2011 - Metareasoning Thinking about thinking.pdf:pdf},
isbn = {9780262014809},
pages = {340},
publisher = {The MIT Press},
title = {{Metareasoning: Thinking about thinking}},
year = {2011}
}
@article{Verma2018,
abstract = {We study the problem of generating interpretable and verifiable policies through reinforcement learning. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, in which the policy is represented by a neural network, the aim in Programmatically Interpretable Reinforcement Learning is to find a policy that can be represented in a high-level programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maxima reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural "oracle". We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also find that a well-designed policy language can serve as a regularizer, and result in the discovery of policies that lead to smoother trajectories and are more easily transferred to environments not encountered during training.},
archivePrefix = {arXiv},
arxivId = {1804.02477},
author = {Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
eprint = {1804.02477},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Verma et al/Verma et al. - 2018 - Programmatically Interpretable Reinforcement Learning.pdf:pdf},
issn = {1938-7228},
title = {{Programmatically Interpretable Reinforcement Learning}},
url = {http://arxiv.org/abs/1804.02477},
year = {2018}
}
@article{Adams2018,
author = {Adams, Julie A},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proc. ofthe 17th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2018)/Adams/Adams - 2018 - Task Fusion Heuristics for Coalition Formation and Planning Robotics Track.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proc. ofthe 17th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2018)/Adams/Adams - 2018 - Task Fusion Heuristics for Coalition Formation and Planning Robotics Track.jpg:jpg},
journal = {Proc. ofthe 17th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2018)},
keywords = {coalition formation,continuous planning,temporal planning},
pages = {2198--2200},
title = {{Task Fusion Heuristics for Coalition Formation and Planning Robotics Track}},
year = {2018}
}
@article{Besold2017,
abstract = {The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research.},
archivePrefix = {arXiv},
arxivId = {1711.03902},
author = {Besold, Tarek R. and d'Avila Garcez, Artur and Bader, Sebastian and Bowman, Howard and Domingos, Pedro and Hitzler, Pascal and Kuehnberger, Kai-Uwe and Lamb, Luis C. and Lowd, Daniel and Lima, Priscila Machado Vieira and de Penning, Leo and Pinkas, Gadi and Poon, Hoifung and Zaverucha, Gerson},
eprint = {1711.03902},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Besold et al/Besold et al. - 2017 - Neural-Symbolic Learning and Reasoning A Survey and Interpretation.pdf:pdf},
pages = {1--58},
title = {{Neural-Symbolic Learning and Reasoning: A Survey and Interpretation}},
url = {http://arxiv.org/abs/1711.03902},
year = {2017}
}
@article{Cox2016,
abstract = {Many algorithms have been presented in artificial intelligence for problem solving and planning. Given a goal, these algorithms search for solutions that achieve a goal state by actions or interactions with an environment. However a major assumption is that goals are given, usually by a user directly as input or as part of the problem definition. Furthermore, once given, the goals do not change. Here we formalize the notion that goal specification and goal change are themselves major parts of the problem-solving process. We apply this model to learning in a goal reasoning context. 1.},
author = {Cox, Michael T.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Advances in Cognitive Systems/Cox/Cox - 2016 - A Model of Planning, Action and Interpretation with Goal Reasoning.pdf:pdf},
journal = {Advances in Cognitive Systems},
number = {4},
pages = {1--16},
title = {{A Model of Planning, Action and Interpretation with Goal Reasoning}},
url = {https://smartech.gatech.edu/bitstream/handle/1853/53646/Technical{\%}5CnReport{\%}5CnGT-IRIM-CR-2015-001.pdf{\%}7B{\#}{\%}7Dpage=40{\%}5Cnhttps://smartech.gatech.edu/bitstream/handle/1853/53646/Technical Report GT-IRIM-CR-2015-001.pdf{\%}23page=40},
year = {2016}
}
@article{Hassabis2017,
abstract = {The fields of neuroscience and artificial intelligence (AI) have a long and intertwined history. In more recent times, however, communication and collaboration between the two fields has become less commonplace. In this article, we argue that better understanding biological brains could play a vital role in building intelligent machines. We survey historical interactions between the AI and neuroscience fields and emphasize current advances in AI that have been inspired by the study of neural computation in humans and other animals. We conclude by highlighting shared themes that may be key for advancing future research in both fields. Hassabis et al. review how neuroscience has informed research in artificial intelligence. They argue that a better understanding of biological brains will play a vital role in building intelligent machines.},
archivePrefix = {arXiv},
arxivId = {1404.7282},
author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
doi = {10.1016/j.neuron.2017.06.011},
eprint = {1404.7282},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Neuron/Hassabis et al/Hassabis et al. - 2017 - Neuroscience-Inspired Artificial Intelligence.pdf:pdf},
isbn = {0896-6273},
issn = {10974199},
journal = {Neuron},
keywords = {artificial intelligence,brain,cognition,learning,neural network},
number = {2},
pages = {245--258},
pmid = {28728020},
publisher = {Elsevier Inc.},
title = {{Neuroscience-Inspired Artificial Intelligence}},
url = {http://dx.doi.org/10.1016/j.neuron.2017.06.011},
volume = {95},
year = {2017}
}
@article{Haarnoja2018a,
abstract = {We address the problem of learning hierarchical deep neural network policies for reinforcement learning. Our aim is to design a hierarchical reinforcement learning algorithm that can construct hierarchical representations in bottom-up layerwise fashion. In contrast to methods that explicitly restrict or cripple lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse strategies via a maximum entropy reinforcement learning objective. Each layer is also augmented with latent random variables, which are sampled from a prior distribution during the training of that layer. The maximum entropy objective causes these latent variables to be incorporated into the layer's policy, and the higher level layer can directly control the behavior of the lower layer through this latent space. Furthermore, by constraining the mapping from latent variables to actions to be invertible, higher layers retain full expressivity: neither the higher layers nor the lower layers are constrained in their behavior. Our experimental evaluation demonstrates that we can improve on the performance of single-layer policies on standard benchmark tasks simply by adding additional layers, and that our method can solve more complex sparse-reward tasks by learning higher-level policies on top of high-entropy skills optimized for simple low-level objectives.},
archivePrefix = {arXiv},
arxivId = {1804.02808},
author = {Haarnoja, Tuomas and Hartikainen, Kristian and Abbeel, Pieter and Levine, Sergey},
eprint = {1804.02808},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Haarnoja et al/Haarnoja et al. - 2018 - Latent Space Policies for Hierarchical Reinforcement Learning.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Haarnoja et al/Haarnoja et al. - 2018 - Latent Space Policies for Hierarchical Reinforcement Learning.jpg:jpg},
issn = {1938-7228},
title = {{Latent Space Policies for Hierarchical Reinforcement Learning}},
url = {http://arxiv.org/abs/1804.02808},
year = {2018}
}
@inproceedings{Goel2017a,
author = {Goel, Karan and Mu, Tong and Brunskill, Emma},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Goel, Mu, Brunskill/Goel, Mu, Brunskill - 2017 - Optimal Hierarchical Policy Extraction From Noisy Imperfect Demonstrations.pdf:pdf},
title = {{Optimal Hierarchical Policy Extraction From Noisy Imperfect Demonstrations}},
url = {https://drive.google.com/file/d/101FsZkczKMfGeUBTP-089mhkTeepj8IW/view},
year = {2017}
}
@article{Madl2017,
abstract = {Computational cognitive models of spatial memory often neglect difficulties posed by the real world, such as sensory noise, uncertainty, and high spatial complexity. On the other hand, robotics is unconcerned with understanding biological cognition. Here, we describe a computational framework for robotic architectures aiming to function in realistic environments, as well as to be cognitively plausible. We motivate and describe several mechanisms towards achieving this despite the sensory noise and spatial complexity inherent in the physical world. We tackle error accumulation during path integration by means of Bayesian localization, and loop closing with sequential gradient descent. Finally, we outline a method for structuring spatial representations using metric learning and clustering. Crucially, unlike the algorithms of traditional robotics, we show that these mechanisms can be implemented in neuronal or cognitive models. We briefly outline a concrete implementation of the proposed framework as part of the LIDA cognitive architecture, and argue that this kind of probabilistic framework is well-suited for use in cognitive robotic architectures aiming to combine spatial functionality and psychological plausibility.},
author = {Madl, Tamas and Franklin, Stan and Chen, Ke and Trappl, Robert},
doi = {10.1016/j.cogsys.2017.08.002},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Cognitive Systems Research/Madl et al/Madl et al. - 2017 - A computational cognitive framework of spatial memory in brains and robots.pdf:pdf},
issn = {13890417},
journal = {Cognitive Systems Research},
keywords = {Bayesian brain,Cognitive architecture,Computational cognitive modeling,LIDA,Spatial memory},
pages = {147--172},
publisher = {Elsevier B.V.},
title = {{A computational cognitive framework of spatial memory in brains and robots}},
url = {https://doi.org/10.1016/j.cogsys.2017.08.002},
volume = {47},
year = {2017}
}
@article{Barto2003,
abstract = {Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of {\{}semi-Markov{\}} decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.},
author = {Barto, Andrew G. and Mahadevan, Sridhar},
doi = {10.1023/A:1025696116075},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Discrete Event Dynamic Systems/Barto, Mahadevan/Barto, Mahadevan - 2003 - Recent Advances in Hierarchical Reinforcement Learning.pdf:pdf},
isbn = {0924-6703},
issn = {09246703},
journal = {Discrete Event Dynamic Systems},
pages = {341--379},
title = {{Recent Advances in Hierarchical Reinforcement Learning}},
url = {http://dx.doi.org/10.1023/A:1025696116075},
volume = {13},
year = {2003}
}
@article{Hawes2011,
abstract = {The ability to achieve ones goals is a defining characteristic of intelligent behaviour. A great many existing theories, systems and research programmes address the problems associated with generating behaviour to achieve a goal; much fewer address the related problems of how and why goals should be generated in an intelligent artifact, and how a subset of all possible goals are selected as the focus of behaviour. It is research into these problems of motivation, which this article aims to stimulate. Building from the analysis of a scenario involving a futuristic household robot, we extend an existing account of motivation in intelligent systems to provide a framework for surveying relevant literature in AI and robotics. This framework guides us to look at the problems of encoding drives (how the needs of the system are represented), goal generation (how particular instances of goals are generated from the drives with reference to the current state), and goal selection (how the system determines which goal instances to act on). After surveying a variety of existing approaches in these terms, we build on the results of the survey to sketch a design for a new motive management framework which goes beyond the current state of the art. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Hawes, Nick},
doi = {10.1016/j.artint.2011.02.002},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Artificial Intelligence/Hawes/Hawes - 2011 - A survey of motivation frameworks for intelligent systems.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Agent architecture,Goal-directed behaviour,Motivation,Planning},
number = {5-6},
pages = {1020--1036},
publisher = {Elsevier B.V.},
title = {{A survey of motivation frameworks for intelligent systems}},
url = {http://dx.doi.org/10.1016/j.artint.2011.02.002},
volume = {175},
year = {2011}
}
@book{Sun1994,
abstract = {Concerned with understanding and modeling commonsense reasoning with a combination of rules and similarities under a connectionist rubric. Examines the areas of reasoning, connectionist models, inheritance, causality, rule-based systems and similarity-based reasoning. Introduces a new structure, a novel connectionist architecture and a set of fresh ideas leading to new applications.},
author = {Sun, Ron},
pages = {273},
publisher = {Wiley-Interscience},
title = {{Integrating Rules and Connectionism for Robust Commonsense Reasoning}},
year = {1994}
}
@article{Frank2012,
abstract = {Growing evidence suggests that the prefrontal cortex (PFC) is organized hierarchically, with more anterior regions having increasingly abstract representations. How does this organization support hierarchical cognitive control and the rapid discovery of abstract action rules? We present computational models at different levels of description. A neural circuit model simulates interacting corticostriatal circuits organized hierarchically. In each circuit, the basal ganglia gate frontal actions, with some striatal units gating the inputs to PFC and others gating the outputs to influence response selection. Learning at all of these levels is accomplished via dopaminergic reward prediction error signals in each corticostriatal circuit. This functionality allows the system to exhibit conditional if-then hypothesis testing and to learn rapidly in environments with hierarchical structure. We also develop a hybrid Bayesian-reinforcement learning mixture of experts (MoE) model, which can estimate the most likely hypothesis state of individual participants based on their observed sequence of choices and rewards. This model yields accurate probabilistic estimates about which hypotheses are attended by manipulating attentional states in the generative neural model and recovering them with the MoE model. This 2-pronged modeling approach leads to multiple quantitative predictions that are tested with functional magnetic resonance imaging in the companion paper.},
author = {Frank, Michael J. and Badre, David},
doi = {10.1093/cercor/bhr114},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Cerebral cortex (New York, N.Y. 1991)/Frank, Badre/Frank, Badre - 2012 - Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1 Computational analysis.pdf:pdf},
issn = {1460-2199},
journal = {Cerebral cortex (New York, N.Y. : 1991)},
keywords = {Computer Simulation,Corpus Striatum,Corpus Striatum: cytology,Corpus Striatum: physiology,Humans,Learning,Learning: physiology,Models,Neural Pathways,Neural Pathways: cytology,Neural Pathways: physiology,Neurological,Neurons,Neurons: physiology,Prefrontal Cortex,Prefrontal Cortex: cytology,Prefrontal Cortex: physiology,Reinforcement (Psychology)},
number = {3},
pages = {509--26},
pmid = {21693490},
title = {{Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: Computational analysis}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3278315{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {22},
year = {2012}
}
@article{Levy2018a,
abstract = {Reinforcement Learning (RL) algorithms can suffer from poor sample efficiency when rewards are delayed and sparse. We introduce a solution that enables agents to learn temporally extended actions at multiple levels of abstraction in a sample efficient and automated fashion. Our approach combines universal value functions and hindsight learning, allowing agents to learn policies belonging to different time scales in parallel. We show that our method significantly accelerates learning in a variety of discrete and continuous tasks.},
archivePrefix = {arXiv},
arxivId = {1805.08180},
author = {Levy, Andrew and Platt, Robert and Saenko, Kate},
eprint = {1805.08180},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Levy, Platt, Saenko/Levy, Platt, Saenko - 2018 - Hierarchical Reinforcement Learning with Abductive Planning.pdf:pdf},
title = {{Hierarchical Reinforcement Learning with Abductive Planning}},
url = {http://arxiv.org/abs/1805.08180},
year = {2018}
}
@inproceedings{Mannor2004a,
abstract = {We consider a graph theoretic approach for automatic construction of options in a dynamic environment. A map of the environment is generated on-line by the learning agent, representing the topological structure of the state transitions. A clustering algorithm is then used to partition the state space to different regions. Policies for reaching the different parts of the space are separately learned and added to the model in a form of options (macro-actions). The options are used for accelerating the Q-Learning algorithm. We extend the basic algorithm and consider building a map that includes preliminary indication of the location of "interesting" regions of the state space, where the value gradient is significant and additional exploration might be beneficial. Experiments indicate significant speedups, especially in the initial learning phase.},
address = {New York, New York, USA},
author = {Mannor, Shie and Menache, Ishai and Hoze, Amit and Klein, Uri},
booktitle = {Twenty-first international conference on Machine learning - ICML '04},
doi = {10.1145/1015330.1015355},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Twenty-first international conference on Machine learning - ICML '04/Mannor et al/Mannor et al. - 2004 - Dynamic abstraction in reinforcement learning via clustering.pdf:pdf},
isbn = {1581138285},
keywords = {clustering,hierarchical reinforcement learning,options,q-learning,reinforcement learning},
pages = {71},
publisher = {ACM Press},
title = {{Dynamic abstraction in reinforcement learning via clustering}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015355},
year = {2004}
}
@article{Levy2018,
abstract = {The ability to learn at different resolutions in time may help overcome one of the main challenges in deep reinforcement learning -- sample efficiency. Hierarchical agents that operate at different levels of temporal abstraction can learn tasks more quickly because they can divide the work of learning behaviors among multiple policies and can also explore the environment at a higher level. In this paper, we present a novel approach to hierarchical reinforcement learning called Hierarchical Actor-Critic (HAC) that enables agents to learn to break down problems involving continuous action spaces into simpler subproblems belonging to different time scales. HAC has two key advantages over most existing hierarchical learning methods: (i) the potential for faster learning as agents learn short policies at each level of the hierarchy and (ii) an end-to-end approach. We demonstrate that HAC significantly accelerates learning in a series of tasks that require behavior over a relatively long time horizon and involve sparse rewards.},
archivePrefix = {arXiv},
arxivId = {1712.00948},
author = {Levy, Andrew and Platt, Robert and Saenko, Kate},
eprint = {1712.00948},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Levy, Platt, Saenko/Levy, Platt, Saenko - 2018 - Hierarchical Actor-Critic.pdf:pdf},
title = {{Hierarchical Actor-Critic}},
url = {http://arxiv.org/abs/1712.00948},
year = {2018}
}
@article{Konidaris2012a,
abstract = {We present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to significantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that significantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-specific skills.},
author = {Konidaris, George and Scheidwasser, Ilya and Barto, Andrew G.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Journal of Machine Learning Research/Konidaris, Scheidwasser, Barto/Konidaris, Scheidwasser, Barto - 2012 - Transfer in reinforcement learning via shared features.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {reinforcement learning,shaping,skills,transfer},
number = {1},
pages = {1333--1371},
title = {{Transfer in reinforcement learning via shared features}},
url = {http://dl.acm.org/citation.cfm?id=2503308.2343689{\%}5Cnhttp://dl.acm.org/ft{\_}gateway.cfm?id=2343689{\&}type=pdf},
volume = {13},
year = {2012}
}
@article{Kurup2011,
abstract = {Generating future states of the world is an essential component of high level cognitive tasks such as planning. We explore the notion that such future state generation is more widespread and forms an integral part of cognition. We call these generated states expectations, and propose that cognitive systems constantly generate expectations, match them to observed behavior and react when a difference exists between the two. We describe an ACT R model that performs expectation driven cognition on two tasks pedestrian tracking and behavior classification. The model generates expectations of pedestrian movements to track them. The model also uses differences in expectations to identify distinctive features that differentiate these tracks. During learning, the model learns the association between these features and the various behaviors. During testing, it classifies pedestrian tracks by recalling the behavior associated with the features of each track. We tested the model on both single and multiple behavior datasets and compared the results against a k NN classifier. The k NN classifier outperformed the model in correct classifications, but the model had fewer incorrect classifications in the multiple behavior case, and both systems had about equal incorrect classifications in the single behavior case},
author = {Kurup, Unmesh and Lebiere, Christian and Stentz, Anthony and Hebert, Martial},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/AAAI Conference on Artificial Intelligence/Kurup et al/Kurup et al. - 2011 - Using Expectations to Drive Cognitive Behavior.pdf:pdf},
isbn = {9781577355687},
journal = {AAAI Conference on Artificial Intelligence},
keywords = {Special Track on Cognitive Systems},
pages = {221--227},
title = {{Using Expectations to Drive Cognitive Behavior}},
year = {2011}
}
@article{Vogt2003,
author = {Vogt, Paul},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Robotics and Autonomous Systems/Vogt/Vogt - 2003 - Anchoring of semiotic symbols.pdf:pdf},
journal = {Robotics and Autonomous Systems},
number = {2-3},
pages = {109--120},
title = {{Anchoring of semiotic symbols}},
volume = {43},
year = {2003}
}
@article{Kaplan2004,
abstract = {This chapter presents a generic internal reward system that drives an agent to increase the complexity of its behavior. This reward system does not reinforce a predefined task. Its purpose is to drive the agent to progress in learning given its embodiment and the environment in which it is placed. The dynamics created by such a system are studied first in a simple environment and then in the context of active vision.},
author = {Kaplan, Frederic and Oudeyer, Pierre-Yves},
doi = {10.1007/b99075},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Embodied artificial intelligence/Kaplan, Oudeyer/Kaplan, Oudeyer - 2004 - Maximizing learning progress An internal reward system for development.pdf:pdf},
isbn = {3-540-22484-X},
issn = {03029743},
journal = {Embodied artificial intelligence},
keywords = {Active vision,Artificial intelligence,Reward},
pages = {259--270},
title = {{Maximizing learning progress: An internal reward system for development}},
volume = {3139},
year = {2004}
}
@article{Parr1998,
abstract = {We present a new approach to reinforcement learning in which the poli- cies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework inwhich knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learn- ing and “behavior-based” or “teleo-reactive” approaches to control. We present provably convergent algorithms for problem-solving and learn- ing with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states. 1},
author = {Parr, Ronald and Russell, Stuart},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Neural Information Processing Systems (NIPS)/Parr, Russell/Parr, Russell - 1998 - Reinforcement learning with hierarchies of machines.pdf:pdf},
isbn = {0-262-10076-2},
issn = {0031-8116},
journal = {Neural Information Processing Systems (NIPS)},
pages = {1043--1049},
title = {{Reinforcement learning with hierarchies of machines}},
url = {http://www.cs.berkeley.edu/{~}russell/classes/cs294/s11/readings/Parr+Russell:1998.pdf},
year = {1998}
}
@article{Ding2017a,
abstract = {When a stimulus is presented, its encoding is known to progress from low-to high-level features. How these features are decoded to produce perception is less clear, and most models assume that decoding follows the same low-to high-level hierarchy of encoding. There are also theories arguing for global precedence, reversed hierarchy, or bidirectional processing, but they are descriptive without quantitative comparison with human perception. Moreover, ob-servers often inspect different parts of a scene sequentially to form overall perception, suggesting that perceptual decoding requires working memory, yet few models consider how working-memory properties may affect decoding hierarchy. We probed decoding hierarchy by comparing absolute judgments of single orientations and relative/ordinal judgments between two sequentially presented orientations. We found that lower-level, absolute judgments failed to account for higher-level, relative/ordinal judgments. However, when ordinal judgment was used to retrospectively decode memory representations of absolute orientations, striking aspects of absolute judgments, including the correlation and forward/backward afteref-fects between two reported orientations in a trial, were explained. We propose that the brain prioritizes decoding of higher-level features because they are more behaviorally relevant, and more invariant and categorical, and thus easier to specify and maintain in noisy working memory, and that more reliable higher-level decoding constrains less reliable lower-level decoding. Bayesian prior | interreport correlation | bidirectional tilt aftereffect | efficient coding | adaptation theory},
author = {Ding, Stephanie and Cueva, Christopher J and Tsodyks, Misha and Qian, Ning},
doi = {10.1073/pnas.1706906114},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the National Academy of Sciences/Ding et al/Ding et al. - 2017 - Visual perception as retrospective Bayesian decoding from high- to low-level features.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
month = {oct},
number = {43},
pages = {E9115--E9124},
title = {{Visual perception as retrospective Bayesian decoding from high- to low-level features}},
url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1706906114},
volume = {114},
year = {2017}
}
@inproceedings{Hengst2002a,
abstract = {An open problem in reinforcement learning is discovering hierarchical structure. HEXQ, an algorithm which automatically attempts to decompose and solve a model-free fac- tored MDP hierarchically is described. By searching for aliased Markov sub-space re- gions based on the state variables the algo- rithm uses temporal and state abstraction to construct a hierarchy of interlinked smaller MDPs.},
author = {Hengst, Bernhard},
booktitle = {Proceedings of the International Conference on Machine Learning ICML 2002},
doi = {10.1.1.9.5839},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the International Conference on Machine Learning ICML 2002/Hengst/Hengst - 2002 - Discovering hierarchy in reinforcement learning with HEXQ.pdf:pdf},
isbn = {1-55860-873-7},
pages = {243--250},
title = {{Discovering hierarchy in reinforcement learning with HEXQ}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.5839},
year = {2002}
}
@incollection{Papiez2018,
address = {Cham},
author = {Papiez, Piotr and Horzyk, Adrian},
doi = {10.1007/978-3-319-91253-0_17},
editor = {Rutkowski, Leszek and Scherer, Rafa{\l} and Korytkowski, Marcin and Pedrycz, Witold and Tadeusiewicz, Ryszard and Zurada, Jacek M.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Papiez, Horzyk/Papiez, Horzyk - 2018 - Motivated Reinforcement Learning Using Self-Developed Knowledge in Autonomous Cognitive Agent.pdf:pdf},
isbn = {978-3-319-91252-3},
keywords = {ARIMA,LSTM,Multi-step forecasting,Time series analysis,time series analysis},
pages = {170--182},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Motivated Reinforcement Learning Using Self-Developed Knowledge in Autonomous Cognitive Agent}},
url = {http://link.springer.com/10.1007/978-3-319-91253-0 http://link.springer.com/10.1007/978-3-319-91253-0{\_}17},
volume = {10841},
year = {2018}
}
@article{Touretzky1985,
abstract = {Pattern matching and variable binding are easily implemented in conventional computer architectures, but not necessarily in all architectures. In a distributed neural network architecture each symbol is represented by activity in many units and each unit contributes to the representation of many symbols. Manipulating symbols using this type of distributed representation is not as easy as with a local representation whore each unit denotes one symbol, but there is evidence that the distributed approach is the one chosen by nature. We describe a working implementation of a production system interpreter in a neural network using distributed representations for both symbols and rules. The research provides a detailed account of two important symbolic reasoning operations, pattern matching and variable binding, as emergent properties of collections of neuron-like elements. The success of our production system implementation goes some way towards answering a common criticism of connectionist theories: that they aren't powerful enough to do symbolic reasoning.},
author = {Touretzky, D S and Hinton, G E},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the Ninth International Joint Conference on Artificial Intelligence/Touretzky, Hinton/Touretzky, Hinton - 1985 - Symbols among the neurons details of a connectionist inference architecture.pdf:pdf},
isbn = {CMU-CS-A},
journal = {Proceedings of the Ninth International Joint Conference on Artificial Intelligence},
pages = {238--243},
title = {{Symbols among the neurons: details of a connectionist inference architecture}},
volume = {1},
year = {1985}
}
@book{DAvilaGarcez2009,
author = {d'Avila Garcez, Artur S. and Lamb, Luis C. and Gabbay, Dov M.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/d'Avila Garcez, Lamb, Gabbay/d'Avila Garcez, Lamb, Gabbay - 2009 - Neural-Symbolic Cognitive Reasoning.pdf:pdf},
isbn = {9783540732457},
publisher = {Springer},
title = {{Neural-Symbolic Cognitive Reasoning}},
year = {2009}
}
@incollection{Palamara2009,
author = {Palamara, Pier Francesco and Ziparo, Vittorio a and Iocchi, Luca and Nardi, Daniele and Lima, Pedro},
booktitle = {RoboCup 2008: Robot Soccer World Cup XII},
editor = {Iocchi, Luca and Matsubara, Hitoshi and Weitzenfeld, Alfredo and Zhou, Changjiu},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/RoboCup 2008 Robot Soccer World Cup XII/Palamara et al/Palamara et al. - 2009 - Teamwork Design Based on Petri Net Plans.pdf:pdf},
pages = {200--211},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Teamwork Design Based on Petri Net Plans}},
year = {2009}
}
@article{Kansky2017,
abstract = {The recent adaptation of deep neural network-based methods to reinforcement learning and planning domains has yielded remarkable progress on individual tasks. Nonetheless, progress on task-to-task transfer remains limited. In pursuit of efficient and robust generalization, we introduce the Schema Network, an object-oriented generative physics simulator capable of disentangling multiple causes of events and reasoning backward through causes to achieve goals. The richly structured architecture of the Schema Network can learn the dynamics of an environment directly from data. We compare Schema Networks with Asynchronous Advantage Actor-Critic and Progressive Networks on a suite of Breakout variations, reporting results on training efficiency and zero-shot generalization, consistently demonstrating faster, more robust learning and better transfer. We argue that generalizing from limited data and learning causal relationships are essential abilities on the path toward generally intelligent systems.},
archivePrefix = {arXiv},
arxivId = {1706.04317},
author = {Kansky, Ken and Silver, Tom and M{\'{e}}ly, David A. and Eldawy, Mohamed and L{\'{a}}zaro-Gredilla, Miguel and Lou, Xinghua and Dorfman, Nimrod and Sidor, Szymon and Phoenix, Scott and George, Dileep},
eprint = {1706.04317},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Kansky et al/Kansky et al. - 2017 - Schema Networks Zero-shot Transfer with a Generative Causal Model of Intuitive Physics.pdf:pdf},
isbn = {9781510855144},
title = {{Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics}},
url = {http://arxiv.org/abs/1706.04317},
year = {2017}
}
@inproceedings{Rasmussen1998,
author = {Rasmussen, Daniel and Eliasmith, Chris},
booktitle = {Proceedings of the 36th Annual Conference of the Cognitive Science Society},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 36th Annual Conference of the Cognitive Science Society/Rasmussen, Eliasmith/Rasmussen, Eliasmith - 2014 - A neural model of hierarchical reinforcement learning.pdf:pdf},
keywords = {brain could achieve its,hierarchical,hrl into a theory,neural engineering framework,neural model,of neural function,providing a hypothe-,reinforcement learning,sis for how the,strengths in scaling},
number = {1},
pages = {1252--1257},
title = {{A neural model of hierarchical reinforcement learning}},
year = {2014}
}
