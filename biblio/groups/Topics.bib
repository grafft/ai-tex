Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@incollection{Hengst2012a,
abstract = {Hierarchical decomposition tackles complex problems by reducing them to a smaller set of interrelated problems. The smaller problems are solved separately and the results re-combined to find a solution to the original problem. It is well known that the na{\"{i}}ve application of reinforcement learning (RL) techniques fails to scale to more complex domains. This Chapter introduces hierarchical approaches to reinforcement learning that hold out the promise of reducing a reinforcement learning problems to a manageable size. Hierarchical Reinforcement Learning (HRL) rests on finding good re-usable temporally extended actions that may also provide opportunities for state abstraction. Methods for reinforcement learning can be extended to work with abstract states and actions over a hierarchy of subtasks that decompose the original problem, potentially reducing its computational complexity. We use a four-room task as a running example to illustrate the various concepts and approaches, including algorithms that can automatically learn the hierarchical structure from interactions with the domain.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.02971},
author = {Hengst, Bernhard},
booktitle = {Reinforcement Learning},
doi = {10.1007/978-3-642-27645-3_9},
eprint = {arXiv:1509.02971},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Hengst/2012/Hierarchical Approaches.pdf:pdf},
isbn = {978-3-642-27644-6},
issn = {18726240},
pages = {293--323},
title = {{Hierarchical Approaches}},
url = {http://dx.doi.org/10.1007/978-3-642-27645-3{\_}9 http://link.springer.com/10.1007/978-3-642-27645-3{\_}9},
year = {2012}
}
@inproceedings{Mannor2004a,
abstract = {We consider a graph theoretic approach for automatic construction of options in a dynamic environment. A map of the environment is generated on-line by the learning agent, representing the topological structure of the state transitions. A clustering algorithm is then used to partition the state space to different regions. Policies for reaching the different parts of the space are separately learned and added to the model in a form of options (macro-actions). The options are used for accelerating the Q-Learning algorithm. We extend the basic algorithm and consider building a map that includes preliminary indication of the location of "interesting" regions of the state space, where the value gradient is significant and additional exploration might be beneficial. Experiments indicate significant speedups, especially in the initial learning phase.},
address = {New York, New York, USA},
author = {Mannor, Shie and Menache, Ishai and Hoze, Amit and Klein, Uri},
booktitle = {Twenty-first international conference on Machine learning - ICML '04},
doi = {10.1145/1015330.1015355},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Mannor et al/2004/Dynamic abstraction in reinforcement learning via clustering.pdf:pdf},
isbn = {1581138285},
keywords = {clustering,hierarchical reinforcement learning,options,q-learning,reinforcement learning},
pages = {71},
publisher = {ACM Press},
title = {{Dynamic abstraction in reinforcement learning via clustering}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015355},
year = {2004}
}
@article{Touretzky1985,
abstract = {Pattern matching and variable binding are easily implemented in conventional computer architectures, but not necessarily in all architectures. In a distributed neural network architecture each symbol is represented by activity in many units and each unit contributes to the representation of many symbols. Manipulating symbols using this type of distributed representation is not as easy as with a local representation whore each unit denotes one symbol, but there is evidence that the distributed approach is the one chosen by nature. We describe a working implementation of a production system interpreter in a neural network using distributed representations for both symbols and rules. The research provides a detailed account of two important symbolic reasoning operations, pattern matching and variable binding, as emergent properties of collections of neuron-like elements. The success of our production system implementation goes some way towards answering a common criticism of connectionist theories: that they aren't powerful enough to do symbolic reasoning.},
author = {Touretzky, D S and Hinton, G E},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Touretzky, Hinton/1985/Symbols among the neurons details of a connectionist inference architecture.pdf:pdf},
isbn = {CMU-CS-A},
journal = {Proceedings of the Ninth International Joint Conference on Artificial Intelligence},
pages = {238--243},
title = {{Symbols among the neurons: details of a connectionist inference architecture}},
volume = {1},
year = {1985}
}
@inproceedings{Eckstein2017,
author = {Eckstein, Maria K and Collins, Anne G E},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Eckstein, Collins/2017/Combining intrinsic motivation and hierarchical reinforcement learning.pdf:pdf},
title = {{Combining intrinsic motivation and hierarchical reinforcement learning}},
url = {https://drive.google.com/file/d/1zjc6q0i2J4JoJ8fnVnQslPRoiiQku3Gl/view},
year = {2017}
}
@inproceedings{Goel2017a,
author = {Goel, Karan and Mu, Tong and Brunskill, Emma},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Goel, Mu, Brunskill/2017/Optimal Hierarchical Policy Extraction From Noisy Imperfect Demonstrations.pdf:pdf},
title = {{Optimal Hierarchical Policy Extraction From Noisy Imperfect Demonstrations}},
url = {https://drive.google.com/file/d/101FsZkczKMfGeUBTP-089mhkTeepj8IW/view},
year = {2017}
}
@article{Cox2007,
author = {Cox, Michael T},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cox/2007/Perpetual Self-Aware Cognitive Agents.pdf:pdf},
journal = {AI Magazine},
number = {1},
pages = {32--45},
title = {{Perpetual Self-Aware Cognitive Agents}},
volume = {28},
year = {2007}
}
@article{Hayes2016b,
abstract = {Collaboration between humans and robots requires solutions to an array of challenging problems, including multi-agent planning, state estimation, and goal inference. There already exist feasible solutions for many of these challenges, but they depend upon having rich task models. In this work we detail a novel type of Hierarchical Task Network we call a Clique/Chain HTN (CC-HTN), alongside an algorithm for autonomously constructing them from topological properties derived from graphical task representations. As the presented method relies on the structure of the task itself, our work imposes no particular type of symbolic insight into motor primitives or environmental representation, making it applicable to a wide variety of use cases critical to human-robot interaction. We present evaluations within a multi-resolution goal inference task and a transfer learning application showing the utility of our approach.},
author = {Hayes, Bradley and Scassellati, Brian},
doi = {10.1109/ICRA.2016.7487760},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Hayes, Scassellati/2016/Autonomously constructing hierarchical task networks for planning and human-robot collaboration.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {5469--5476},
title = {{Autonomously constructing hierarchical task networks for planning and human-robot collaboration}},
volume = {2016-June},
year = {2016}
}
@article{Al-Emran2015,
author = {Al-Emran, Mostafa},
doi = {10.12785/ijcds/040207},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Al-Emran/2015/Hierarchical Reinforcement Learning - A Survey.pdf:pdf},
issn = {2210-142X},
journal = {International Journal of Computing and Digital Systems},
keywords = {hierarchical reinforcement learning,q-learning,reinforcement learning},
month = {apr},
number = {2},
pages = {137--143},
title = {{Hierarchical Reinforcement Learning - A Survey}},
url = {https://journal.journals.uob.edu.bh//Article/ArticleFile/2144},
volume = {4},
year = {2015}
}
@article{Konidaris2016a,
abstract = {We describe a framework for building abstraction hierarchies whereby an agent alternates skill- and representation-acquisition phases to construct a sequence of increasingly abstract Markov decision processes. Our formulation builds on recent results showing that the appropriate abstract representation of a problem is specified by the agent's skills. We describe how such a hierarchy can be used for fast planning, and illustrate the construction of an appropriate hierarchy for the Taxi domain.},
archivePrefix = {arXiv},
arxivId = {1509.07582},
author = {Konidaris, George},
doi = {10.1016/j.surg.2015.03.052.Rate},
eprint = {1509.07582},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Konidaris/2016/Constructing abstraction hierarchies using a skill-symbol loop.pdf:pdf},
isbn = {6176366127},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning},
pages = {1648--1654},
pmid = {28579718},
title = {{Constructing abstraction hierarchies using a skill-symbol loop}},
volume = {2016-Janua},
year = {2016}
}
@inproceedings{Hengst2002a,
abstract = {An open problem in reinforcement learning is discovering hierarchical structure. HEXQ, an algorithm which automatically attempts to decompose and solve a model-free fac- tored MDP hierarchically is described. By searching for aliased Markov sub-space re- gions based on the state variables the algo- rithm uses temporal and state abstraction to construct a hierarchy of interlinked smaller MDPs.},
author = {Hengst, Bernhard},
booktitle = {Proceedings of the International Conference on Machine Learning ICML 2002},
doi = {10.1.1.9.5839},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Hengst/2002/Discovering hierarchy in reinforcement learning with HEXQ.pdf:pdf},
isbn = {1-55860-873-7},
pages = {243--250},
title = {{Discovering hierarchy in reinforcement learning with HEXQ}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.5839},
year = {2002}
}
@article{Dann2017,
abstract = {In platform videogames, players are frequently tasked with solving medium-term navigation prob-lems in order to gather items or powerups. Arti-ficial agents must generally obtain some form of direct experience before they can solve such tasks. Experience is gained either through training runs, or by exploiting knowledge of the game's physics to generate detailed simulations. Human players, on the other hand, seem to look ahead in high-level, abstract steps. Motivated by human play, we intro-duce an approach that leverages not only abstract " skills " , but also knowledge of what those skills can and cannot achieve. We apply this approach to Infinite Mario, where despite facing randomly generated, maze-like levels, our agent is capable of deriving complex plans in real-time, without rely-ing on perfect knowledge of the game's physics.},
author = {Dann, Michael and Zambetta, Fabio and Thangarajah, John},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Dann, Zambetta, Thangarajah/2017/Real-time navigation in classical platform games via skill reuse.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning: Reinforcement Learning,Multidisciplinary Topics and Applications: Compute},
pages = {1582--1588},
title = {{Real-time navigation in classical platform games via skill reuse}},
year = {2017}
}
@article{Daniel2016a,
abstract = {Tasks that require many sequential decisions are hard to solve using conventional rein- forcement learning algorithms. Based on the option framework, we propose a model which aims to alleviate these concerns. Instead of learning a single monolithic policy, the agent learns a set of simpler sub-policies as well as the initiation and termination probabili- ties for each of those sub-policies. While ex- isting option learning algorithms frequently require manual specification of components such as the sub-policies, we present an algo- rithm which infers all relevant components of the option framework from data. We present results on SMDPs with discrete as well as continuous state-action spaces. The results show that the presented algorithm can combine simple sub-policies to solve complex tasks and can improve learning performance on simpler tasks.},
author = {Daniel, Christian and van Hoof, Herke and Peters, Jan and Neumann, Gerhard},
doi = {10.1007/s10994-016-5580-x},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Daniel et al/2016/Probabilistic inference for determining options in reinforcement learning.pdf:pdf},
issn = {15730565},
journal = {Machine Learning},
keywords = {Options,Reinforcement learning,Robot learning,Semi Markov decision process},
number = {2-3},
pages = {337--357},
publisher = {Springer US},
title = {{Probabilistic inference for determining options in reinforcement learning}},
volume = {104},
year = {2016}
}
@inproceedings{Singh2005a,
author = {Singh, Satinder and Barto, Andrew G. and Chentanez, Nuttapong},
booktitle = {Proceeding of NIPS 2005},
doi = {10.1.1.123.395},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Singh, Barto, Chentanez/2005/Intrinsically motivated reinforcement learning.pdf:pdf},
pages = {1281--1288},
title = {{Intrinsically motivated reinforcement learning}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2005{\_}724.pdf},
year = {2005}
}
@book{Cox2011,
abstract = {The capacity to think about our own thinking may lie at the heart of what it means to be both human and intelligent. Philosophers and cognitive scientists have investigated these matters for many years. Researchers in artificial intelligence have gone further, attempting to implement actual machines that mimic, simulate, and perhaps even replicate this capacity, called metareasoning. In this volume, leading authorities offer a variety of perspectives--drawn from philosophy, cognitive psychology, and computer science--on reasoning about the reasoning process. The book offers a simple model of reasoning about reason as a framework for its discussions. Following this framework, the contributors consider metalevel control of computational activities, introspective monitoring, distributed metareasoning, and, putting all these aspects of metareasoning together, models of the self. Taken together, the chapters offer an integrated narrative on metareasoning themes from both artificial intelligence and cognitive science perspectives.},
doi = {10.7551/mitpress/9780262014809.001.0001},
editor = {Cox, Michael T. and Raja, Anita},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2011/Metareasoning Thinking about thinking.pdf:pdf},
isbn = {9780262014809},
pages = {340},
publisher = {The MIT Press},
title = {{Metareasoning: Thinking about thinking}},
year = {2011}
}
@inproceedings{Roncone2017,
abstract = {Collaborative robots represent a clear added value to manufacturing, as they promise to increase productivity and improve working conditions of such environments. Although modern robotic systems have become safe and reliable enough to operate close to human workers on a day-to-day basis, the workload is still skewed in favor of a limited contribution from the robot's side, and a significant cognitive load is allotted to the human. We believe the transition from robots as recipients of human instruction to robots as capable collaborators hinges around the implementation of transparent systems, where mental models about the task are shared between peers, and the human partner is freed from the responsibility of taking care of both actors. In this work, we implement a transparent task planner able to be deployed in realistic, near-future applications. The proposed framework is capable of basic reasoning capabilities for what concerns role assignment and task allocation, and it interfaces with the human partner at the level of abstraction he is most comfortable with. The system is readily available to non-expert users, and programmable with high-level commands in an intuitive interface. Our results demonstrate an overall improvement in terms of completion time, as well as a reduced cognitive load for the human partner.},
author = {Roncone, Alessandro and Mangin, Olivier and Scassellati, Brian},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2017.7989122},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Roncone, Mangin, Scassellati/2017/Transparent role assignment and task allocation in human robot collaboration.pdf:pdf},
isbn = {9781509046331},
issn = {10504729},
keywords = {Human Factors and H,Social Human-Robot Interaction},
pages = {1014--1021},
publisher = {IEEE},
title = {{Transparent role assignment and task allocation in human robot collaboration}},
year = {2017}
}
@inproceedings{Mehta2008b,
abstract = {We present an algorithm, HI-MAT (Hierar- chy Induction via Models And Trajectories), that discovers MAXQ task hierarchies by ap- plying dynamic Bayesian network models to a successful trajectory from a source rein- forcement learning task. HI-MAT discovers subtasks by analyzing the causal and tem- poral relationships among the actions in the trajectory. Under appropriate assumptions, HI-MAT induces hierarchies that are consis- tent with the observed trajectory and have compact value-function tables employing safe state abstractions. We demonstrate empir- ically that HI-MAT constructs compact hi- erarchies that are comparable to manually- engineered hierarchies and facilitate signifi- cant speedup in learning when transferred to a target task.},
address = {New York, New York, USA},
author = {Mehta, Neville and Ray, Soumya and Tadepalli, Prasad and Dietterich, Thomas},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390238},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Mehta et al/2008/Automatic discovery and transfer of MAXQ hierarchies.pdf:pdf},
isbn = {9781605582054},
pages = {648--655},
publisher = {ACM Press},
title = {{Automatic discovery and transfer of MAXQ hierarchies}},
url = {http://portal.acm.org/citation.cfm?id=1390238 http://portal.acm.org/citation.cfm?doid=1390156.1390238},
year = {2008}
}
@incollection{Menache2002a,
abstract = {We present the Q-Cut algorithm, a graph theoretic approach for automatic detection of sub-goals in a dynamic environment, which is used for acceleration of the Q-Learning algorithm. The learning agent creates an on-line map of the process history, and uses an efficient Max-Flow/Min-Cut algorithm for identifying bottlenecks. The policies for reaching bottlenecks are separately learned and added to the model in a form of options (macro-actions). We then extend the basic Q-Cut algorithm to the Segmented Q-Cut algorithm, which uses previously identified bottlenecks for state space partitioning, necessary for finding additional bottlenecks in complex environments. Experiments show significant performance improvements, particulary in the initial learning phase.},
author = {Menache, Ishai and Mannor, Shie and Shimkin, Nahum},
booktitle = {ECML 2002: Machine Learning: ECML 2002},
doi = {10.1007/3-540-36755-1_25},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Menache, Mannor, Shimkin/2002/Q-Cut—Dynamic Discovery of Sub-goals in Reinforcement Learning.pdf:pdf},
isbn = {978-3-540-44036-9, 978-3-540-36755-0},
issn = {16113349},
pages = {295--306},
title = {{Q-Cut—Dynamic Discovery of Sub-goals in Reinforcement Learning}},
url = {http://link.springer.com/10.1007/3-540-36755-1{\_}25},
year = {2002}
}
@article{Ding2017a,
abstract = {When a stimulus is presented, its encoding is known to progress from low-to high-level features. How these features are decoded to produce perception is less clear, and most models assume that decoding follows the same low-to high-level hierarchy of encoding. There are also theories arguing for global precedence, reversed hierarchy, or bidirectional processing, but they are descriptive without quantitative comparison with human perception. Moreover, ob-servers often inspect different parts of a scene sequentially to form overall perception, suggesting that perceptual decoding requires working memory, yet few models consider how working-memory properties may affect decoding hierarchy. We probed decoding hierarchy by comparing absolute judgments of single orientations and relative/ordinal judgments between two sequentially presented orientations. We found that lower-level, absolute judgments failed to account for higher-level, relative/ordinal judgments. However, when ordinal judgment was used to retrospectively decode memory representations of absolute orientations, striking aspects of absolute judgments, including the correlation and forward/backward afteref-fects between two reported orientations in a trial, were explained. We propose that the brain prioritizes decoding of higher-level features because they are more behaviorally relevant, and more invariant and categorical, and thus easier to specify and maintain in noisy working memory, and that more reliable higher-level decoding constrains less reliable lower-level decoding. Bayesian prior | interreport correlation | bidirectional tilt aftereffect | efficient coding | adaptation theory},
author = {Ding, Stephanie and Cueva, Christopher J and Tsodyks, Misha and Qian, Ning},
doi = {10.1073/pnas.1706906114},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Ding et al/2017/Visual perception as retrospective Bayesian decoding from high- to low-level features.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
month = {oct},
number = {43},
pages = {E9115--E9124},
title = {{Visual perception as retrospective Bayesian decoding from high- to low-level features}},
url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1706906114},
volume = {114},
year = {2017}
}
@inproceedings{MacGlashan2010a,
abstract = {We present skill bootstrapping, a proposed new research direction for agent learning and planning that allows an agent to start with low-level primitive actions, and develop skills that can be used for higher-level planning. Skills are developed over the course of solving many different problems in a domain, using reinforcement learning techniques to complement the benefits and disadvantages of heuristic-search planning. We describe the overall architecture of the proposed approach, discuss how it relates to other work, and give motivating examples for why this approach would be successful.},
author = {MacGlashan, James and DesJArdins, Marie},
booktitle = {24th AAAI Conference on Artificial Intelligence},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/MacGlashan, DesJArdins/2010/Hierarchical Skill Learning for High-Level Planning.pdf:pdf;:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/MacGlashan, DesJArdins/2010/Hierarchical Skill Learning for High-Level Planning(2).pdf:pdf},
isbn = {9781577354666},
keywords = {abstraction,approximation,planning,reinforcement learning},
pages = {1988--1989},
title = {{Hierarchical Skill Learning for High-Level Planning}},
volume = {3},
year = {2010}
}
@inproceedings{Shu2017a,
abstract = {Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task corresponds to a human language description. Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices. In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills. We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.},
archivePrefix = {arXiv},
arxivId = {1712.07294},
author = {Shu, Tianmin and Xiong, Caiming and Socher, Richard},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
doi = {10.1051/0004-6361/201527329},
eprint = {1712.07294},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Shu, Xiong, Socher/2017/Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pmid = {23459267},
title = {{Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning}},
url = {http://arxiv.org/abs/1712.07294},
year = {2017}
}
@article{Huynh2006,
abstract = {Abstract Trust and reputation are central to effective interactions in open multi-agent systems (MAS) in which agents, that are owned by a variety of stakeholders, continuously enter and leave the system. This openness means existing trust and reputation models cannot readily be used since their performance suffers when there are various (unforseen) changes in the environment. To this end, this paper presents FIRE, a trust and reputation model that integrates a number of information sources to produce a comprehensive assessment of an agents likely performance in open systems. Specifically, FIRE incorporates interaction trust, role-based trust, witness reputation, and certified reputation to provide trust metrics in most circumstances. FIRE is empirically evaluated and is shown to help agents gain better utility (by effectively selecting appropriate interaction partners) than our benchmarks in a variety of agent populations. It is also shown that FIRE is able to effectively respond to changes that occur in an agents environment.},
author = {Huynh, Trung Dong and Jennings, Nicholas R. and Shadbolt, Nigel R.},
doi = {10.1007/s10458-005-6825-4},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Huynh, Jennings, Shadbolt/2006/An integrated trust and reputation model for open multi-agent systems.pdf:pdf},
isbn = {1387-2532},
issn = {1387-2532},
journal = {Autonomous Agents and Multi-Agent Systems},
keywords = {Multi-agent systems,Reputation,Trust},
month = {sep},
number = {2},
pages = {119--154},
pmid = {8581365399728019542},
title = {{An integrated trust and reputation model for open multi-agent systems}},
url = {http://link.springer.com/10.1007/s10458-005-6825-4},
volume = {13},
year = {2006}
}
@article{Cao2012a,
abstract = {We describe an approach to incorporating Bayesian priors in the MAXQ framework for hierarchical reinforcement learning (HRL). We define priors on the primitive environment model and on task pseudo-rewards. Since models for composite tasks can be complex, we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy. We show empirically that (i) our approach results in improved convergence over non-Bayesian baselines, (ii) using both task hierarchies and Bayesian priors is better than either alone, (iii) taking advantage of the task hierarchy reduces the computational cost of Bayesian reinforcement learning and (iv) in this framework, task pseudo-rewards can be learned instead of being manually specified, leading to hierarchically optimal rather than recursively optimal policies.},
author = {Cao, F and Ray, Soumya},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cao, Ray/2012/Bayesian hierarchical reinforcement learning.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {73--81},
title = {{Bayesian hierarchical reinforcement learning}},
url = {http://papers.nips.cc/paper/4752-bay},
year = {2012}
}
@inproceedings{Davoodabadi2011a,
abstract = {In this paper the problem of automatically discovering subtasks and hierarchies in reinforcement learning is considered. We present a novel method that allows an agent to autonomously discover subgoals and create a hierarchy from actions. Our method identifies subgoals by partitioning local state transition graphs. Options constructed for reaching these subgoals are added to action choices and used for accelerating the Q-Learning algorithm. Experimental results show significant performance improvements, especially in the initial learning phase.},
author = {Davoodabadi, M and Beigy, H},
booktitle = {Proceedings of the 5th Indian International Conference on Artificial Intelligence, IICAI 2011},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Davoodabadi, Beigy/2011/A new method for discovering subgoals and constructing options in reinforcement learning.pdf:pdf},
isbn = {9780972741286},
keywords = {Artificial intelligence,Autonomously discovering subgoals,Community detection,Hierarchical reinforcement learning,Learning algorithms,Learning phase,Local state,Option,Performance improvements,Q-learning algorithms,Reinforcement learning,Subgoals,Subtasks},
pages = {441--450},
title = {{A new method for discovering subgoals and constructing options in reinforcement learning}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84872195092{\&}partnerID=40{\&}md5=9bf17b3f8f09d77ec8cd05285124028d},
year = {2011}
}
@article{Garcez2015,
abstract = {The goal of neural-symbolic computation is to integrate ro- bust connectionist learning and sound symbolic reasoning. With the recent advances in connectionist learning, in par- ticular deep neural networks, forms of representation learn- ing have emerged. However, such representations have not become useful for reasoning. Results from neural-symbolic computation have shown to offer powerful alternatives for knowledge representation, learning and reasoning in neural computation. This paper recalls the main contributions and discusses key challenges for neural-symbolic integration which have been identified at a recent Dagstuhl seminar.},
author = {Garcez, Avila and Besold, Tarek R and Raedt, Luc De and F{\"{o}}ldiak, Peter and Hitzler, Pascal and Icard, Thomas and K{\"{u}}hnberger, Kai-uwe and Lamb, Luis C and Miikkulainen, Risto and Silver, Daniel L},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Garcez et al/2015/Neural-Symbolic Learning and Reasoning Contributions and Challenges.pdf:pdf},
journal = {Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches: Papers from the 2015 AAAI Spring Symposium},
pages = {18--21},
title = {{Neural-Symbolic Learning and Reasoning : Contributions and Challenges}},
year = {2015}
}
@article{DeWolf2016,
abstract = {We present a spiking neuron model of the motor cortices and cerebellum of the motor control system. The model consists of anatomically organized spiking neurons encompassing premotor, primary motor, and cerebellar cortices. The model proposes novel neural computations within these areas to control a nonlinear three-link arm model that can adapt to unknown changes in arm dynamics and kinematic structure. We demonstrate the mathematical stability of both forms of adaptation, suggesting that this is a robust approach for common biological problems of changing body size (e.g. during growth), and unexpected dynamic perturbations (e.g. when moving through different media, such as water or mud). To demonstrate the plausibility of the proposed neural mechanisms, we show that the model accounts for data across 19 studies of the motor control system. These data include a mix of behavioural and neural spiking activity, across subjects performing adaptive and static tasks. Given this proposed characterization of the biological processes involved in motor control of the arm, we provide several experimentally testable predictions that distinguish our model from previous work.},
author = {DeWolf, Travis and Stewart, Terrence C. and Slotine, Jean-Jacques and Eliasmith, Chris},
doi = {10.1098/rspb.2016.2134},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/DeWolf et al/2016/A spiking neural model of adaptive arm control.pdf:pdf},
issn = {0962-8452},
journal = {Proceedings of the Royal Society B: Biological Sciences},
keywords = {computational biology,neuroscience},
month = {nov},
number = {1843},
pages = {20162134},
pmid = {27903878},
title = {{A spiking neural model of adaptive arm control}},
url = {http://rspb.royalsocietypublishing.org/lookup/doi/10.1098/rspb.2016.2134},
volume = {283},
year = {2016}
}
@article{Precup1998,
abstract = {Planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence. In this paper we summarize an ap-proach to this problem based on the mathematical framework of Markov decision processes and reinforcement learning. Current model-based re-inforcement learning is based on one-step models that cannot represent common-sense higher-level actions, such as going to lunch, grasping an object, or flying to Denver. This paper generalizes prior work on tem-porally abstract models [Sutton, 1995] and extends it from the prediction setting to include actions, control, and planning. We introduce a more general form of temporally abstract model, the multi-time model, and es-tablish its suitability for planning and learning by virtue of its relationship to the Bellman equations. This paper summarizes the theoretical frame-work of multi-time models and illustrates their potential advantages in a grid world planning task. The need for hierarchical and abstract planning is a fundamental problem in AI (see, e.g., Sacerdoti, 1977; Laird et aI., 1986; Korf, 1985; Kaelbling, 1993; Dayan {\&} Hinton, 1993). Model-based reinforcement learning offers a possible solution to the problem of integrating planning with real-time learning and decision-making (Peng {\&} Williams, 1993, Moore {\&} Atkeson, 1993; Sutton and Barto, 1998). However, current model-based reinforcement learning is based on one-step models that cannot represent common-sense, higher-level actions. Modeling such actions requires the ability to handle different, interrelated levels of temporal abstraction.},
author = {Precup, Doina and Sutton, Rs and Singh, S},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Precup, Sutton, Singh/1998/Multi-time models for temporally abstract planning.pdf:pdf},
isbn = {0-262-10076-2},
issn = {1049-5258},
journal = {Advances in Neural Information Processing Systems},
number = {1995},
pages = {1050--1056},
title = {{Multi-time models for temporally abstract planning}},
url = {http://www.researchgate.net/publication/221619262{\_}Multi-time{\_}Models{\_}for{\_}Temporally{\_}Abstract{\_}Planning/file/d912f506316e202652.pdf},
volume = {10},
year = {1998}
}
@article{2017a,
abstract = {Системы комического наблюдения являются важным источником информации, необ- ходимой для оперативного решения широкого круга задач гражданского и военного назначе- ния. Эффективность функционирования таких систем зависит от многих факторов. Наряду с множеством технических, информационных, технологических, инфраструктурных, эконо- мических и других факторов, их эффективность в значительной степени зависит от качест- ва планирования миссий космических аппаратов и управления в реальном времени процессом исполнения миссии. В работе сформулирована проблема построения самоорганизующейся системы группового управления поведением кластера малых спутников, реализующего авто- номное исполнение заявок на сервис по добыванию информации о наземных объектах средст- вами космического наблюдения. В ней предлагается новая концепция группового управления в системе космического наблюдения. В основу этой концепции положен принцип самоорганиза- ции группового поведения кластера спутников. Такая система управления оказывается в со- стоянии реализовать автономное планирование и оперативное управления космической груп- пировкой, в которой все базовые функции процесса управления реализуются ее бортовыми средствами. Теоретический фундамент этой концепции строится на моделях коллективной робототехники, которые в настоящее время активно развиваются в области многоагент- ных систем. В работе приведен краткий обзор современного состояния исследований в об- ласти систем управления кластерами малых спутников, дана достаточно общая постановка задачи, в которой кластер малых спутников рассматривается как полностью автономная система, предназначенная для выполнения заказов на сбор и доставку космической информа- ции о наземных объектах. При этом полагается, что система управления самостоятельно в реальном времени распределяет задачи наблюдения на множестве спутников группировки, планирует и составляет расписание выполнения наблюдений в соответствии с пространст- венно-временными требованиями заказчика, выполняет оперативное управление распреде- ленным исполнением построенного расписания и выполняет коррекцию распределения задач и расписания их выполнения при возникновении нештатных ситуаций. В работе дано деталь- ное описание разработанной концепции самоорганизующейся системы группового управления кластером малых спутников, и архитектуры ее программной реализации.},
author = {Городецкий, В. И. and Карасев, О. В.},
doi = {10.18522/2311-3103-2017-1-234247},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Городецкий, Карасев/2017/Самоорганизация группового поведения кластера малых спутников распределенной системы наблюдения.pdf:pdf},
issn = {23113103},
journal = {Известия ЮФУ. Технические науки},
keywords = {Малый спутник,автономная миссия,блюдения,динамическая коммуникационная сеть.,задача наблюдения,коллективное поведение,парное взаимо- действие спутников,распределенная система на-,ресурсы спутника,самоорганизация},
language = {russian},
month = {jan},
number = {2},
pages = {234--247},
title = {{Самоорганизация группового поведения кластера малых спутников распределенной системы наблюдения}},
url = {http://izv-tn.tti.sfedu.ru/wp-content/uploads/2017/1/19.pdf},
volume = {187},
year = {2017}
}
@article{Botvinick2009a,
abstract = {Research on human and animal behavior has long emphasized its hierarchical structure-the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior.},
author = {Botvinick, Matthew M. and Niv, Yael and Barto, Andrew C.},
doi = {10.1016/j.cognition.2008.08.011},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Botvinick, Niv, Barto/2009/Hierarchically organized behavior and its neural foundations a reinforcement learning perspective.pdf:pdf},
issn = {1873-7838},
journal = {Cognition},
keywords = {Animals,Humans,Models,Nerve Net,Nerve Net: physiology,Prefrontal Cortex,Prefrontal Cortex: physiology,Problem Solving,Problem Solving: physiology,Psychological,Reinforcement (Psychology)},
number = {3},
pages = {262--80},
pmid = {18926527},
title = {{Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2783353{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {113},
year = {2009}
}
@article{Wang2012b,
abstract = {Reinforcement learning has been an important category of machine learning approaches exhibiting self-learning and online learning characteristics. Using reinforcement learning, an agent can learn its behaviors through trial-and-error interactions with a dynamic environment and finally come up with an optimal strategy. Reinforcement learning suffers the curse of dimensionality, though there has been significant progress to overcome this issue in recent years. MAXQ is one of the most common approaches for reinforcement learning. To function properly, MAXQ requires a decomposition of the agent's task into a task hierarchy. Previously, the decomposition can only be done manually. In this paper, we propose a mechanism for automatic subtask discovery. The mechanism applies clustering to automatically construct task hierarchy required by MAXQ, such that MAXQ can be fully automated. We present the design of our mechanism, and demonstrate its effectiveness through theoretical analysis and an extensive experimental evaluation. {\textcopyright} 2012 IEEE.},
author = {Wang, Hongbing and Li, Wenya and Zhou, Xuan},
doi = {10.1109/ICTAI.2012.165},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Wang, Li, Zhou/2012/Automatic discovery and transfer of MAXQ hierarchies in a complex system.pdf:pdf},
isbn = {9780769549156},
issn = {10823409},
journal = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
keywords = {Clustering,MAXQ,Reinforcement Learning,System of Systems},
pages = {1157--1162},
title = {{Automatic discovery and transfer of MAXQ hierarchies in a complex system}},
volume = {1},
year = {2012}
}
@book{DAvilaGarcez2009,
author = {d'Avila Garcez, Artur S. and Lamb, Luis C. and Gabbay, Dov M.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/d'Avila Garcez, Lamb, Gabbay/2009/Neural-Symbolic Cognitive Reasoning.pdf:pdf},
isbn = {9783540732457},
publisher = {Springer},
title = {{Neural-Symbolic Cognitive Reasoning}},
year = {2009}
}
@article{Cox2016,
abstract = {Many algorithms have been presented in artificial intelligence for problem solving and planning. Given a goal, these algorithms search for solutions that achieve a goal state by actions or interactions with an environment. However a major assumption is that goals are given, usually by a user directly as input or as part of the problem definition. Furthermore, once given, the goals do not change. Here we formalize the notion that goal specification and goal change are themselves major parts of the problem-solving process. We apply this model to learning in a goal reasoning context. 1.},
author = {Cox, Michael T.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cox/2016/A Model of Planning, Action and Interpretation with Goal Reasoning.pdf:pdf},
journal = {Advances in Cognitive Systems},
number = {4},
pages = {1--16},
title = {{A Model of Planning, Action and Interpretation with Goal Reasoning}},
url = {https://smartech.gatech.edu/bitstream/handle/1853/53646/Technical{\%}5CnReport{\%}5CnGT-IRIM-CR-2015-001.pdf{\%}7B{\#}{\%}7Dpage=40{\%}5Cnhttps://smartech.gatech.edu/bitstream/handle/1853/53646/Technical Report GT-IRIM-CR-2015-001.pdf{\#}page=40},
year = {2016}
}
@inproceedings{Jonsson2001a,
abstract = {Learning a complex task can be significantly facilitated by defining a hierarchy of subtasks. An agent can learn to choose between various temporally abstract actions, each solving an assigned subtask, to accomplish the overall task. In this paper, we study hierarchical learning using the framework of options. We argue that to take full advantage of hierarchical structure, one should perform option-specific state abstraction, and that if this is to scale to larger tasks, state abstraction ...},
author = {Jonsson, Anders and Barto, Andrew G.},
booktitle = {Proceedings of NIPS 2001},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Jonsson, Barto/2001/Automated State Abstraction for Options using the U-Tree Algorithm.pdf:pdf},
isbn = {0262122413},
issn = {1049-5258},
pages = {1054--1060},
title = {{Automated State Abstraction for Options using the U-Tree Algorithm}},
year = {2001}
}
@article{Oakley2017,
author = {Oakley, David A. and Halligan, Peter W.},
doi = {10.3389/fpsyg.2017.01924},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Oakley, Halligan/2017/Chasing the Rainbow The Non-conscious Nature of Being.pdf:pdf},
isbn = {1664-1078},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {agency,consciousness,evolu,evolution,social,suggestion,volition},
month = {nov},
pmid = {29184516},
title = {{Chasing the Rainbow: The Non-conscious Nature of Being}},
url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2017.01924/full},
volume = {8},
year = {2017}
}
@article{Kaplan2004,
abstract = {This chapter presents a generic internal reward system that drives an agent to increase the complexity of its behavior. This reward system does not reinforce a predefined task. Its purpose is to drive the agent to progress in learning given its embodiment and the environment in which it is placed. The dynamics created by such a system are studied first in a simple environment and then in the context of active vision.},
author = {Kaplan, Frederic and Oudeyer, Pierre-Yves},
doi = {10.1007/b99075},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kaplan, Oudeyer/2004/Maximizing learning progress An internal reward system for development.pdf:pdf},
isbn = {3-540-22484-X},
issn = {03029743},
journal = {Embodied artificial intelligence},
keywords = {Active vision,Artificial intelligence,Reward},
pages = {259--270},
title = {{Maximizing learning progress: An internal reward system for development}},
volume = {3139},
year = {2004}
}
@article{Kurup2011,
abstract = {Generating future states of the world is an essential component of high level cognitive tasks such as planning. We explore the notion that such future state generation is more widespread and forms an integral part of cognition. We call these generated states expectations, and propose that cognitive systems constantly generate expectations, match them to observed behavior and react when a difference exists between the two. We describe an ACT R model that performs expectation driven cognition on two tasks pedestrian tracking and behavior classification. The model generates expectations of pedestrian movements to track them. The model also uses differences in expectations to identify distinctive features that differentiate these tracks. During learning, the model learns the association between these features and the various behaviors. During testing, it classifies pedestrian tracks by recalling the behavior associated with the features of each track. We tested the model on both single and multiple behavior datasets and compared the results against a k NN classifier. The k NN classifier outperformed the model in correct classifications, but the model had fewer incorrect classifications in the multiple behavior case, and both systems had about equal incorrect classifications in the single behavior case},
author = {Kurup, Unmesh and Lebiere, Christian and Stentz, Anthony and Hebert, Martial},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kurup et al/2011/Using Expectations to Drive Cognitive Behavior.pdf:pdf},
isbn = {9781577355687},
journal = {AAAI Conference on Artificial Intelligence},
keywords = {Special Track on Cognitive Systems},
pages = {221--227},
title = {{Using Expectations to Drive Cognitive Behavior}},
year = {2011}
}
@article{Spratling2017,
author = {Spratling, M. W.},
doi = {10.1007/s12559-016-9445-1},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Spratling/2017/A Hierarchical Predictive Coding Model of Object Recognition in Natural Images.pdf:pdf},
issn = {18669964},
journal = {Cognitive Computation},
keywords = {Deep neural networks,Implicit shape model,Neural networks,Object recognition,Predictive coding,Sparse coding},
number = {2},
pages = {151--167},
publisher = {Cognitive Computation},
title = {{A Hierarchical Predictive Coding Model of Object Recognition in Natural Images}},
volume = {9},
year = {2017}
}
@article{Frank2012,
abstract = {Growing evidence suggests that the prefrontal cortex (PFC) is organized hierarchically, with more anterior regions having increasingly abstract representations. How does this organization support hierarchical cognitive control and the rapid discovery of abstract action rules? We present computational models at different levels of description. A neural circuit model simulates interacting corticostriatal circuits organized hierarchically. In each circuit, the basal ganglia gate frontal actions, with some striatal units gating the inputs to PFC and others gating the outputs to influence response selection. Learning at all of these levels is accomplished via dopaminergic reward prediction error signals in each corticostriatal circuit. This functionality allows the system to exhibit conditional if-then hypothesis testing and to learn rapidly in environments with hierarchical structure. We also develop a hybrid Bayesian-reinforcement learning mixture of experts (MoE) model, which can estimate the most likely hypothesis state of individual participants based on their observed sequence of choices and rewards. This model yields accurate probabilistic estimates about which hypotheses are attended by manipulating attentional states in the generative neural model and recovering them with the MoE model. This 2-pronged modeling approach leads to multiple quantitative predictions that are tested with functional magnetic resonance imaging in the companion paper.},
author = {Frank, Michael J. and Badre, David},
doi = {10.1093/cercor/bhr114},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Frank, Badre/2012/Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1 Computational analysis.pdf:pdf},
issn = {1460-2199},
journal = {Cerebral cortex (New York, N.Y. : 1991)},
keywords = {Computer Simulation,Corpus Striatum,Corpus Striatum: cytology,Corpus Striatum: physiology,Humans,Learning,Learning: physiology,Models,Neural Pathways,Neural Pathways: cytology,Neural Pathways: physiology,Neurological,Neurons,Neurons: physiology,Prefrontal Cortex,Prefrontal Cortex: cytology,Prefrontal Cortex: physiology,Reinforcement (Psychology)},
number = {3},
pages = {509--26},
pmid = {21693490},
title = {{Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: Computational analysis}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3278315{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {22},
year = {2012}
}
@article{Ghallab2014,
abstract = {Planning is motivated by acting. Most of the existing work on automated planning underestimates the reasoning and deliberation needed for acting; it is instead biased towards path-finding methods in a compactly specified state-transition system. Researchers in this AI field have developed many planners, but very few actors. We believe this is one of the main causes of the relatively low deployment of automated planning applications. In this paper, we advocate a change in focus to actors as the primary topic of investigation. Actors are not mere plan executors: they may use planning and other deliberation tools, before and during acting. This change in focus entails two interconnected principles: a hierarchical structure to integrate the actor's deliberation functions, and continual online planning and reasoning throughout the acting process. In the paper, we discuss open problems and research directions toward that objective in knowledge representations, model acquisition and verification, synthesis and refinement, monitoring, goal reasoning, and integration. {\textcopyright} 2013 Elsevier B.V.},
author = {Ghallab, Malik and Nau, Dana and Traverso, Paolo},
doi = {10.1016/j.artint.2013.11.002},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Ghallab, Nau, Traverso/2014/The actor's view of automated planning and acting A position paper.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Automated planning and acting},
number = {1},
pages = {1--17},
publisher = {Elsevier B.V.},
title = {{The actor's view of automated planning and acting: A position paper}},
url = {http://dx.doi.org/10.1016/j.artint.2013.11.002},
volume = {208},
year = {2014}
}
@incollection{Vezhnevets2017,
abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.},
archivePrefix = {arXiv},
arxivId = {1703.01161},
author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
booktitle = {Proceedings of the 34th International Conference on Machine Learning},
editor = {Precup, Doina and Teh, Yee Whye},
eprint = {1703.01161},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Vezhnevets et al/2017/FeUdal Networks for Hierarchical Reinforcement Learning.pdf:pdf},
issn = {1938-7228},
pages = {3540--3549},
publisher = {PMLR},
title = {{FeUdal Networks for Hierarchical Reinforcement Learning}},
url = {http://arxiv.org/abs/1703.01161},
year = {2017}
}
@inproceedings{Rasmussen1998,
author = {Rasmussen, Daniel and Eliasmith, Chris},
booktitle = {Proceedings of the 36th Annual Conference of the Cognitive Science Society},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Rasmussen, Eliasmith/2014/A neural model of hierarchical reinforcement learning.pdf:pdf},
keywords = {brain could achieve its,hierarchical,hrl into a theory,neural engineering framework,neural model,of neural function,providing a hypothe-,reinforcement learning,sis for how the,strengths in scaling},
number = {1},
pages = {1252--1257},
title = {{A neural model of hierarchical reinforcement learning}},
year = {2014}
}
@article{Vogt2003,
author = {Vogt, Paul},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Vogt/2003/Anchoring of semiotic symbols.pdf:pdf},
journal = {Robotics and Autonomous Systems},
number = {2-3},
pages = {109--120},
title = {{Anchoring of semiotic symbols}},
volume = {43},
year = {2003}
}
@inproceedings{Harutyunyan2017,
abstract = {A temporally abstract action, or an option, is specified by a policy and a termination condition: the policy guides option behavior, and the termination condition roughly determines its length. Generally, learning with longer options (like learning with multi-step returns) is known to be more efficient. However, if the option set for the task is not ideal, and cannot express the primitive optimal policy exactly, shorter options offer more flexibility and can yield a better solution. Thus, the termination condition puts learning efficiency at odds with solution quality. We propose to resolve this dilemma by decoupling the behavior and target terminations, just like it is done with policies in off-policy learning. To this end, we give a new algorithm, Q($\backslash$beta), that learns the solution with respect to any termination condition, regardless of how the options actually terminate. We derive Q($\backslash$beta) by casting learning with options into a common framework with well-studied multi-step off-policy learning. We validate our algorithm empirically, and show that it holds up to its motivating claims.},
archivePrefix = {arXiv},
arxivId = {1711.03817},
author = {Harutyunyan, Anna and Vrancx, Peter and Bacon, Pierre-Luc and Precup, Doina and Nowe, Ann},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
eprint = {1711.03817},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Harutyunyan et al/2017/Learning with Options that Terminate Off-Policy.pdf:pdf},
title = {{Learning with Options that Terminate Off-Policy}},
url = {http://arxiv.org/abs/1711.03817},
year = {2017}
}
@incollection{Lorini2010,
author = {Lorini, Emiliano and Verdicchio, Mario},
booktitle = {Coordination, Organizations, Institutions and Norms in Agent Systems V},
doi = {10.1007/978-3-642-14962-7_10},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Lorini, Verdicchio/2010/Towards a Logical Model of Social Agreement for Agent Societies.pdf:pdf},
isbn = {3642149618},
issn = {03029743},
pages = {147--162},
title = {{Towards a Logical Model of Social Agreement for Agent Societies}},
url = {http://link.springer.com/10.1007/978-3-642-14962-7{\_}10},
year = {2010}
}
@article{Botvinick2008,
abstract = {The recognition of hierarchical structure in human behavior was one of the founding insights of the cognitive revolution. Despite decades of research, however, the computational mechanisms underlying hierarchically organized behavior are still not fully understood. Recent findings from behavioral and neuroscientific research have fueled a resurgence of interest in the problem, inspiring a new generation of computational models. In addition to developing some classic proposals, these models also break fresh ground, teasing apart different forms of hierarchical structure, placing a new focus on the issue of learning and addressing recent findings concerning the representation of behavioral hierarchies within the prefrontal cortex. In addition to offering explanations for some key aspects of behavior and functional neuroanatomy, the latest models also pose new questions for empirical research. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
author = {Botvinick, Matthew M.},
doi = {10.1016/j.tics.2008.02.009},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Botvinick/2008/Hierarchical models of behavior and prefrontal function.pdf:pdf},
isbn = {1364-6613 (Print)$\backslash$n1364-6613 (Linking)},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {5},
pages = {201--208},
pmid = {18420448},
title = {{Hierarchical models of behavior and prefrontal function}},
volume = {12},
year = {2008}
}
@article{Gorodetsky2007,
abstract = {Данная работа представляет реализованную авторами P2P агентскую плат- форму, экземпляры которой, установленные в узлах сети поверх стандартного P2P серви- са, образуют распределенную базу знаний, предназначенную для организации семантическо- го P2P взаимодействия агентов. Прикладные агенты, в свою очередь, устанавливаются в узлах сети поверх экземпляров агентской платформы. В основу разработки положены функциональная архитектура, разработанная рабочей группой FIPA в качестве предложе- ния для последующей программной реализации и стандартизации. Разработанная про- граммная реализация платформы поддерживается также механизмом парных взаимодейст- вий агентов на основе сообщений разработанных форматов, а также парных коммуникаций узлов сети. Такой механизмом парных взаимодействий агентов также разработан автора- ми. Роль, функции и существо процессов функционирования этой платформы поясняется на примерах двух приложений, которые сами по себе являются достаточно важными с практи- ческой точки зрения. Эти же приложения использованы для верификации основных решений, предложенных в работе.},
author = {Городецкий, В. И. and Карсаев, О. В. and Самойлов, В. В. and Серебряков, С. В.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Городецкий et al/2007/Открытые сети агентов.pdf:pdf},
journal = {Труды СПИИРАН},
language = {russian},
number = {4},
pages = {11--35},
title = {{Открытые сети агентов}},
year = {2007}
}
@inproceedings{Serrano2012,
abstract = {We propose a novel method for assessing the reputation of agents in multiagent systems that is capable of exploiting the structure and semantics of rich agent interaction protocols and agent communication languages. Our method is based on using so-called conversation models, i.e. succinct, qualitative models of agents' behaviours derived from the application of data mining techniques on protocol execution data in a way that takes advantage of the semantics of interagent communication available in many multiagent systems. Contrary to existing systems, which only allow for querying agents regarding their assessment of others' reputation in an outcome-based way (often limited to distinguishing between "successful" and "unsuccessful" interactions), our method allows for contextualised queries regarding the structure of past interactions, the values of content variables, and the behaviour of agents across different protocols. Moreover, this is achieved while preserving maximum privacy for the reputation querying agent and the witnesses queried, and without requiring a common definition of reputation, trust or reliability among the agents exchanging reputation information. A case study shows that, even with relatively simple reputation measures, our qualitative method outperforms quantitative approaches, proving that we can meaningfully exploit the additional information afforded by rich interaction protocols and agent communication semantics. Copyright {\textcopyright} 2012, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.},
author = {Serrano, Emilio and Rovatsos, Michael and Botia, Juan},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Serrano, Rovatsos, Botia/2012/A qualitative reputation system for multiagent systems with protocol-based communication.pdf:pdf},
keywords = {agent communication,data mining,trust and reputation},
pages = {307--314},
title = {{A qualitative reputation system for multiagent systems with protocol-based communication}},
volume = {1},
year = {2012}
}
@incollection{Castro2012a,
abstract = {Abstract. Temporally extended actions are usually effective in speeding up reinforcement learning. In this paper we present a mechanism for automatically constructing such actions, expressed as options [Sutton et al., 1999], in a finite Markov Decision Process (MDP). To do this, we compute a bisimulation metric [Ferns et al., 2004] between the states in a small MDP and the states in a large MDP, which we want to solve. The shape of this metric is then used to completely define a set of options for the large MDP. We demonstrate empirically that our approach is able to improve the speed of reinforcement learning, and is generally not sensitive to parameter tuning.},
author = {Castro, Pablo Samuel and Precup, Doina},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-29946-9_16},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Castro, Precup/2012/Automatic Construction of Temporally Extended Actions for MDPs Using Bisimulation Metrics.pdf:pdf},
isbn = {9783642299452},
issn = {03029743},
pages = {140--152},
title = {{Automatic Construction of Temporally Extended Actions for MDPs Using Bisimulation Metrics}},
url = {http://link.springer.com/10.1007/978-3-642-29946-9{\_}16},
volume = {7188 LNAI},
year = {2012}
}
@inproceedings{Stulp2011,
abstract = {Temporal abstraction and task decomposition drastically reduce the search space for planning and control, and are fundamental to making complex tasks amenable to learning. In the context of reinforcement learning, temporal abstractions are studied within the paradigm of hierarchical reinforcement learning.},
author = {Stulp, Freek and Schaal, Stefan},
booktitle = {2011 11th IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/Humanoids.2011.6100841},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Stulp, Schaal/2011/Hierarchical reinforcement learning with movement primitives.pdf:pdf},
isbn = {978-1-61284-868-6},
issn = {21640572},
month = {oct},
pages = {231--238},
publisher = {IEEE},
title = {{Hierarchical reinforcement learning with movement primitives}},
url = {http://ieeexplore.ieee.org/document/6100841/},
year = {2011}
}
@inproceedings{Paxton2017a,
abstract = {We consider task and motion planning in complex dynamic environments for problems expressed in terms of a set of Linear Temporal Logic (LTL) constraints, and a reward function. We propose a methodology based on reinforcement learning that employs deep neural networks to learn low-level control policies as well as task-level option policies. A major challenge in this setting, both for neural network approaches and classical planning, is the need to explore future worlds of a complex and interactive environment. To this end, we integrate Monte Carlo Tree Search with hierarchical neural net control policies trained on expressive LTL specifications. This paper investigates the ability of neural networks to learn both LTL constraints and control policies in order to generate task plans in complex environments. We demonstrate our approach in a simulated autonomous driving setting, where a vehicle must drive down a road in traffic, avoid collisions, and navigate an intersection, all while obeying given rules of the road.},
archivePrefix = {arXiv},
arxivId = {1703.07887},
author = {Paxton, Chris and Raman, Vasumathi and Hager, Gregory D. and Kobilarov, Marin},
booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2017.8206505},
eprint = {1703.07887},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Paxton et al/2017/Combining Neural Networks and Tree Search for Task and Motion Planning in Challenging Environments(2).pdf:pdf},
isbn = {978-0-12-084802-7},
keywords = {Deep Learning in Robotics and Automa,Task Planning},
pages = {6059--6066},
publisher = {IEEE},
title = {{Combining Neural Networks and Tree Search for Task and Motion Planning in Challenging Environments}},
url = {http://ieeexplore.ieee.org/document/8206505/{\%}0Ahttp://arxiv.org/abs/1703.07887},
year = {2017}
}
@article{George2017,
author = {George, D. and Lehrach, W. and Kansky, K. and L{\'{a}}zaro-Gredilla, M. and Laan, C. and Marthi, B. and Lou, X. and Meng, Z. and Liu, Y. and Wang, H. and Lavin, A. and Phoenix, D. S.},
doi = {10.1126/science.aag2612},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/George et al/2017/A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs.pdf:pdf;:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/George et al/2017/A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs(2).pdf:pdf},
issn = {0036-8075},
journal = {Science},
month = {oct},
number = {October},
pages = {eaag2612},
pmid = {29074582},
title = {{A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs}},
url = {http://www.sciencemag.org/lookup/doi/10.1126/science.aag2612},
volume = {10},
year = {2017}
}
@article{Dayan1993,
abstract = {One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time. This paper shows how to create a Q-learning managerial hierarchy in which high level managers learn how to set tasks to their sub-managers who, in turn, learn how to satisfy them. Sub-managers need not initially understand their managers' commands. They simply learn to maximise their reinforcement in the context of the current command. We illustrate the system using a simple maze task.. As the system learns how to get around, satisfying commands at the multiple levels, it explores more efficiently than standard, flat, Q-learning and builds a more comprehensive map.},
author = {Dayan, Peter and Hinton, Geoffrey},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Dayan, Hinton/1993/Feudal Reinforcement Learning.pdf:pdf},
isbn = {1-55860-274-7},
issn = {1049-5258},
journal = {Advances in neural information processing systems},
keywords = {Reinforcement Learning},
pages = {271--278},
title = {{Feudal Reinforcement Learning}},
url = {http://www.cs.utoronto.ca/{~}hinton/absps/dh93.pdf},
year = {1993}
}
@article{Florensa2017,
abstract = {Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.},
archivePrefix = {arXiv},
arxivId = {1704.03012},
author = {Florensa, Carlos and Duan, Yan and Abbeel, Pieter},
eprint = {1704.03012},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Florensa, Duan, Abbeel/2017/Stochastic Neural Networks for Hierarchical Reinforcement Learning.pdf:pdf},
isbn = {9781613242643},
journal = {ArXiv},
month = {apr},
title = {{Stochastic Neural Networks for Hierarchical Reinforcement Learning}},
url = {https://openreview.net/pdf?id=B1oK8aoxe http://arxiv.org/abs/1704.03012},
year = {2017}
}
@article{Dehaene2017,
author = {Dehaene, Stanislas and Lau, Hakwan and Kouider, Sid},
doi = {10.1126/science.aan8871},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Dehaene, Lau, Kouider/2017/What is consciousness, and could machines have it.pdf:pdf},
issn = {0036-8075},
journal = {Science},
month = {oct},
number = {6362},
pages = {486--492},
pmid = {29074769},
title = {{What is consciousness, and could machines have it?}},
url = {http://www.sciencemag.org/lookup/doi/10.1126/science.aan8871},
volume = {358},
year = {2017}
}
@article{Khardon1999,
abstract = {We introduce a new framework for the study of reasoning. The Learning (in order) to Reason approach developed here views learning as an integral part of the inference process, and suggests that learning and reasoning should be studied together. The Learning to Reason framework combines the interfaces to the world used by known learning models with the reasoning task and a performance criterion suitable for it. In this framework, the intelligent agent is given access to its favorite learning interface, and is also given a grace period in which it can interact with this interface and construct a representation KB of the world W. The reasoning performance is measured only after this period, when the agent is presented with queries a from some query language, relevant to the world, and has to answer whether W implies a. The approach is meant to overcome the main computational difficulties in the traditional treatment of reasoning which stem from its separation from the “world”. Since the agent interacts with the world when constructing its knowledge representation it can choose a representation that is useful for the task at hand. Moreover, we can now make explicit the dependence of the reasoning performance on the environment the agent interacts with. We show how previous results from learning theory and reasoning fit into this framework and illustrate the usefulness of the Learning to Reason approach by exhibiting new results that are not possible in the traditional setting. First, we give Learning to Reason algorithms for classes of propositional languages for which there are no efficient reasoning algorithms, when represented as a traditional (formula-based) knowledge base. Second, we exhibit a Learning to Reason algorithm for a class of propositional languages that is not known to be learnable in the traditional sense.},
author = {Khardon, Roni and Roth, Dan},
doi = {10.1023/A:1007581123604},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Khardon, Roth/1999/Learning to reason with a restricted view.pdf:pdf},
isbn = {0-262-61102-3},
issn = {08856125},
journal = {Machine Learning},
keywords = {computational learning theory,partial assignments,reasoning},
number = {2},
pages = {95--116},
title = {{Learning to reason with a restricted view}},
volume = {35},
year = {1999}
}
@inproceedings{Pitis2017,
abstract = {This paper presents preliminary work on a framework for reasoning over multiple competing representations of the value function during reinforcement learning, with a focus on landmark and factor-based reasoning. This approach enables consistency-based learning, explicit justification of actions and learning by partial supervision, and has the potential to improve agent performance.},
author = {Pitis, Silviu},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Pitis/2017/Reasoning for Reinforcement Learning.pdf:pdf},
title = {{Reasoning for Reinforcement Learning}},
url = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-51.html},
year = {2017}
}
@article{Rasmussen2017a,
abstract = {We present the first model capable of performing hierarchical reinforcement learning in a general, neurally detailed imple-mentation. We show that this model is able to learn a spatial pickup and delivery task more quickly than one without hier-archical abilities. In addition, we show that this model is able to leverage its hierarchical structure to transfer learned knowl-edge between related tasks. These results point towards the advantages to be gained by using a hierarchical RL framework to understand the brain's powerful learning ability.},
author = {Rasmussen, Daniel and Voelker, Aaron and Eliasmith, Chris},
doi = {10.1371/journal.pone.0180234},
editor = {Cymbalyuk, Gennady},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Rasmussen, Voelker, Eliasmith/2017/A neural model of hierarchical reinforcement learning.pdf:pdf;:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Rasmussen, Voelker, Eliasmith/2017/A neural model of hierarchical reinforcement learning(2).pdf:pdf},
isbn = {1111111111},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {brain could achieve its,hierarchical,hrl into a theory,neural engineering framework,neural model,of neural function,providing a hypothe-,reinforcement learning,sis for how the,strengths in scaling},
month = {jul},
number = {7},
pages = {e0180234},
title = {{A neural model of hierarchical reinforcement learning}},
url = {http://dx.plos.org/10.1371/journal.pone.0180234},
volume = {12},
year = {2017}
}
@article{Hawes2011,
abstract = {The ability to achieve ones goals is a defining characteristic of intelligent behaviour. A great many existing theories, systems and research programmes address the problems associated with generating behaviour to achieve a goal; much fewer address the related problems of how and why goals should be generated in an intelligent artifact, and how a subset of all possible goals are selected as the focus of behaviour. It is research into these problems of motivation, which this article aims to stimulate. Building from the analysis of a scenario involving a futuristic household robot, we extend an existing account of motivation in intelligent systems to provide a framework for surveying relevant literature in AI and robotics. This framework guides us to look at the problems of encoding drives (how the needs of the system are represented), goal generation (how particular instances of goals are generated from the drives with reference to the current state), and goal selection (how the system determines which goal instances to act on). After surveying a variety of existing approaches in these terms, we build on the results of the survey to sketch a design for a new motive management framework which goes beyond the current state of the art. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Hawes, Nick},
doi = {10.1016/j.artint.2011.02.002},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Hawes/2011/A survey of motivation frameworks for intelligent systems.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Agent architecture,Goal-directed behaviour,Motivation,Planning},
number = {5-6},
pages = {1020--1036},
publisher = {Elsevier B.V.},
title = {{A survey of motivation frameworks for intelligent systems}},
url = {http://dx.doi.org/10.1016/j.artint.2011.02.002},
volume = {175},
year = {2011}
}
@article{Parr1998,
abstract = {We present a new approach to reinforcement learning in which the poli- cies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework inwhich knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learn- ing and “behavior-based” or “teleo-reactive” approaches to control. We present provably convergent algorithms for problem-solving and learn- ing with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states. 1},
author = {Parr, Ronald and Russell, Stuart},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Parr, Russell/1998/Reinforcement learning with hierarchies of machines.pdf:pdf},
isbn = {0-262-10076-2},
issn = {0031-8116},
journal = {Neural Information Processing Systems (NIPS)},
pages = {1043--1049},
title = {{Reinforcement learning with hierarchies of machines}},
url = {http://www.cs.berkeley.edu/{~}russell/classes/cs294/s11/readings/Parr+Russell:1998.pdf},
year = {1998}
}
@article{Singh2010a,
abstract = {There is great interest in building intrinsic motivation into artificial systems using the reinforcement learning framework. Yet, what intrinsic motivation may mean computationally, and how it may differ from extrinsic motivation, remains a murky and controversial subject. In this paper, we adopt an evolutionary perspective and define a new optimal reward framework that captures the pressure to design good primary reward functions that lead to evolutionary success across environments. The results of two computational experiments show that optimal primary reward signals may yield both emergent intrinsic and extrinsic motivation. The evolutionary perspective and the associated optimal reward framework thus lead to the conclusion that there are no hard and fast features distinguishing intrinsic and extrinsic reward computationally. Rather, the directness of the relationship between rewarding behavior and evolutionary success varies along a continuum.},
author = {Singh, Satinder and Lewis, Richard L. and Barto, Andrew G. and Sorg, Jonathan},
doi = {10.1109/TAMD.2010.2051031},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Singh et al/2010/Intrinsically Motivated Reinforcement Learning An Evolutionary Perspective(2).pdf:pdf},
isbn = {0262195348, 9780262195348},
issn = {1943-0604},
journal = {IEEE Transactions on Autonomous Mental Development},
number = {2},
pages = {70--82},
title = {{Intrinsically Motivated Reinforcement Learning: An Evolutionary Perspective}},
url = {http://ieeexplore.ieee.org/document/5471106/},
volume = {2},
year = {2010}
}
@article{Madl2017,
abstract = {Computational cognitive models of spatial memory often neglect difficulties posed by the real world, such as sensory noise, uncertainty, and high spatial complexity. On the other hand, robotics is unconcerned with understanding biological cognition. Here, we describe a computational framework for robotic architectures aiming to function in realistic environments, as well as to be cognitively plausible. We motivate and describe several mechanisms towards achieving this despite the sensory noise and spatial complexity inherent in the physical world. We tackle error accumulation during path integration by means of Bayesian localization, and loop closing with sequential gradient descent. Finally, we outline a method for structuring spatial representations using metric learning and clustering. Crucially, unlike the algorithms of traditional robotics, we show that these mechanisms can be implemented in neuronal or cognitive models. We briefly outline a concrete implementation of the proposed framework as part of the LIDA cognitive architecture, and argue that this kind of probabilistic framework is well-suited for use in cognitive robotic architectures aiming to combine spatial functionality and psychological plausibility.},
author = {Madl, Tamas and Franklin, Stan and Chen, Ke and Trappl, Robert},
doi = {10.1016/j.cogsys.2017.08.002},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Madl et al/2017/A computational cognitive framework of spatial memory in brains and robots.pdf:pdf},
issn = {13890417},
journal = {Cognitive Systems Research},
keywords = {Bayesian brain,Cognitive architecture,Computational cognitive modeling,LIDA,Spatial memory},
pages = {147--172},
publisher = {Elsevier B.V.},
title = {{A computational cognitive framework of spatial memory in brains and robots}},
url = {https://doi.org/10.1016/j.cogsys.2017.08.002},
volume = {47},
year = {2017}
}
@inproceedings{Kumar2017a,
abstract = {We present a framework combining hierarchical and multi-agent deep reinforcement learning approaches to solve coordination problems among a multitude of agents using a semi-decentralized model. The framework extends the multi-agent learning setup by introducing a meta-controller that guides the communication between agent pairs, enabling agents to focus on communicating with only one other agent at any step. This hierarchical decomposition of the task allows for efficient exploration to learn policies that identify globally optimal solutions even as the number of collaborating agents increases. We show promising initial experimental results on a simulated distributed scheduling problem.},
archivePrefix = {arXiv},
arxivId = {1712.08266},
author = {Kumar, Saurabh and Shah, Pararth and Hakkani-Tur, Dilek and Heck, Larry},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
eprint = {1712.08266},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kumar et al/2017/Federated Control with Hierarchical Multi-Agent Deep Reinforcement Learning.pdf:pdf},
title = {{Federated Control with Hierarchical Multi-Agent Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1712.08266},
year = {2017}
}
@book{2013b,
author = {Новиков, Д. А. and Чхартишвили, А. Г.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Новиков, Чхартишвили/2013/Рефлексия и управление математические модели.pdf:pdf},
isbn = {9785940522263},
language = {russian},
pages = {412},
publisher = {Физматлит},
title = {{Рефлексия и управление: математические модели}},
year = {2013}
}
@inproceedings{Roderick2017a,
abstract = {We examine the problem of learning and planning on high-dimensional domains with long horizons and sparse rewards. Recent approaches have shown great successes in many Atari 2600 domains. However, domains with long horizons and sparse rewards, such as Montezuma's Revenge and Venture, remain challenging for existing methods. Methods using abstraction (Dietterich 2000; Sutton, Precup, and Singh 1999) have shown to be useful in tackling long-horizon problems. We combine recent techniques of deep reinforcement learning with existing model-based approaches using an expert-provided state abstraction. We construct toy domains that elucidate the problem of long horizons, sparse rewards and high-dimensional inputs, and show that our algorithm significantly outperforms previous methods on these domains. Our abstraction-based approach outperforms Deep Q-Networks (Mnih et al. 2015) on Montezuma's Revenge and Venture, and exhibits backtracking behavior that is absent from previous methods.},
archivePrefix = {arXiv},
arxivId = {1710.00459},
author = {Roderick, Melrose and Grimm, Christopher and Tellex, Stefanie},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
eprint = {1710.00459},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Roderick, Grimm, Tellex/2017/Deep Abstract Q-Networks.pdf:pdf},
title = {{Deep Abstract Q-Networks}},
url = {http://arxiv.org/abs/1710.00459},
year = {2017}
}
@book{Sun1994,
abstract = {Concerned with understanding and modeling commonsense reasoning with a combination of rules and similarities under a connectionist rubric. Examines the areas of reasoning, connectionist models, inheritance, causality, rule-based systems and similarity-based reasoning. Introduces a new structure, a novel connectionist architecture and a set of fresh ideas leading to new applications.},
author = {Sun, Ron},
pages = {273},
publisher = {Wiley-Interscience},
title = {{Integrating Rules and Connectionism for Robust Commonsense Reasoning}},
year = {1994}
}
@article{2007a,
author = {Лукьянова, Л. М.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Лукьянова/2007/Целеполагание, анализ и синтез целей в сложных системах модели и методы моделирования.pdf:pdf},
journal = {Известия РАН. Теория и системы управления},
language = {russian},
number = {5},
pages = {100--113},
title = {{Целеполагание, анализ и синтез целей в сложных системах: модели и методы моделирования}},
year = {2007}
}
@article{Konidaris2012a,
abstract = {We present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to significantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that significantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-specific skills.},
author = {Konidaris, George and Scheidwasser, Ilya and Barto, Andrew G.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Konidaris, Scheidwasser, Barto/2012/Transfer in reinforcement learning via shared features.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {reinforcement learning,shaping,skills,transfer},
number = {1},
pages = {1333--1371},
title = {{Transfer in reinforcement learning via shared features}},
url = {http://dl.acm.org/citation.cfm?id=2503308.2343689{\%}5Cnhttp://dl.acm.org/ft{\_}gateway.cfm?id=2343689{\&}type=pdf},
volume = {13},
year = {2012}
}
@article{Barto2003,
abstract = {Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of {\{}semi-Markov{\}} decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.},
author = {Barto, Andrew G. and Mahadevan, Sridhar},
doi = {10.1023/A:1025696116075},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Barto, Mahadevan/2003/Recent Advances in Hierarchical Reinforcement Learning.pdf:pdf},
isbn = {0924-6703},
issn = {09246703},
journal = {Discrete Event Dynamic Systems},
pages = {341--379},
title = {{Recent Advances in Hierarchical Reinforcement Learning}},
url = {http://dx.doi.org/10.1023/A:1025696116075},
volume = {13},
year = {2003}
}
@article{Cox1999,
abstract = {A central problem in multistrategy learning systems is the selection and sequencing of machine learning algorithms for particular situations. This is typically done by the system designer who analyzes the learning task and implements the appropriate algorithm or sequence of algorithms for that task. A solution to this problem is proposed to enable an AI system with a library of machine learning algorithms to select and sequence appropriate algorithms autonomously.},
author = {Cox, Michael T. and Ashwin, Ram},
doi = {10.1016/S0004-3702(99)00047-8},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cox, Ashwin/1999/Introspective multistrategy learning On the construction of learning strategies.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
number = {1},
pages = {1--55},
title = {{Introspective multistrategy learning: On the construction of learning strategies}},
volume = {112},
year = {1999}
}
@article{Gorodetsky2009,
abstract = {В работе рассматривается технология построения прикладных систем группового управления, со- стоящих из большого числа автономных подсистем, организованных в сеть, узлы которой могут работать под управлением различных операционных систем и в различных коммуникационных средах. Технология интегрирует подходы распределенного принятия решений, многоагентных систем, ориентированной на сервис архитектуры и вычислений на основе парных взаимодействий. Технология поддерживается инструментальными средствами, ко- торые обеспечивают эффективную разработку агентов и механизмов их взаимодействия. Приводятся примеры использования технологии в ряде приложений, в частности, для автономного управления воздушным движением в районе аэропорта.},
author = {Городецкий, В. И. and Карсаев, О. В. and Самойлов, В. В. and Серебряков, С. В.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Городецкий et al/2009/Прикладные многоагентные системы группового управления.pdf:pdf},
journal = {Искусственный интеллект и принятие решений},
language = {russian},
number = {2},
pages = {3--24},
title = {{Прикладные многоагентные системы группового управления}},
year = {2009}
}
@article{Hassabis2017,
abstract = {The fields of neuroscience and artificial intelligence (AI) have a long and intertwined history. In more recent times, however, communication and collaboration between the two fields has become less commonplace. In this article, we argue that better understanding biological brains could play a vital role in building intelligent machines. We survey historical interactions between the AI and neuroscience fields and emphasize current advances in AI that have been inspired by the study of neural computation in humans and other animals. We conclude by highlighting shared themes that may be key for advancing future research in both fields. Hassabis et al. review how neuroscience has informed research in artificial intelligence. They argue that a better understanding of biological brains will play a vital role in building intelligent machines.},
archivePrefix = {arXiv},
arxivId = {1404.7282},
author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
doi = {10.1016/j.neuron.2017.06.011},
eprint = {1404.7282},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Hassabis et al/2017/Neuroscience-Inspired Artificial Intelligence.pdf:pdf},
isbn = {0896-6273},
issn = {10974199},
journal = {Neuron},
keywords = {artificial intelligence,brain,cognition,learning,neural network},
number = {2},
pages = {245--258},
pmid = {28728020},
publisher = {Elsevier Inc.},
title = {{Neuroscience-Inspired Artificial Intelligence}},
url = {http://dx.doi.org/10.1016/j.neuron.2017.06.011},
volume = {95},
year = {2017}
}
@article{Simsek2005a,
abstract = {We present a new subgoal-based method for automatically creating useful skills in reinforcement learning. Our method identifies subgoals by partitioning local state transition graphs—those that are constructed using only the most recent experiences of the agent. The local scope of our subgoal discovery method allows it to successfully identify the type of subgoals we seek—states that lie between two densely-connected regions of the state space—while producing an algorithm with low computational cost.},
author = {Şimşek, {\"{O}}zg{\"{u}}r and Wolfe, Alicia P. and Barto, Andrew G.},
doi = {10.1145/1102351.1102454},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Şimşek, Wolfe, Barto/2005/Identifying useful subgoals in reinforcement learning by local graph partitioning.pdf:pdf},
isbn = {1595931805},
journal = {Proceedings of the 22nd international conference on Machine learning  - ICML '05},
pages = {816--823},
title = {{Identifying useful subgoals in reinforcement learning by local graph partitioning}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102454},
year = {2005}
}
@inproceedings{Kulkarni2016,
abstract = {Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.},
archivePrefix = {arXiv},
arxivId = {1604.06057},
author = {Kulkarni, Tejas D. and Narasimhan, Karthik R. and Saeedi, Ardavan and Tenenbaum, Joshua B.},
booktitle = {Proceeding of NIPS 2016},
doi = {10.1023/A:1025696116075},
eprint = {1604.06057},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kulkarni et al/2016/Hierarchical Deep Reinforcement Learning Integrating Temporal Abstraction and Intrinsic Motivation.pdf:pdf},
isbn = {0924-6703},
issn = {1573-7594},
pages = {3682--3690},
title = {{Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation}},
url = {http://arxiv.org/abs/1604.06057},
year = {2016}
}
@article{Botvinick2012,
abstract = {The hierarchical structure of human and animal behavior has been of critical interest in neuroscience for many years. Yet understanding the neural processes that give rise to such structure remains an open challenge. In recent research, a new perspective on hierarchical behavior has begun to take shape, inspired by ideas from machine learning, and in particular the framework of hierarchical reinforcement learning. Hierarchical reinforcement learning builds on traditional reinforcement learning mechanisms, extending them to accommodate temporally extended behaviors or subroutines. The resulting computational paradigm has begun to influence both theoretical and empirical work in neuroscience, conceptually aligning the study of hierarchical behavior with research on other aspects of learning and decision making, and giving rise to some thought-provoking new findings. ?? 2012.},
author = {Botvinick, Matthew Michael},
doi = {10.1016/j.conb.2012.05.008},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Botvinick/2012/Hierarchical reinforcement learning and decision making.pdf:pdf},
isbn = {0818653302},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
number = {6},
pages = {956--962},
pmid = {22695048},
publisher = {Elsevier Ltd},
title = {{Hierarchical reinforcement learning and decision making}},
url = {http://dx.doi.org/10.1016/j.conb.2012.05.008},
volume = {22},
year = {2012}
}
@article{Dietterich2000,
abstract = {This paper presents the MAXQ approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges wih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {cs/9905014},
author = {Dietterich, Thomas G.},
doi = {10.1613/jair.639},
eprint = {9905014},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Dietterich/2000/Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition.pdf:pdf},
isbn = {978-3-540-67839-7},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {227--303},
primaryClass = {cs},
title = {{Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition}},
volume = {13},
year = {2000}
}
@inproceedings{Barto2004a,
abstract = {Humans and other animals often engage in activities for their own sakes rather than as steps toward solving practical problems. Psychologists call these intrinsically motivated behaviors. What we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy. At the core of the model are recent theoretical and algorithmic advances in computational reinforcement learning, specifically, new concepts related to skills and new learning algorithms for learning with skill hierarchies. 1},
author = {Barto, Andrew G. and Singh, Satinder},
booktitle = {Proceedings of the 3rd International Conference on Development and Learning (ICDL 2004)},
doi = {10.1.1.123.395},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Barto, Singh/2004/Intrinsically motivated learning of hierarchical collections of skills.pdf:pdf},
pages = {112--119},
title = {{Intrinsically motivated learning of hierarchical collections of skills}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.117.6436{\%}5Cnhttp://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A083E47D2FE080D11716EC249E464BE2?doi=10.1.1.117.6436{\&}rep=rep1{\&}type=pdf},
year = {2004}
}
