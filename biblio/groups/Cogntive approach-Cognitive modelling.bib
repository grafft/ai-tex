Automatically generated by Mendeley Desktop 1.17.6
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Balaguer2016,
author = {Balaguer, Jan and Spiers, Hugo and Hassabis, Demis and Summerfield, Christopher},
doi = {10.1016/j.neuron.2016.03.037},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neuron/2016/Balaguer et al. - 2016.pdf:pdf},
issn = {08966273},
journal = {Neuron},
number = {4},
pages = {893--903},
pmid = {27196978},
publisher = {The Authors},
title = {{Neural Mechanisms of Hierarchical Planning in a Virtual Subway Network}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627316300575},
volume = {90},
year = {2016}
}
@article{Frank2012,
abstract = {Growing evidence suggests that the prefrontal cortex (PFC) is organized hierarchically, with more anterior regions having increasingly abstract representations. How does this organization support hierarchical cognitive control and the rapid discovery of abstract action rules? We present computational models at different levels of description. A neural circuit model simulates interacting corticostriatal circuits organized hierarchically. In each circuit, the basal ganglia gate frontal actions, with some striatal units gating the inputs to PFC and others gating the outputs to influence response selection. Learning at all of these levels is accomplished via dopaminergic reward prediction error signals in each corticostriatal circuit. This functionality allows the system to exhibit conditional if-then hypothesis testing and to learn rapidly in environments with hierarchical structure. We also develop a hybrid Bayesian-reinforcement learning mixture of experts (MoE) model, which can estimate the most likely hypothesis state of individual participants based on their observed sequence of choices and rewards. This model yields accurate probabilistic estimates about which hypotheses are attended by manipulating attentional states in the generative neural model and recovering them with the MoE model. This 2-pronged modeling approach leads to multiple quantitative predictions that are tested with functional magnetic resonance imaging in the companion paper.},
author = {Frank, Michael J. and Badre, David},
doi = {10.1093/cercor/bhr114},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cerebral cortex (New York, N.Y. 1991)/2012/Frank, Badre - 2012.pdf:pdf},
issn = {1460-2199},
journal = {Cerebral cortex (New York, N.Y. : 1991)},
keywords = {Computer Simulation,Corpus Striatum,Corpus Striatum: cytology,Corpus Striatum: physiology,Humans,Learning,Learning: physiology,Models,Neural Pathways,Neural Pathways: cytology,Neural Pathways: physiology,Neurological,Neurons,Neurons: physiology,Prefrontal Cortex,Prefrontal Cortex: cytology,Prefrontal Cortex: physiology,Reinforcement (Psychology)},
number = {3},
pages = {509--26},
pmid = {21693490},
title = {{Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: Computational analysis}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3278315{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {22},
year = {2012}
}
@incollection{Hexmoor1996,
abstract = {Routine interactions in the world of an autonomous agent are a major source of learning for the agent. In my approach an agent interacts in the world in several different ways, from cognitive to automatic. I show that an agent can learn and also improve its routine interactions in its different modes of interaction in the world. I present a formalism and use for a goal structure known as goal sketch [11]. Rewards and punishments generated from a goal sketch which indicate progress in goal satisfaction are used to improve automatic interactions and enhance agent's strategies and concepts about action. I will discuss my experiments with a physical robot that uses a goal sketch in order to generate rewards and punishments which are then used in improving robot skills and discovering new actions.},
author = {Hexmoor, Henry},
booktitle = {Intelligent Agents II Agent Theories, Architectures, and Languages},
doi = {10.1007/3540608052_61},
editor = {Wooldridge, Michael and Muller, J{\"{o}}rg P. and Tambe, Milind},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Intelligent Agents II Agent Theories, Architectures, and Languages/1996/Hexmoor - 1996.pdf:pdf},
pages = {97--110},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Learning Routines}},
year = {1996}
}
@article{Buschman2015,
abstract = {The brain has a limited capacity and therefore needs mechanisms to selectively enhance the information most relevant to one's current behavior. We refer to these mechanisms as "attention." Attention acts by increasing the strength of selected neural representations and preferentially routing them through the brain's large-scale network. This is a critical component of cognition and therefore has been a central topic in cognitive neuroscience. Here we review a diverse literature that has studied attention at the level of behavior, networks, circuits, and neurons. We then integrate these disparate results into a unified theory of attention.},
author = {Buschman, Timothy J and Kastner, Sabine},
doi = {10.1016/j.neuron.2015.09.017},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neuron/2015/Buschman, Kastner - 2015.pdf:pdf},
issn = {08966273},
journal = {Neuron},
number = {1},
pages = {127--144},
pmid = {26447577},
publisher = {Elsevier Inc.},
title = {{From Behavior to Neural Dynamics: An Integrated Theory of Attention}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26447577 http://dx.doi.org/10.1016/j.neuron.2015.09.017},
volume = {88},
year = {2015}
}
@article{Sergin2008,
author = {Sergin, A. V. and Sergin, V. Ya.},
journal = {Neural Network World},
number = {3},
pages = {227--244},
title = {{Model of perception: The hierarchy of inclusive sensory characteristics and top-down cascade transfer of excitation}},
volume = {18},
year = {2008}
}
@article{Fernando2013,
abstract = {How do human infants learn the causal dependencies between events? Evidence suggests that this remarkable feat can be achieved by observation of only a handful of examples. Many computational models have been produced to explain how infants perform causal inference without explicit teaching about statistics or the scientific method. Here, we propose a spiking neuronal network implementation that can be entrained to form a dynamical model of the temporal and causal relationships between events that it observes. The network uses spike-time dependent plasticity, long-term depression, and heterosynaptic competition rules to implement Rescorla-Wagner-like learning. Transmission delays between neurons allow the network to learn a forward model of the temporal relationships between events. Within this framework, biologically realistic synaptic plasticity rules account for well-known behavioral data regarding cognitive causal assumptions such as backwards blocking and screening-off. These models can then be run as emulators for state inference. Furthermore, this mechanism is capable of copying synaptic connectivity patterns between neuronal networks by observing the spontaneous spike activity from the neuronal circuit that is to be copied, and it thereby provides a powerful method for transmission of circuit functionality between brain regions.},
author = {Fernando, Chrisantha},
doi = {10.1111/cogs.12073},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive science/2013/Fernando - 2013.pdf:pdf},
issn = {1551-6709},
journal = {Cognitive science},
keywords = {backwards blocking,causal inference,groups,neuronal replicator hypothesis,polychronous,rational process model,screening-off},
number = {8},
pages = {1426--70},
pmid = {23957457},
title = {{From blickets to synapses: inferring temporal causal networks by observation}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23957457},
volume = {37},
year = {2013}
}
@article{Ivanitsky1996,
author = {Иваницкий, А. М.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Журнал высшей нервной деятельности/1996/Иваницкий - 1996.pdf:pdf},
journal = {Журнал высшей нервной деятельности},
language = {russian},
number = {2},
pages = {241--282},
title = {{Мозговая основа субъективных переживаний: гипотеза информационного синтеза}},
volume = {46},
year = {1996}
}
@inproceedings{Tarasov2016,
author = {Тарасов, В. Б.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2016/Тарасов - 2016.pdf:pdf},
language = {russian},
pages = {94--114},
title = {{От спецификации когнитонов и инженерии интенций к обобщенной архитектуре деятельности агентов}},
year = {2016}
}
@article{Chernavsky2012b,
author = {Чернавская, О. Д. and Чернавский, Д. С. and Карп, В. П. and Никитин, А. П. and Рожило, Я. А.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Сложные системы/2012/Чернавская et al. - 2012.pdf:pdf},
journal = {Сложные системы},
language = {russian},
number = {2},
pages = {25--41},
title = {{Процесс мышления в контексте динамической теории информации. Часть I. Цели и задачи мышления}},
volume = {1},
year = {2012}
}
@article{Sanborn2010,
author = {Sanborn, Adam N. and Griffiths, Thomas L. and Shiffrin, Richard M.},
doi = {10.1016/j.cogpsych.2009.07.001},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive Psychology/2010/Sanborn, Griffiths, Shiffrin - 2010.pdf:pdf},
issn = {00100285},
journal = {Cognitive Psychology},
number = {2},
pages = {63--106},
publisher = {Elsevier Inc.},
title = {{Uncovering mental representations with Markov chain Monte Carlo}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0010028509000449},
volume = {60},
year = {2010}
}
@article{Thilakarathne2015a,
abstract = {Human awareness under different circumstances is complex and non-trivial to understand. Nevertheless, due to the importance of awareness for safety and efficiency in many domains (e.g., the aviation domain), it is necessary to study the processes behind situation awareness, to eliminate possible errors in action selection that may lead to disasters. Interesting models for situation awareness have been presented, mainly from an ecological psychology perspective, but they are debatable with respect to the latest neurocognitive evidences. With the developments in brain imaging and recording techniques, more and more detailed information on complex cognitive processes becomes available. This provides room to further investigate the mechanisms behind many cognitive phenomena, including situation awareness. This paper presents a computational cognitive agent model for situation awareness from the perspective of action selection, which is inspired by neurocognitive evidences. The model integrates bottom-up and top-down cognitive processes, related to various cognitive states: perception, desires, attention, intention, (prior and retrospective) awareness, ownership, feeling, and communication. Based on the model, various cognitive effects can be explained, such as perceptual load, predictive processes, inferential processes, cognitive controlling, unconscious bias, and conscious bias. A model like this will be useful in domains that benefit from complex simulations of socio-technical systems (e.g. the aviation domain) based on computational models of human behaviour. In such domains, existing agent-based simulations are limited, since most of the agent models do not include realistic nature-inspired processes. The validity of the model is illustrated based on simulations for the aviation domain, focusing on a particular situation where an agent has biased perception, poor comprehension, habitual driven projection, and conflict between prior and retrospective effects on action execution.},
author = {Thilakarathne, Dilhan J.},
doi = {10.1016/j.bica.2015.04.010},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Biologically Inspired Cognitive Architectures/2015/Thilakarathne - 2015.pdf:pdf},
isbn = {2212-683X},
issn = {2212683X},
journal = {Biologically Inspired Cognitive Architectures},
keywords = {Bottom-up,Cognitive modelling,Prior and retrospective awareness,Situation awareness,Top-down},
pages = {77--104},
publisher = {Elsevier B.V.},
title = {{Modelling of situation awareness with perception, attention, and prior and retrospective awareness}},
url = {http://dx.doi.org/10.1016/j.bica.2015.04.010},
volume = {12},
year = {2015}
}
@article{Crawford2016,
author = {Crawford, Eric and Gingerich, Matthew and Eliasmith, Chris},
doi = {10.1111/cogs.12261},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive Science/2016/Crawford, Gingerich, Eliasmith - 2016.pdf:pdf},
issn = {03640213},
journal = {Cognitive Science},
keywords = {biologically plausible,connectionism,knowledge representation,neural network,scaling,vector symbolic architecture,wordnet},
number = {4},
pages = {782--821},
pmid = {26173464},
title = {{Biologically Plausible, Human-Scale Knowledge Representation}},
url = {http://doi.wiley.com/10.1111/cogs.12261},
volume = {40},
year = {2016}
}
@article{Rensink2000,
author = {Rensink, Ronald A.},
doi = {10.1080/135062800394667},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Visual Cognition/2000/Rensink - 2000.pdf:pdf},
issn = {1350-6285},
journal = {Visual Cognition},
number = {1-3},
pages = {17--42},
title = {{The Dynamic Representation of Scenes}},
url = {http://www.tandfonline.com/doi/abs/10.1080/135062800394667},
volume = {7},
year = {2000}
}
@article{Habenschuss2013,
abstract = {Experimental data from neuroscience suggest that a substantial amount of knowledge is stored in the brain in the form of probability distributions over network states and trajectories of network states. We provide a theoretical foundation for this hypothesis by showing that even very detailed models for cortical microcircuits, with data-based diverse nonlinear neurons and synapses, have a stationary distribution of network states and trajectories of network states to which they converge exponentially fast from any initial state. We demonstrate that this convergence holds in spite of the non-reversibility of the stochastic dynamics of cortical microcircuits. We further show that, in the presence of background network oscillations, separate stationary distributions emerge for different phases of the oscillation, in accordance with experimentally reported phase-specific codes. We complement these theoretical results by computer simulations that investigate resulting computation times for typical probabilistic inference tasks on these internally stored distributions, such as marginalization or marginal maximum-a-posteriori estimation. Furthermore, we show that the inherent stochastic dynamics of generic cortical microcircuits enables them to quickly generate approximate solutions to difficult constraint satisfaction problems, where stored knowledge and current inputs jointly constrain possible solutions. This provides a powerful new computing paradigm for networks of spiking neurons, that also throws new light on how networks of neurons in the brain could carry out complex computational tasks such as prediction, imagination, memory recall and problem solving.},
author = {Habenschuss, Stefan and Jonke, Zeno and Maass, Wolfgang},
doi = {10.1371/journal.pcbi.1003311},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/PLoS Computational Biology/2013/Habenschuss, Jonke, Maass - 2013.PDF:PDF},
isbn = {1553-7358 (Electronic)$\backslash$r1553-734X (Linking)},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {11},
pages = {e1003311},
pmid = {24244126},
title = {{Stochastic Computations in Cortical Microcircuit Models}},
volume = {9},
year = {2013}
}
@book{Steyvers2003,
abstract = {Information about the structure of a causal system can come in the form of observational data-random samples of the system's autonomous behavior-or interventional data-samples conditioned on the particular values of one or more variables that have been experimentally manipulated. Here we study people's ability to infer causal structure from both observation and intervention, and to choose informative interventions on the basis of observational data. In three causal inference tasks, participants were to some degree capable of distinguishing between competing causal hypotheses on the basis of purely observational data. Performance improved substantially when participants were allowed to observe the effects of interventions that they performed on the systems. We develop computational models of how people infer causal structure from data and how they plan intervention experiments, based on the representational framework of causal graphical models and the inferential principles of optimal Bayesian decision-making and maximizing expected information gain. These analyses suggest that people can make rational causal inferences, subject to psychologically reasonable representational assumptions and computationally reasonable processing constraints. {\textcopyright} 2003 Cognitive Science Society, Inc. All rights reserved.},
author = {Steyvers, Mark and Tenenbaum, Joshua B. and Wagenmakers, Eric Jan and Blum, Ben},
booktitle = {Cognitive Science},
doi = {10.1016/S0364-0213(03)00010-7},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive Science/2003/Steyvers et al. - 2003.pdf:pdf},
isbn = {1949824764},
issn = {03640213},
keywords = {Active learning,Bayesian models,Bayesian networks,Causal reasoning,Computer simulation,Decision making,Human experimentation,Hypothesis testing,Interventions,Observational learning,Rational inference,Structure learning,Web experiments},
number = {3},
pages = {453--489},
title = {{Inferring causal networks from observations and interventions}},
volume = {27},
year = {2003}
}
@article{Blanke2015,
abstract = {Recent work in human cognitive neuroscience has linked self-consciousness to the processing of multisen- sory bodily signals (bodily self-consciousness [BSC]) in fronto-parietal cortex and more posterior temporo- parietal regions.Wehighlight the behavioral, neurophysiological, neuroimaging, and computational laws that subtend BSCin humans and non-human primates.Wepropose thatBSCincludes body-centered perception (hand, face, and trunk), based on the integration of proprioceptive, vestibular, and visual bodily inputs, and involves spatio-temporal mechanisms integrating multisensory bodily stimuli within peripersonal space (PPS). We develop four major constraints of BSC (proprioception, body-related visual information, PPS, and embodiment) and argue that the fronto-parietal and temporo-parietal processing of trunk-centered multisensory signals in PPS is of particular relevance for theoretical models and simulations of BSC and eventually of self-consciousness.},
author = {Blanke, Olaf and Slater, Mel and Serino, Andrea},
doi = {10.1016/j.neuron.2015.09.029},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neuron/2015/Blanke, Slater, Serino - 2015.pdf:pdf},
issn = {08966273},
journal = {Neuron},
number = {1},
pages = {145--166},
publisher = {Elsevier Inc.},
title = {{Behavioral, Neural, and Computational Principles of Bodily Self-Consciousness}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627315008181},
volume = {88},
year = {2015}
}
@article{Borji2013,
abstract = {Modeling visual attention--particularly stimulus-driven, saliency-based attention--has been a very active research area over the past 25 years. Many different models of attention are now available which, aside from lending theoretical contributions to other fields, have demonstrated successful applications in computer vision, mobile robotics, and cognitive systems. Here we review, from a computational perspective, the basic concepts of attention implemented in these models. We present a taxonomy of nearly 65 models, which provides a critical comparison of approaches, their capabilities, and shortcomings. In particular, 13 criteria derived from behavioral and computational studies are formulated for qualitative comparison of attention models. Furthermore, we address several challenging issues with models, including biological plausibility of the computations, correlation with eye movement datasets, bottom-up and top-down dissociation, and constructing meaningful performance measures. Finally, we highlight current research trends in attention modeling and provide insights for future.},
author = {Borji, Ali and Itti, Laurent},
doi = {10.1109/TPAMI.2012.89},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/IEEE transactions on pattern analysis and machine intelligence/2013/Borji, Itti - 2013.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Animals,Attention,Attention: physiology,Computer Simulation,Fixation,Humans,Models,Neurological,Ocular,Ocular: physiology,Visual Cortex,Visual Cortex: physiology,Visual Perception,Visual Perception: physiology},
number = {1},
pages = {185--207},
pmid = {22487985},
title = {{State-of-the-art in visual attention modeling}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22487985},
volume = {35},
year = {2013}
}
@article{Aoun2014,
author = {Aoun, Mario Antoine and Boukadoum, Mounir},
doi = {10.1109/ICCI-CC.2014.6921451},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/2014 IEEE 13th International Conference on Cognitive Informatics and Cognitive Computing/2014/Aoun, Boukadoum - 2014.pdf:pdf},
isbn = {978-1-4799-6081-1},
journal = {2014 IEEE 13th International Conference on Cognitive Informatics and Cognitive Computing},
keywords = {chaos control,chaotic spiking neural network,inputs to a pool,liquid,nds neuron,nonlinear transient computation,of,online signature verification,property,sp mentions that different,state machines,stdp},
pages = {126--132},
publisher = {Ieee},
title = {{Learning algorithm and neurocomputing architecture for NDS Neurons}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6921451},
year = {2014}
}
@article{Navalpakkam2005,
abstract = {We propose a computational model for the task-specific guidance of visual attention in real-world scenes. Our model emphasizes four aspects that are important in biological vision: determining task-relevance of an entity, biasing attention for the low-level visual features of desired targets, recognizing these targets using the same low-level features, and incrementally building a visual map of task-relevance at every scene location. Given a task definition in the form of keywords, the model first determines and stores the task-relevant entities in working memory, using prior knowledge stored in long-term memory. It attempts to detect the most relevant entity by biasing its visual attention system with the entity's learned low-level features. It attends to the most salient location in the scene, and attempts to recognize the attended object through hierarchical matching against object representations stored in long-term memory. It updates its working memory with the task-relevance of the recognized entity and updates a topographic task-relevance map with the location and relevance of the recognized entity. The model is tested on three types of tasks: single-target detection in 343 natural and synthetic images, where biasing for the target accelerates target detection over twofold on average; sequential multiple-target detection in 28 natural images, where biasing, recognition, working memory and long term memory contribute to rapidly finding all targets; and learning a map of likely locations of cars from a video clip filmed while driving on a highway. The model's performance on search for single features and feature conjunctions is consistent with existing psychophysical data. These results of our biologically-motivated architecture suggest that the model may provide a reasonable approximation to many brain processes involved in complex task-driven visual behaviors.},
author = {Navalpakkam, Vidhya and Itti, Laurent},
doi = {10.1016/j.visres.2004.07.042},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Vision research/2005/Navalpakkam, Itti - 2005.pdf:pdf},
issn = {0042-6989},
journal = {Vision research},
keywords = {Attention,Attention: physiology,Cues,Humans,Memory,Memory: physiology,Models,Pattern Recognition,Psychological,Psychophysics,Recognition (Psychology),Recognition (Psychology): physiology,Visual,Visual Perception,Visual Perception: physiology},
number = {2},
pages = {205--31},
pmid = {15581921},
title = {{Modeling the influence of task on attention}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15581921},
volume = {45},
year = {2005}
}
@article{Dipoppa2013,
abstract = {Cognitive effort leads to a seeming cacophony of brain oscillations. For example, during tasks engaging working memory (WM), specific oscillatory frequency bands modulate in space and time. Despite ample data correlating such modulation to task performance, a mechanistic explanation remains elusive. We propose that flexible control of neural oscillations provides a unified mechanism for the rapid and controlled transitions between the computational operations required by WM. We show in a spiking network model that modulating the input oscillation frequency sets the network in different operating modes: rapid memory access and load is enabled by the beta-gamma oscillations, maintaining a memory while ignoring distractors by the theta, rapid memory clearance by the alpha. The various frequency bands determine the dynamic gating regimes enabling the necessary operations for WM, whose succession explains the need for the complex oscillatory brain dynamics during effortful cognition.},
author = {Dipoppa, Mario and Gutkin, Boris S},
doi = {10.1073/pnas.1303270110},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceedings of the National Academy of Sciences of the United States of America/2013/Dipoppa, Gutkin - 2013.pdf:pdf},
isbn = {1303270110},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Biological Clocks,Biological Clocks: physiology,Cerebral Cortex,Cerebral Cortex: physiology,Cognition,Cognition: physiology,Humans,Memory,Memory: physiology,Models, Neurological,Neurons,Neurons: physiology},
number = {31},
pages = {12828--33},
pmid = {23858465},
title = {{Flexible frequency control of cortical oscillations enables computations required for working memory.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3732977{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {110},
year = {2013}
}
@article{Cisek2007,
author = {Cisek, P.},
doi = {10.1098/rstb.2007.2054},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Philosophical Transactions of the Royal Society B Biological Sciences/2007/Cisek - 2007.pdf:pdf},
issn = {0962-8436},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
keywords = {action selection,cerebral cortex,computational modelling,decision making},
number = {1485},
pages = {1585--1599},
title = {{Cortical mechanisms of action selection: the affordance competition hypothesis}},
url = {http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2007.2054},
volume = {362},
year = {2007}
}
@article{Hampshire2016,
abstract = {The ability to learn new tasks rapidly is a prominent characteristic of human behaviour. This ability relies on flexible cognitive systems that adapt in order to encode temporary programs for processing non-automated tasks. Previous functional imaging studies have revealed distinct roles for the lateral frontal cortices (LFCs) and the ventral striatum in intentional learning processes. However, the human LFCs are complex; they house multiple distinct sub-regions, each of which co-activates with a different functional network. It remains unclear how these LFC networks differ in their functions and how they coordinate with each other, and the ventral striatum, to support intentional learning. Here, we apply a suite of fMRI connectivity methods to determine how LFC networks activate and interact at different stages of two novel tasks, in which arbitrary stimulus-response rules are learnt either from explicit instruction or by trial-and-error. We report that the networks activate en masse and in synchrony when novel rules are being learnt from instruction. However, these networks are not homogeneous in their functions; instead, the directed connectivities between them vary asymmetrically across the learning timecourse and they disengage from the task sequentially along a rostro-caudal axis. Furthermore, when negative feedback indicates the need to switch to alternative stimulus-response rules, there is additional input to the LFC networks from the ventral striatum. These results support the hypotheses that LFC networks interact as a hierarchical system during intentional learning and that signals from the ventral striatum have a driving influence on this system when the internal program for processing the task is updated.},
author = {Hampshire, Adam and Hellyer, Peter J. and Parkin, Beth and Hiebert, Nole and MacDonald, Penny and Owen, Adrian M. and Leech, Robert and Rowe, James},
doi = {10.1016/j.neuroimage.2015.11.060},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/NeuroImage/2016/Hampshire et al. - 2016.pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
keywords = {Caudate,Dynamic causal modelling,Frontal cortex,Functional connectivity,Learning},
pages = {123--134},
publisher = {The Authors},
title = {{Network mechanisms of intentional learning}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2015.11.060},
volume = {127},
year = {2016}
}
@article{Simoes2015,
abstract = {Understanding consciousness is one of the most fasci- nating challenges of our time. Fromancient civilizations to modern philosophers, questions have been asked on howone is conscious of his/her own existence and about theworld that surrounds him/her. Although there is no precise definition for consciousness, there is an agreement that it is strongly related to human cognitive pro- cesses such as attention, a process capable of promoting a selection of a few stimuli from a huge amount of information that reaches us constantly. In order to bring the consciousness discussion to a com- putational scenario, this paper presents conscious attention-based integrated model (CONAIM), a formal model for machine con- sciousness based on an attentional schema for human-like agent cognition that integrates: short- and long-term memories, rea- soning, planning, emotion, decision-making, learning, motivation, and volition. Experimental results in a mobile robotics domain show that the agent can attentively use motivation, volition, and memories to set its goals and learn new concepts and procedures based on exogenous and endogenous stimuli. By performing com- putation over an attentional space, the model also allowed the agent to learn over a much reduced state space. Further imple- mentation under this model could potentially allow the agent to express sentience, self-awareness, self-consciousness, autonoetic consciousness, mineness, and perspectivalness.},
author = {Simoes, Alexandre and Colmbini, Esther and Ribeiro, Carlos},
doi = {10.1109/JSYST.2015.2498542},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/IEEE Systems Journal/2016/Simoes, Colmbini, Ribeiro - 2016.pdf:pdf},
issn = {19379234},
journal = {IEEE Systems Journal},
number = {99},
pages = {1--12},
title = {{CONAIM : A Conscious Attention-Based Integrated Model for Human-Like Robots}},
year = {2016}
}
@article{Yan2014,
abstract = {The accessibility verification of the assembly/disassembly plays an important role in the process of product design. In the last decade, the sampling based motion planners have been successfully applied to solve the accessibility verification. However, the narrow passage which is a common problem in the assembly tasks is still a bottleneck. Meanwhile, the requirement of perception and emotion assessment drives the interaction between users and automatic path planners in the virtual assembly process. In this paper, a curve matching method is used to explore the implicit relationship between the topological information of scenarios and the motion of objects, based on which an interactive motion planning framework that can learn from experience is constructed. Our framework consists of two main processes: a learning process and a motion generation process. In the former process, the motion segment (a part of motion path) and its related scenario segment (a part of workspace passed through by the object) are gathered, after an interactive motion planning process finds a collision-free motion path or reaches the conclusion of inaccessibility. According to the similarity between the skeletons of scenario segments, the gathered scenario segments and motion segments are organized by a hierarchical structure in the motion library. The latter process permits users to control only one point in the workspace for the selection of a new scenario, and then the similar scenarios are retrieved from the motion library, to help quickly detect the connectivity of the new scenario and generate a repaired motion path to guide users with feasible manipulations. We highlight the performance of our framework on a challenging problem in 2D, in which a non-convex object passes through a cluttered environment filled with randomly shaped and located non-convex obstacles.},
author = {Yan, Yu and Poirson, Emilie and Bennis, Fouad},
doi = {10.1016/j.cad.2014.07.007},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Computer-Aided Design/2014/Yan, Poirson, Bennis - 2014.pdf:pdf},
issn = {00104485},
journal = {Computer-Aided Design},
keywords = {accessibility verification,assembly path planning},
pages = {23--38},
publisher = {Elsevier Ltd},
title = {{An interactive motion planning framework that can learn from experience}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0010448514001584},
volume = {59},
year = {2014}
}
@article{Culhane1995,
author = {Tsotsos, John K. and Culhane, Sean M and Wai, Winky Yan Kei and Lai, Yuzhong and Davis, Neal and Nuflo, Fernando},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Artificial Intelligence/1995/Tsotsos et al. - 1995.pdf:pdf},
journal = {Artificial Intelligence},
number = {78},
pages = {507--545},
title = {{Modeling visual attention via selective tuning}},
year = {1995}
}
@article{Klampfl2013,
abstract = {Numerous experimental data suggest that simultaneously or sequentially activated assemblies of neurons play a key role in the storage and computational use of long-term memory in the brain. However, a model that elucidates how these memory traces could emerge through spike-timing-dependent plasticity (STDP) has been missing. We show here that stimulus-specific assemblies of neurons emerge automatically through STDP in a simple cortical microcircuit model. The model that we consider is a randomly connected network of well known microcircuit motifs: pyramidal cells with lateral inhibition. We show that the emergent assembly codes for repeatedly occurring spatiotemporal input patterns tend to fire in some loose, sequential manner that is reminiscent of experimentally observed stereotypical trajectories of network states. We also show that the emergent assembly codes add an important computational capability to standard models for online computations in cortical microcircuits: the capability to integrate information from long-term memory with information from novel spike inputs.},
author = {Klampfl, Stefan and Maass, Wolfgang},
doi = {10.1523/JNEUROSCI.5044-12.2013},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/The Journal of neuroscience the official journal of the Society for Neuroscience/2013/Klampfl, Maass - 2013.pdf:pdf},
isbn = {1529-2401 (Electronic)$\backslash$r0270-6474 (Linking)},
issn = {1529-2401},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
keywords = {Action Potentials,Action Potentials: physiology,Cerebral Cortex,Cerebral Cortex: physiology,Memory,Memory: physiology,Models,Neural Networks (Computer),Neurological,Neuronal Plasticity,Neuronal Plasticity: physiology},
number = {28},
pages = {11515--11529},
pmid = {23843522},
title = {{Emergence of dynamic memory traces in cortical microcircuit models through STDP.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23843522},
volume = {33},
year = {2013}
}
@inproceedings{Process,
abstract = {This paper shows a formal model of cognitive function of decision-making. The decision-making is only one of several cognitive functions of high level of natural intelligence. Our model has been inspired by human decision-making process. In order to show a comprehensive and coherent model of human decision-making process based on a rigorous formalism, we have adopted a multidisciplinary approach encompassing knowledge in cognitive informatics, neuroscience, and psychology. The model has been divided into conceptual, formal, and computational model. However, in this paper we show the conceptual and part of the formal model. In order to develop a comprehensive and coherent conceptual model of the decision- making process and its relationship with others cognitive processes, we have adopted the layered reference model postulated by Wang. Our conceptual model shows the main brain areas involved in the decision-making process and describes their functions. While our formal model tries to show a rigorous explanation for the cognitive decision-making process.},
author = {Process, Decision-making and Wang, Yingxu},
booktitle = {Proceedings IEEE International Conference on Cognitive Inlormatics {\&} Cognitive Computing},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceedings IEEE International Conference on Cognitive Inlormatics {\&} Cognitive Computing/2015/Process, Wang - 2015.pdf:pdf},
isbn = {9181461312909},
keywords = {cognitive informatics,decision-making,fuzzy logic,natural intelligence},
pages = {375--383},
title = {{A Formal Model Inspired on Human Decision-Making Process}},
year = {2015}
}
@article{Fu2014a,
author = {Fu, XiaoLan and Cai, LianHong and Liu, Ye YongJin and Jia, Jia and Chen, WenFeng and Yi, Zhang and Zhao, GuoZhen and Liu, Ye YongJin and Wu, ChangXu},
doi = {10.1007/s11432-013-4911-9},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Science China Information Sciences/2014/Fu et al. - 2014.pdf:pdf},
issn = {1674-733X},
journal = {Science China Information Sciences},
keywords = {NeuroModels,computational cognition model,judgment,memory,perception},
language = {english},
mendeley-tags = {NeuroModels},
number = {3},
pages = {1--15},
title = {{A computational cognition model of perception, memory, and judgment}},
url = {http://link.springer.com/10.1007/s11432-013-4911-9},
volume = {57},
year = {2014}
}
@article{Wang2012,
abstract = {In this review, I briefly summarize current neurobiological studies of decision-making that bear on two general themes. The first focuses on the nature of neural representation and dynamics in a decision circuit. Experimental and computational results suggest that ramping-to-threshold in the temporal domain and trajectory of population activity in the state space represent a duality of perspectives on a decision process. Moreover, a decision circuit can display several different dynamical regimes, such as the ramping mode and the jumping mode with distinct defining properties. The second is concerned with the relationship between biologically-based mechanistic models and normative-type models. A fruitful interplay between experiments and these models at different levels of abstraction have enabled investigators to pose increasingly refined questions and gain new insights into the neural basis of decision-making. In particular, recent work on multi-alternative decisions suggests that deviations from rational models of choice behavior can be explained by established neural mechanisms. ?? 2012 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Wang, Xiao Jing},
doi = {10.1016/j.conb.2012.08.006},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Current Opinion in Neurobiology/2012/Wang - 2012.pdf:pdf},
isbn = {1873-6882 (Electronic)$\backslash$r0959-4388 (Linking)},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
number = {6},
pages = {1039--1046},
pmid = {23026743},
publisher = {Elsevier Ltd},
title = {{Neural dynamics and circuit mechanisms of decision-making}},
url = {http://dx.doi.org/10.1016/j.conb.2012.08.006},
volume = {22},
year = {2012}
}
@article{Stankevich2006,
author = {Станкевич, Л. А. and Серебряков, С. В.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Труды СПИИРАН/2006/Станкевич, Серебряков - 2006.pdf:pdf},
journal = {Труды СПИИРАН},
language = {russian},
number = {3},
pages = {71--87},
title = {{Когнитивные системы и агенты}},
volume = {1},
year = {2006}
}
@article{Lamme2003,
abstract = {Now that the study of consciousness is warmly embraced by cognitive scientists, much confusion seems to arise between the concepts of visual attention and visual awareness. Often, visual awareness is equated to what is in the focus of attention. There are, however, two sets of arguments to separate attention from awareness: a psychological/theoretical one and a neurobiological one. By combining these arguments I present definitions of visual attention and awareness that clearly distinguish between the two, yet explain why attention and awareness are so intricately related. In fact, there seems more overlap between mechanisms of memory and awareness than between those of attention and awareness.},
author = {Lamme, V. a F},
doi = {10.1016/S1364-6613(02)00013-X},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Trends in Cognitive Sciences/2003/Lamme - 2003.pdf:pdf},
isbn = {1364-6613},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {1},
pages = {12--18},
pmid = {12517353},
title = {{Why visual attention and awareness are different}},
volume = {7},
year = {2003}
}
@inproceedings{Drix2014,
author = {Drix, Damien and Hafner, Verena V},
booktitle = {Joint IEEE International Conference on Development and Learning},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Joint IEEE International Conference on Development and Learning/2014/Drix, Hafner - 2014.pdf:pdf},
isbn = {9781479975402},
pages = {374--378},
title = {{Learning proprioceptive and motor features}},
year = {2014}
}
@inproceedings{Licato2015,
abstract = {The ability to generate explanations of perceived events and of one's own actions is of central importance to how we make sense of the world. When modeling explanation generation, one common tactic used by cognitive systems is to construct a linkage of previously created cause- effect pairs. But where do such cause-effect pairs come from in the first place, and how can they be created automatically by cognitive systems? In this paper, we discuss the development of causal representations in children, by analyzing the literature surrounding a Piagetian experiment, and show how the conditions making cause-effect pair creation possi- ble can start to be modeled using a combination of feature-extraction techniques and the structured knowledge representation in the hybrid cognitive architecture CLARION. We create a task in PAGI World for learning causality, and make this task available for download.},
author = {Licato, John and Marton, Nick and Dong, Boning and Sun, Ron and Bringsjord, Selmer},
booktitle = {Proceedings of the 3rd International Workshop on Artificial Intelligence and Cognition},
editor = {Lieto, Antonio and Battaglino, Cristina and Radicioni, Daniele P. and Sanguinetti, Manuela},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceedings of the 3rd International Workshop on Artificial Intelligence and Cognition/2015/Licato et al. - 2015.pdf:pdf},
keywords = {analogy,clarion,cognitive architecture,explanation},
pages = {29--39},
series = {CEUR Workshop Proceedings},
title = {{Modeling the Creation and Development of Cause-Effect Pairs for Explanation Generation in a Cognitive Architecture}},
year = {2015}
}
@incollection{Osaka2016,
abstract = {‘Working memory' refers to the capacity-constrained active memory in which information is temporarily maintained and concurrently processed for the use in an ongoing goal-directed activity. The neural mechanisms responsible for con- sciousness are located in certain brain regions, such as the DLPFC, PPC TPJ and ACC, and these brain regions are coupled with a network that includes the central executive of working memory. In this chapter, we explore the nature of the neural basis of working memory and try to explain the mechanisms of working memory. In order to understand the neural basis of active consciousness, we also investigate how information is controlled by the neural basis of working memory. We use reading span test (RST), which measures the working memory capacity to memo- rize the target words of sentences during reading, to measure individual differences in working memory capacity.},
author = {Osaka, Mariko},
booktitle = {Cognitive Neuroscience Robotics B},
doi = {10.1007/978-4-431-54598-9_3},
editor = {Kasaki, Masashi and Ishiguro, Hiroshi and Asada, Minoru and Osaka, Mariko and Fujikado, Takashi},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive Neuroscience Robotics B/2016/Osaka - 2016.pdf:pdf},
isbn = {978-4-431-54594-1},
pages = {39--57},
publisher = {Springer Japan},
title = {{Working Memory as a Basis of Consciousness}},
url = {http://link.springer.com/10.1007/978-4-431-54598-9{\_}3},
year = {2016}
}
@article{Franklin1999,
author = {Franklin, S. and Graesser, A.},
journal = {Conscious Cognition},
number = {8},
pages = {285--301},
title = {{A software agent model of consciousness}},
year = {1999}
}
@article{Bruce2009,
abstract = {A proposal for saliency computation within the visual cortex is put forth based on the premise that localized saliency computation serves to maximize information sampled from one's environment. The model is built entirely on computational constraints but nevertheless results in an architecture with cells and connectivity reminiscent of that appearing in the visual cortex. It is demonstrated that a variety of visual search behaviors appear as emergent properties of the model and therefore basic principles of coding and information transmission. Experimental results demonstrate greater efficacy in predicting fixation patterns across two different data sets as compared with competing models.},
author = {Bruce, Neil D. B. and Tsotsos, John K.},
doi = {10.1167/9.3.5},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Journal of Vision/2009/Bruce, Tsotsos - 2009.pdf:pdf},
issn = {1534-7362},
journal = {Journal of Vision},
number = {9},
pages = {1--24},
pmid = {19757944},
title = {{Saliency, attention, and visual search: An information theoretic approach}},
url = {http://journalofvision.org/9/3/5/article.aspx},
volume = {3},
year = {2009}
}
@article{DeWolf2011,
abstract = {Our empirical, neuroscientific understanding of biological motor systems has been rapidly growing in recent years. However, this understanding has not been systematically mapped to a quantitative characterization of motor control based in control theory. Here, we attempt to bridge this gap by describing the neural optimal control hierarchy (NOCH), which can serve as a foundation for biologically plausible models of neural motor control. The NOCH has been constructed by taking recent control theoretic models of motor control, analyzing the required processes, generating neurally plausible equivalent calculations and mapping them on to the neural structures that have been empirically identified to form the anatomical basis of motor control. We demonstrate the utility of the NOCH by constructing a simple model based on the identified principles and testing it in two ways. First, we perturb specific anatomical elements of the model and compare the resulting motor behavior with clinical data in which the corresponding area of the brain has been damaged. We show that damaging the assigned functions of the basal ganglia and cerebellum can cause the movement deficiencies seen in patients with Huntington's disease and cerebellar lesions. Second, we demonstrate that single spiking neuron data from our model's motor cortical areas explain major features of single-cell responses recorded from the same primate areas. We suggest that together these results show how NOCH-based models can be used to unify a broad range of data relevant to biological motor control in a quantitative, control theoretic framework.},
author = {DeWolf, T and Eliasmith, C},
doi = {10.1088/1741-2560/8/6/065009},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Journal of Neural Engineering/2011/DeWolf, Eliasmith - 2011.pdf:pdf},
isbn = {1741-2552},
issn = {1741-2560},
journal = {Journal of Neural Engineering},
number = {6},
pages = {065009},
pmid = {22056418},
title = {{The neural optimal control hierarchy for motor control}},
volume = {8},
year = {2011}
}
@inproceedings{Cubek2015,
author = {Cubek, Richard and Ertel, Wolfgang},
booktitle = {Proceedings of the 2015 IEEE International Conference on Robotics and Automation (ICRA), Seattle, Washington, USA, May 26-30, 2015},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceedings of the 2015 IEEE International Conference on Robotics and Automation (ICRA), Seattle, Washington, USA, May 26-30, 2015/2015/Cubek, Ertel - 2015.pdf:pdf},
isbn = {9781479969234},
pages = {2592--2597},
publisher = {IEEE},
title = {{High-Level Learning from Demonstration with Conceptual Spaces and Subspace Clustering}},
year = {2015}
}
@article{Paraense2016,
abstract = {In this work, we present a distributed cognitive architecture used to control the traffic in an urban network. This architecture relies on a machine consciousness approach – Global Work- space Theory – in order to use competition and broadcast, allowing a group of local traffic con- trollers to interact, resulting in a better group performance. The main idea is that the local controllers usually perform a purely reactive behavior, defining the times of red and green lights, according just to local information. These local controllers compete in order to define which of them is experiencing the most critical traffic situation. The controller in the worst condition gains access to the global workspace, further broadcasting its condition (and its loca- tion) to all other controllers, asking for their help in dealing with its situation. This call from the controller accessing the global workspace will cause an interference in the reactive local behavior, for those local controllers with some chance in helping the controller in a critical con- dition, by containing traffic in its direction. This group behavior, coordinated by the global workspace strategy, turns the once reactive behavior into a kind of deliberative one. We show that this strategy is capable of improving the overall mean travel time of vehicles flowing through the urban network. A consistent gain in performance with the ‘‘Artificial Consciousness” traffic signal controller during all simulation time, throughout different simulated scenarios, could be observed, ranging from around 13.8{\%} to more than 21{\%}.},
author = {Paraense, Andr{\'{e}} Luis O. and Raizer, Klaus and Gudwin, Ricardo R.},
doi = {10.1016/j.bica.2015.10.001},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Biologically Inspired Cognitive Architectures/2016/Paraense, Raizer, Gudwin - 2016.pdf:pdf},
issn = {2212683X},
journal = {Biologically Inspired Cognitive Architectures},
keywords = {consciousness,global workspace theory,machine consciousness,traffic lights control},
mendeley-tags = {consciousness},
pages = {61--73},
title = {{A machine consciousness approach to urban traffic control}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S2212683X15000614},
volume = {15},
year = {2016}
}
@article{Walther2006,
abstract = {Selective visual attention is believed to be responsible for serializing visual information for recognizing one object at a time in a complex scene. But how can we attend to objects before they are recognized? In coherence theory of visual cognition, so-called proto-objects form volatile units of visual information that can be accessed by selective attention and subsequently validated as actual objects. We propose a biologically plausible model of forming and attending to proto-objects in natural scenes. We demonstrate that the suggested model can enable a model of object recognition in cortex to expand from recognizing individual objects in isolation to sequentially recognizing all objects in a more complex scene.},
author = {Walther, Dirk and Koch, Christof},
doi = {10.1016/j.neunet.2006.10.001},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neural networks/2006/Walther, Koch - 2006.pdf:pdf},
issn = {0893-6080},
journal = {Neural networks},
keywords = {Attention,Biological,Computer Simulation,Discrimination Learning,Discrimination Learning: physiology,Feedback,Humans,Models,Neural Networks (Computer),Pattern Recognition,Photic Stimulation,Photic Stimulation: methods,ROC Curve,Visual,Visual: physiology},
number = {9},
pages = {1395--407},
pmid = {17098563},
title = {{Modeling attention to salient proto-objects}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17098563},
volume = {19},
year = {2006}
}
@article{Triesman1980,
author = {Triesman, A. M. and Gelade, G.},
journal = {Cognitive Psyhology},
pages = {97--136},
title = {{A Feature Integration Theory of Attention}},
volume = {12},
year = {1980}
}
@article{Thilakarathne2015,
author = {Thilakarathne, Dilhan J.},
doi = {10.1016/j.bica.2015.04.010},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Biologically Inspired Cognitive Architectures/2015/Thilakarathne - 2015(2).pdf:pdf},
issn = {2212683X},
journal = {Biologically Inspired Cognitive Architectures},
keywords = {prior and retrospective,situation awareness},
publisher = {Elsevier B.V.},
title = {{Modelling of situation awareness with perception, attention, and prior and retrospective awareness}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S2212683X15000195},
year = {2015}
}
@incollection{Zhdanov1997,
address = {М.},
author = {Жданов, А. А.},
booktitle = {Сборник «Вопросы кибернетики". Научный совет по комплексной проблеме «Кибернетика» РАН. Вып. 3},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Сборник «Вопросы кибернетики. Научный совет по комплексной проблеме «Кибернетика» РАН. Вып. 3/1997/Жданов - 1997.pdf:pdf},
language = {russian},
pages = {258--274},
title = {{Формальная модель нейрона и нейросети в методологии автономного адаптивного управлении}},
year = {1997}
}
@article{Fitch2014,
abstract = {Progress in understanding cognition requires a quantitative, theoretical framework, grounded in the other natural sciences and able to bridge between implementational, algorithmic and computational levels of explanation. I review recent results in neuroscience and cognitive biology that, when combined, provide key components of such an improved conceptual framework for contemporary cognitive science. Starting at the neuronal level, I first discuss the contemporary realization that single neurons are powerful tree-shaped computers, which implies a reorientation of computational models of learning and plasticity to a lower, cellular, level. I then turn to predictive systems theory (predictive coding and prediction-based learning) which provides a powerful formal framework for understanding brain function at a more global level. Although most formal models concerning predictive coding are framed in associationist terms, I argue that modern data necessitate a reinterpretation of such models in cognitive terms: as model-based predictive systems. Finally, I review the role of the theory of computation and formal language theory in the recent explosion of comparative biological research attempting to isolate and explore how different species differ in their cognitive capacities. Experiments to date strongly suggest that there is an important difference between humans and most other species, best characterized cognitively as a propensity by our species to infer tree structures from sequential data. Computationally, this capacity entails generative capacities above the regular (finite-state) level; implementationally, it requires some neural equivalent of a push-down stack. I dub this unusual human propensity "dendrophilia", and make a number of concrete suggestions about how such a system may be implemented in the human brain, about how and why it evolved, and what this implies for models of language acquisition. I conclude that, although much remains to be done, a neurally-grounded framework for theoretical cognitive science is within reach that can move beyond polarized debates and provide a more adequate theoretical future for cognitive biology. {\textcopyright} 2014.},
author = {Fitch, W. Tecumseh},
doi = {10.1016/j.plrev.2014.04.005},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Physics of Life Reviews/2014/Fitch - 2014.pdf:pdf},
issn = {15710645},
journal = {Physics of Life Reviews},
keywords = {Cognitive science,Comparative cognition,Computational neuroscience,Formal language theory,Mathematical psychology,Neurolinguistics},
number = {3},
pages = {329--364},
pmid = {24969660},
publisher = {Elsevier B.V.},
title = {{Toward a computational framework for cognitive biology: Unifying approaches from cognitive neuroscience and comparative cognition}},
url = {http://dx.doi.org/10.1016/j.plrev.2014.04.005},
volume = {11},
year = {2014}
}
@article{Lakhman2013,
author = {Лахман, К. В. and Бурцев, М. С.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Математическая биология и биоинформатика/2013/Лахман, Бурцев - 2013.pdf:pdf},
journal = {Математическая биология и биоинформатика},
language = {russian},
number = {2},
pages = {419--431},
title = {{Механизмы кратковременной памяти в целенаправленном поведении нейросетевых агентов}},
volume = {8},
year = {2013}
}
@article{Langley2009a,
abstract = {In this paper, we review Icarus, a cognitive architecture that utilizes hierarchical skills and concepts for reactive execution in physical environments. In addition, we present two extensions to the framework. The first involves the incorporation of means-ends analysis, which lets the system compose known skills to solve novel problems. The second involves the storage of new skills that are based on successful means-ends traces. We report experimental studies of these mechanisms on three distinct domains. Our results suggest that the two methods interact to acquire useful skill hierarchies that generalize well and that reduce the effort required to handle new tasks. We conclude with a discussion of related work on learning and prospects for additional research, including extending the framework to cover developmental phenomena. ?? 2008 Elsevier B.V. All rights reserved.},
author = {Langley, Pat and Choi, Dongkyu and Rogers, Seth},
doi = {10.1016/j.cogsys.2008.07.003},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive Systems Research/2009/Langley, Choi, Rogers - 2009.pdf:pdf},
isbn = {1389-0417},
issn = {13890417},
journal = {Cognitive Systems Research},
keywords = {Cognitive architecture,Hierarchical skills,Incremental learning,Problem solving,Reactive control,cog{\_}arch,icarus},
mendeley-tags = {cog{\_}arch,icarus},
number = {4},
pages = {316--332},
title = {{Acquisition of hierarchical reactive skills in a unified cognitive architecture}},
volume = {10},
year = {2009}
}
@article{Sandamirskaya2013,
abstract = {Dynamic Field Theory (DFT) is an established framework for modeling embodied cognition. In DFT, elementary cognitive functions such as memory formation, formation of grounded representations, attentional processes, decision making, adaptation, and learning emerge from neuronal dynamics. The basic computational element of this framework is a Dynamic Neural Field (DNF). Under constraints on the time-scale of the dynamics, the DNF is computationally equivalent to a soft winner-take-all (WTA) network, which is considered one of the basic computational units in neuronal processing. Recently, it has been shown how a WTA network may be implemented in neuromorphic hardware, such as analog Very Large Scale Integration (VLSI) device. This paper leverages the relationship between DFT and soft WTA networks to systematically revise and integrate established DFT mechanisms that have previously been spread among different architectures. In addition, I also identify some novel computational and architectural mechanisms of DFT which may be implemented in neuromorphic VLSI devices using WTA networks as an intermediate computational layer. These specific mechanisms include the stabilization of working memory, the coupling of sensory systems to motor dynamics, intentionality, and autonomous learning. I further demonstrate how all these elements may be integrated into a unified architecture to generate behavior and autonomous learning.},
author = {Sandamirskaya, Yulia},
doi = {10.3389/fnins.2013.00276},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Frontiers in neuroscience/2014/Sandamirskaya - 2014.pdf:pdf},
issn = {1662-4548},
journal = {Frontiers in neuroscience},
keywords = {Dynamic Neural Fields,autonomous learning,cognitive neuromorphic architecture,neural dynamics,soft winner take all},
pages = {276},
pmid = {24478620},
title = {{Dynamic neural fields as a step toward cognitive neuromorphic architectures}},
url = {http://journal.frontiersin.org/Journal/10.3389/fnins.2013.00276/abstract{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC3898057/},
volume = {7},
year = {2014}
}
@article{Zhang2015,
author = {Zhang, Yunfeng and Paik, Jaehyon and Pirolli, Peter},
doi = {10.1111/tops.12143},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Topics in Cognitive science/2015/Zhang, Paik, Pirolli - 2015.pdf:pdf},
journal = {Topics in Cognitive science},
keywords = {change detection,cognitive,counterfactual reasoning,modeling,optimal foraging,reinforcement learning},
number = {2},
pages = {368--381},
title = {{Reinforcement Learning and Counterfactual Reasoning Explain Adaptive Behavior in a Changing Environment}},
volume = {7},
year = {2015}
}
@article{Pezzulo2014,
abstract = {A network of brain structures including hippocampus (HC), prefrontal cortex, and striatum controls goal-directed behavior and decision making. However, the neural mechanisms underlying these functions are unknown. Here, we review the role of 'internally generated sequences': structured, multi-neuron firing patterns in the network that are not confined to signaling the current state or location of an agent, but are generated on the basis of internal brain dynamics. Neurophysiological studies suggest that such sequences fulfill functions in memory consolidation, augmentation of representations, internal simulation, and recombination of acquired information. Using computational modeling, we propose that internally generated sequences may be productively considered a component of goal-directed decision systems, implementing a sampling-based inference engine that optimizes goal acquisition at multiple timescales of on-line choice, action control, and learning. {\textcopyright} 2014 Elsevier Ltd. All rights reserved.},
author = {Pezzulo, Giovanni and van der Meer, Matthijs a a and Lansink, Carien S. and Pennartz, Cyriel M a},
doi = {10.1016/j.tics.2014.06.011},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Trends in Cognitive Sciences/2014/Pezzulo et al. - 2014.pdf:pdf},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
keywords = {decision making,forward sweep,generative models,hippocampus,inference,prospection,reinforcement learning,replay,spatial navigation,theta rhythm,ventral striatum},
number = {12},
pages = {647--657},
pmid = {25156191},
publisher = {Elsevier Ltd},
title = {{Internally generated sequences in learning and executing goal-directed behavior}},
url = {http://dx.doi.org/10.1016/j.tics.2014.06.011},
volume = {18},
year = {2014}
}
@article{Vartanov2011,
author = {Вартанов, А. В.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Нейрокомпьютеры разработка, применение/2011/Вартанов - 2011.pdf:pdf},
journal = {Нейрокомпьютеры: разработка, применение},
keywords = {coding,consciousness,meaning,semantics,sign},
language = {russian},
number = {12},
pages = {54--64},
title = {{Механизмы семантики: человек - нейрон - модель}},
year = {2011}
}
@article{Yamada2015,
author = {Yamada, Tatsuro and Murata, Shingo and Arie, Hiroaki and Ogata, Tetsuya},
doi = {10.3389/fnbot.2016.00005},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Frontiers in Neurorobotics/2016/Yamada et al. - 2016.pdf:pdf},
isbn = {978-1-4799-9994-1},
issn = {1662-5218},
journal = {Frontiers in Neurorobotics},
keywords = {dynamical system approach,human,language learning,recurrent neural networks,robot interaction,sequence to sequence learning,symbol grounding problem},
month = {jul},
pages = {4179--4184},
publisher = {IEEE},
title = {{Dynamical Integration of Language and Behavior in a Recurrent Neural Network for Human–Robot Interaction}},
url = {http://ieeexplore.ieee.org/document/7353968/ http://journal.frontiersin.org/Article/10.3389/fnbot.2016.00005/abstract},
volume = {10},
year = {2016}
}
@article{Plebe2016a,
abstract = {Neural computation has an influential role in the study of human capacities and behaviors. It has been the dominant approach in the vision science of the last half century, and it is currently one of the fundamental methods of investigation for most higher cognitive func- tions. Yet, neurocomputational approaches to moral behavior are lacking. Computational modeling in general has been scarcely pursued in morality, and existent non-neural attempts have failed to account for the mental processes involved in morality. In this paper we argue that recently the situation has evolved in a way that subverted the insufficient knowledge on the basic organization of moral cognition in brain circuits, making the project of modeling morality in neurocomputational terms feasible. We will present an original architecture that combines reinforcement learning and Hebbian learning, aimed at simulating forms of moral behavior in a simple artificial context. The relationship between language and morality is controversial. In the analytic tradition of philosophy, morality is essentially the lan- guage of morals. On the other side, current cognitive ethology has shown how non human species display behaviors that are surprisingly similar to those prescribed by human ethics. Nevertheless, morality in humans is deeply entrenched with language, and the semantics of words like ‘wrong' resists consensual explanations. The model here proposed includes an auditory processing pathway, with the purpose of showing how the coding of ‘‘wrong”, even if highly simplified with respect to its rich content in natural language, can emerge in the course of moral learning.},
author = {Plebe, Alessio},
doi = {10.1016/j.cogsys.2015.12.012},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive Systems Research/2016/Plebe - 2016.pdf:pdf},
issn = {13890417},
journal = {Cognitive Systems Research},
keywords = {amygdala,moral cognition,neural computation,orbitofrontal cortex,self-organization},
pages = {4--14},
title = {{What is ‘wrong' in a neural model}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1389041716000085},
volume = {39},
year = {2016}
}
@article{Botvinick2012,
abstract = {The hierarchical structure of human and animal behavior has been of critical interest in neuroscience for many years. Yet understanding the neural processes that give rise to such structure remains an open challenge. In recent research, a new perspective on hierarchical behavior has begun to take shape, inspired by ideas from machine learning, and in particular the framework of hierarchical reinforcement learning. Hierarchical reinforcement learning builds on traditional reinforcement learning mechanisms, extending them to accommodate temporally extended behaviors or subroutines. The resulting computational paradigm has begun to influence both theoretical and empirical work in neuroscience, conceptually aligning the study of hierarchical behavior with research on other aspects of learning and decision making, and giving rise to some thought-provoking new findings. ?? 2012.},
author = {Botvinick, Matthew Michael},
doi = {10.1016/j.conb.2012.05.008},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Current Opinion in Neurobiology/2012/Botvinick - 2012.pdf:pdf},
isbn = {0818653302},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
number = {6},
pages = {956--962},
pmid = {22695048},
publisher = {Elsevier Ltd},
title = {{Hierarchical reinforcement learning and decision making}},
url = {http://dx.doi.org/10.1016/j.conb.2012.05.008},
volume = {22},
year = {2012}
}
@article{Takac2015,
author = {Takac, Martin and Knott, Alistair and Knott, Alistair},
doi = {10.1007/s12559-015-9330-3},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive Computation/2015/Takac, Knott, Knott - 2015.pdf:pdf},
issn = {1866-9964},
journal = {Cognitive Computation},
keywords = {Action preparation,Language processing,Neural network modelling,Sequence learning,Working memory,modelling {\'{a}} sequence learning,network,working memory {\'{a}} neural,{\'{a}} action preparation {\'{a}}},
pages = {1--17},
publisher = {Springer US},
title = {{A Neural Network Model of Episode Representations in Working Memory}},
year = {2015}
}
@article{Sun2006,
abstract = {This paper describes how meta-cognitive processes (i.e., the self monitoring and regulating of cognitive processes) may be captured within a cognitive architecture Clarion. Some currently popular cognitive architectures lack sufficiently complex built-in meta-cognitive mechanisms. However, a sufficiently complex meta-cognitive mechanism is important, in that it is an essential part of cognition and without it, human cognition may not function properly. We contend that such a meta-cognitive mechanism should be an integral part of a cognitive architecture. Thus, such a mechanism has been developed within the Clarion cognitive architecture. The paper demonstrates how human data of two meta-cognitive experiments are simulated using Clarion. The simulations show that the meta-cognitive processes represented by the experimental data (and beyond) can be adequately captured within the Clarion framework. ?? 2005 Elsevier B.V. All rights reserved.},
author = {Sun, Ron and Zhang, Xi and Mathews, Robert},
doi = {10.1016/j.cogsys.2005.09.001},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive Systems Research/2006/Sun, Zhang, Mathews - 2006.pdf:pdf},
issn = {13890417},
journal = {Cognitive Systems Research},
keywords = {Cognitive modeling,Metacognition,Neural networks},
number = {4},
pages = {327--338},
title = {{Modeling meta-cognition in a cognitive architecture}},
volume = {7},
year = {2006}
}
@inproceedings{Menager2016,
address = {Austin},
author = {Menager, David Henri and Choi, Dongkyu},
booktitle = {Proceedings of the 38th Annual Conference of the Cognitive Science Society},
editor = {Papafragou, A. and Grodner, D. and Mirman, D. and Trueswell, J.C.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceedings of the 38th Annual Conference of the Cognitive Science Society/2016/Menager, Choi - 2016.pdf:pdf},
keywords = {cognitive architectures,episodic memory,expectations,impasse resolution,sensing,virtual},
pages = {620--625},
publisher = {Cognitive Science Society},
title = {{A Robust Implementation of Episodic Memory for a Cognitive Architecture}},
year = {2016}
}
@article{Rasmussen2011,
abstract = {Abstract Inductive reasoning is a fundamental and complex aspect of human intelligence. In particular, how do subjects, given a set of particular examples, generate general descriptions of the rules governing that set? We present a biologically plausible method ... $\backslash$n},
author = {Rasmussen, Daniel and Eliasmith, Chris},
doi = {10.1111/j.1756-8765.2010.01127.x},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Topics in Cognitive Science/2011/Rasmussen, Eliasmith - 2011.pdf:pdf},
issn = {17568757},
journal = {Topics in Cognitive Science},
keywords = {Cognitive modeling,Fluid intelligence,Inductive reasoning,Neural engineering framework,Raven's progressive matrices,Realistic neural modeling,Rule generation,Vector symbolic architectures},
number = {1},
pages = {140--153},
title = {{A neural model of rule generation in inductive reasoning}},
volume = {3},
year = {2011}
}
@article{Botvinick2009a,
abstract = {Research on human and animal behavior has long emphasized its hierarchical structure-the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior.},
author = {Botvinick, Matthew M. and Niv, Yael and Barto, Andrew C.},
doi = {10.1016/j.cognition.2008.08.011},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognition/2009/Botvinick, Niv, Barto - 2009.pdf:pdf},
issn = {1873-7838},
journal = {Cognition},
keywords = {Animals,Humans,Models,Nerve Net,Nerve Net: physiology,Prefrontal Cortex,Prefrontal Cortex: physiology,Problem Solving,Problem Solving: physiology,Psychological,Reinforcement (Psychology)},
number = {3},
pages = {262--80},
pmid = {18926527},
title = {{Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2783353{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {113},
year = {2009}
}
@article{Singer2009,
abstract = {The cerebral cortex presents itself as a distributed dynamical system with the characteristics of a small world network. The neuronal correlates of cognitive and executive processes often appear to consist of the coordinated activity of large assemblies of widely distributed neurons. These features require mechanisms for the selective routing of signals across densely interconnected networks, the flexible and context dependent binding of neuronal groups into functionally coherent assemblies and the task and attention dependent integration of subsystems. In order to implement these mechanisms, it is proposed that neuronal responses should convey two orthogonal messages in parallel. They should indicate (1) the presence of the feature to which they are tuned and (2) with which other neurons (specific target cells or members of a coherent assembly) they are communicating. The first message is encoded in the discharge frequency of the neurons (rate code) and it is proposed that the second message is contained in the precise timing relationships between individual spikes of distributed neurons (temporal code). It is further proposed that these precise timing relations are established either by the timing of external events (stimulus locking) or by internal timing mechanisms. The latter are assumed to consist of an oscillatory modulation of neuronal responses in different frequency bands that cover a broad frequency range from {\textless}2 Hz (delta) to {\textgreater}40 Hz (gamma) and ripples. These oscillations limit the communication of cells to short temporal windows whereby the duration of these windows decreases with oscillation frequency. Thus, by varying the phase relationship between oscillating groups, networks of functionally cooperating neurons can be flexibly configurated within hard wired networks. Moreover, by synchronizing the spikes emitted by neuronal populations, the saliency of their responses can be enhanced due to the coincidence sensitivity of receiving neurons in very much the same way as can be achieved by increasing the discharge rate. Experimental evidence will be reviewed in support of the coexistence of rate and temporal codes. Evidence will also be provided that disturbances of temporal coding mechanisms are likely to be one of the pathophysiological mechanisms in schizophrenia.},
author = {Singer, Wolf},
doi = {10.1007/s11571-009-9087-z},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive Neurodynamics/2009/Singer - 2009.pdf:pdf},
isbn = {1871-4080 (Print)},
issn = {1871-4080},
journal = {Cognitive Neurodynamics},
keywords = {Attention,Cerebral cortex,Feature binding,Gamma frequency,Neuronal coding,Oscillations,Response selection,Synchrony,Temporal codes},
month = {sep},
number = {3},
pages = {189--196},
pmid = {19562517},
title = {{Distributed processing and temporal codes in neuronal networks}},
url = {http://link.springer.com/10.1007/s11571-009-9087-z},
volume = {3},
year = {2009}
}
@inproceedings{Kachergis2016,
address = {Austin},
author = {Kachergis, George and Berends, Floris and de Kleijn, Roy and Hommel, Bernhard},
booktitle = {Proceedings of the 38th Annual Conference of the Cognitive Science Society},
editor = {Papafragou, A. and Grodner, D. and Mirman, D. and Trueswell, J.C.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceedings of the 38th Annual Conference of the Cognitive Science Society/2016/Kachergis et al. - 2016.pdf:pdf},
keywords = {movement trajectory,quential action,reinforcement learning,se-,sequence learning,serial reaction time task},
pages = {193--198},
publisher = {Cognitive Science Society},
title = {{Human Reinforcement Learning of Sequential Action}},
year = {2016}
}
@article{Caro2015,
abstract = {Metacognition has been used in artificial intelligence to increase the level of autonomy of intelligent systems. However the design of systems with metacognitive capabilities is a difficult task due to the number and complexity of processes involved. This paper presents a domain-specific visual language specifically developed for modeling metacognition in intelligent systems called M++. In M++ the specifications of the cognitive level (object-level) and metacognitive level (meta-level) are supported in a metamodel configured according to the standard Meta-Object Facility (MOF) of Model-Driven Architecture (MDA) methodology. M++ allows the generation of metacognitive diagrams in a visual editor named MetaThink. A validation process was conducted to ensure the reliability of M++ in terms of quality of the notation and consistency of generated models. The validation was performed using two techniques: (i) empirical study and (ii) model tracing. The results given in the experimental study demonstrate that M++ is a useful notation for the process of modeling metacognitive components in intelligent systems. Metacognitive models generated from the validation process using the Tracing technique were consistent with the MOF-based metamodel. M++ contribute to cognitive architecture research adding precision to metacognitive concepts and enabling cognitive architecture researchers to do fast and exploratory prototyping of metacognitive systems using MetaThink tool.},
author = {Caro, Manuel F. and Josyula, Darsana P. and Jim{\'{e}}nez, Jovani A. and Kennedy, Catriona M. and Cox, Michael T.},
doi = {10.1016/j.bica.2015.06.004},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Biologically Inspired Cognitive Architectures/2015/Caro et al. - 2015.pdf:pdf},
issn = {2212683X},
journal = {Biologically Inspired Cognitive Architectures},
keywords = {Domain-specific visual language,Intelligent system,MOF,Metacognition,Modeling tool},
pages = {75--90},
title = {{A domain-specific visual language for modeling metacognition in intelligent systems}},
volume = {13},
year = {2015}
}
@article{Francisco2011,
author = {Francisco, San},
doi = {10.1007/978-4-431-54595-8},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive Neuroscience Robotics B/2016/Shinohara - 2016.pdf:pdf},
isbn = {978-4-431-54594-1},
keywords = {attention,attentional resources,cocktail party phenomenon,coherence theory,divided attention,feature integration theory,multitasking,orientation,selective,skill-rule-knowledge based model,spotlight,srk model,useful field of view,visual search,working memory},
pages = {1--22},
title = {{Cognitive Neuroscience Robotics}},
year = {2011}
}
@article{Llinas1998a,
author = {Llinas, R. and Ribary, U. and Contreras, D. and Pedroarena, C.},
journal = {Philosophical transactions of the Royal Society of London. Series B. Biological sciences},
number = {353},
pages = {1841--1849},
title = {{The neuronal basis for consciousness}},
year = {1998}
}
@article{Minami2001,
author = {Minami, T. and Inui, T.},
doi = {10.1007/s12559-015-9330-3},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Connectionist Models of Neurons, Learning Processes, and Artificial Intelligence/2001/Minami, Inui - 2001.pdf:pdf},
issn = {18669964},
journal = {Connectionist Models of Neurons, Learning Processes, and Artificial Intelligence},
keywords = {Action preparation,Language processing,Neural network modelling,Sequence learning,Working memory,modelling {\'{a}} sequence learning,network,working memory {\'{a}} neural,{\'{a}} action preparation {\'{a}}},
pages = {126--133},
publisher = {Springer US},
title = {{A Neural Network Model of Working Memory}},
url = {http://www.springerlink.com/index/NW4NLH1ME9PJ8JQY.pdf},
year = {2001}
}
@article{Donnarumma2015,
abstract = {Distributed and hierarchical models of control are nowadays popular in computational modeling and robotics. In the artificial neural network literature, complex behaviors can be produced by composing elementary building blocks or motor primitives, possibly organized in a layered structure. However, it is still unknown how the brain learns and encodes multiple motor primitives, and how it rapidly reassembles, sequences and switches them by exerting cognitive control. In this paper we advance a novel proposal, a hierarchical programmable neural network architecture, based on the notion of programmability and an interpreter-programmer computational scheme. In this approach, complex (and novel) behaviors can be acquired by embedding multiple modules (motor primitives) in a single, multi-purpose neural network. This is supported by recent theories of brain functioning in which skilled behaviors can be generated by combining functional different primitives embedded in ‘‘reusable'' areas of ‘‘recycled'' neurons. Such neuronal substrate supports flexible cognitive control, too. Modules are seen as interpreters of behaviors having controlling input parameters, or programs that encode structures of networks to be interpreted. Flexible cognitive control can be exerted by a programmer module feeding the interpreters with appropriate input parameters, without modifying connectivity. Our results in a multiple T-maze robotic scenario show how this computational framework provides a robust, scalable and flexible scheme that can be iterated at different hierarchical layers permitting to learn, encode and control multiple qualitatively different behaviors.},
author = {Donnarumma, F. and Prevete, R. and de Giorgio, A. and Montone, G. and Pezzulo, G.},
doi = {10.1177/1059712315609412},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Adaptive Behavior/2015/Donnarumma et al. - 2015.pdf:pdf},
issn = {1059-7123},
journal = {Adaptive Behavior},
keywords = {cognitive control,distributed representation,hierarchical organization,neuronal reuse,programming neural networks},
pages = {(In press)},
title = {{Learning programs is better than learning dynamics: A programmable neural network hierarchical architecture in a multi-task scenario}},
url = {http://adb.sagepub.com/cgi/doi/10.1177/1059712315609412},
year = {2015}
}
@incollection{Taylor,
author = {Taylor, Neill R. and Panchev, Christo and Hartley, Matthew and Kasderidis, Stathis and Taylor, John G.},
booktitle = {Artificial Neural Networks - ICANN 2006},
editor = {Kollias, Stefanos D. and Stafylopatis, Andreas and Duch, W{\l}odzis{\l}aw and Oja, Erkki},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Artificial Neural Networks - ICANN 2006/2006/Taylor et al. - 2006.pdf:pdf},
pages = {592--601},
publisher = {Springer-Verlag},
title = {{Occlusion , Attention and Object Representations}},
year = {2006}
}
@article{Deco2015,
abstract = {The brain regulates information flow by balancing the segregation and integration of incoming stimuli to facilitate flexible cognition and behaviour. The topological features of brain networks - in particular, network communities and hubs - support this segregation and integration but do not provide information about how external inputs are processed dynamically (that is, over time). Experiments in which the consequences of selective inputs on brain activity are controlled and traced with great precision could provide such information. However, such strategies have thus far had limited success. By contrast, recent whole-brain computational modelling approaches have enabled us to start assessing the effect of input perturbations on brain dynamics in silico.},
author = {Deco, Gustavo and Tononi, Giulio and Boly, Melanie and Kringelbach, Morten L},
doi = {10.1038/nrn3963},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Nature reviews. Neuroscience/2015/Deco et al. - 2015.pdf:pdf},
isbn = {1471-0048 (Electronic)$\backslash$r1471-003X (Linking)},
issn = {1471-0048},
journal = {Nature reviews. Neuroscience},
number = {7},
pages = {430--439},
pmid = {26081790},
publisher = {Nature Publishing Group},
title = {{Rethinking segregation and integration: contributions of whole-brain modelling}},
url = {http://www.nature.com.sire.ub.edu/nrn/journal/v16/n7/full/nrn3963.html},
volume = {16},
year = {2015}
}
@article{Chernavsky2012c,
abstract = {Рассматривается одна из возможных схем нейропроцессорной конструкции, способной решать задачи, традиционно относимые к мышлению и творчеству. Выделена подсистема, обрабатывающая образную информацию; ее важная составляющая — ―размытое множество‖, содержащее всю образную информацию, доступную системе. Выделена подсистема, способная решать логические задачи. Подсистема распознавания процесса и построения прогноза позволяет ввести понятие континуального времени. Показано, что решение творческих задач (при недостатке информации или противоречивости алгоритмов) в символьной подсистеме невозможно и требует обращения к размытому (образному) множеству.},
author = {Чернавский, Д. С. and Карп, В. П. and Никитин, А. П. and Рожило, Я. А. and Чернавская, О. Д.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Сложны/2012/Чернавский et al. - 2012.pdf:pdf},
journal = {Сложны},
keywords = {мышление,научное творчество,нейропроцессор,самоорганизация,символьная система},
language = {russian},
number = {4},
pages = {25--37},
title = {{Процесс мышления в контексте динамической теории информации. Часть III: один из вариантов конструкции нейропроцессоров для моделирования процесса мышления}},
volume = {3},
year = {2012}
}
@article{Hasselmo2006,
author = {Howard, Marc W. and Fotedar, Mrigankka S. and Datey, Aditya V. and Hasselmo, Michael E.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Psychology Review/2005/Howard et al. - 2005.pdf:pdf},
journal = {Psychology Review},
number = {1},
pages = {75--116},
title = {{The Temporal Context Model in spatial navigation and relational learning: Toward a common explanation of medial temporal lobe function across domains}},
volume = {112},
year = {2005}
}
@article{Battaglia2011,
abstract = {After acquisition, memories underlie a process of consolidation, making them more resistant to interference and brain injury. Memory consolidation involves systems-level interactions, most importantly between the hippocampus and associated structures, which takes part in the initial encoding of memory, and the neocortex, which supports long-term storage. This dichotomy parallels the contrast between episodic memory (tied to the hippocampal formation), collecting an autobiographical stream of experiences, and semantic memory, a repertoire of facts and statistical regularities about the world, involving the neocortex at large. Experimental evidence points to a gradual transformation of memories, following encoding, from an episodic to a semantic character. This may require an exchange of information between different memory modules during inactive periods. We propose a theory for such interactions and for the formation of semantic memory, in which episodic memory is encoded as relational data. Semantic memory is modeled as a modified stochastic grammar, which learns to parse episodic configurations expressed as an association matrix. The grammar produces tree-like representations of episodes, describing the relationships between its main constituents at multiple levels of categorization, based on its current knowledge of world regularities. These regularities are learned by the grammar from episodic memory information, through an expectation-maximization procedure, analogous to the inside-outside algorithm for stochastic context-free grammars. We propose that a Monte-Carlo sampling version of this algorithm can be mapped on the dynamics of "sleep replay" of previously acquired information in the hippocampus and neocortex. We propose that the model can reproduce several properties of semantic memory such as decontextualization, top-down processing, and creation of schemata.},
author = {Battaglia, Francesco P. and Pennartz, Cyriel M. A.},
doi = {10.3389/fncom.2011.00036},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Frontiers in computational neuroscience/2011/Battaglia, Pennartz - 2011.pdf:pdf},
issn = {1662-5188},
journal = {Frontiers in computational neuroscience},
keywords = {episodic memory,memory consolidation,sleep r,sleep replay,stochastic grammars},
number = {August},
pages = {36},
pmid = {21887143},
title = {{The construction of semantic memory: grammar-based representations learned from relational episodic information}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3157741{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {5},
year = {2011}
}
@article{Raghubir2006,
abstract = {This paper examines centrality of physical position as a cue that leads to systematic biases in people' decisions to retain or eliminate a participant from a group. Termed the "center-stage" effect, we argue that people use their belief that "important people sit in the middle" as a schematic cue that they substitute for individuating performance information for individuals who occupy central positions when the goal is to eliminate all but one of the group members. This leads to the errors of those in center-positions being overlooked: or making them the "centers-of-inattention." Study 1 examines people's lay beliefs regarding positions using two stylized placement tasks (a group interview and classroom seating scenarios). These suggest that people believe that more attention is paid to those in the center than those on the extremes. Study 2 tests the center-stage effect using observational data from a real television show, The Weakest Link. Results show that players assigned at random to central positions are more likely to win the game than those in extreme positions. Study 3, a laboratory experiment manipulating attention paid to the game shows that observers overlook the errors of players in the center to a greater extent than the errors of players in extreme positions. Study 4 replicates the game in the laboratory with direct process measures to show that players playing the game make the same error. Study 5 shows that in a stylized group interview setting, participants who believe that "important people sit in the middle" find the performance of candidates in the extreme position easier to recall than the performance of those in the central position, and are more likely to choose them. Study 6 shows that the "center-stage" effects are weaker when the end-game rule allows for two (vs one) contestants to be retained. Overall results converge to show that the use of the "center-stage" heuristic substitutes for the effortful processing of individuating information, leading to a biased (favorable) assessment of people in the center. Implications for decision-making are discussed. ?? 2005 Elsevier Inc. All rights reserved.},
author = {Raghubir, Priya and Valenzuela, Ana},
doi = {10.1016/j.obhdp.2005.06.001},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Organizational Behavior and Human Decision Processes/2006/Raghubir, Valenzuela - 2006.pdf:pdf},
isbn = {07495978},
issn = {07495978},
journal = {Organizational Behavior and Human Decision Processes},
keywords = {Perceptual biases,Performance appraisal,Salience effects,Visual information processing},
number = {1},
pages = {66--80},
title = {{Center-of-inattention: Position biases in decision-making}},
volume = {99},
year = {2006}
}
@article{Itti2001,
abstract = {Five important trends have emerged from recent work on computational models of focal visual attention that emphasize the bottom-up, image-based control of attentional deployment. First, the perceptual saliency of stimuli critically depends on the surrounding context. Second, a unique 'saliency map' that topographically encodes for stimulus conspicuity over the visual scene has proved to be an efficient and plausible bottom-up control strategy. Third, inhibition of return, the process by which the currently attended location is prevented from being attended again, is a crucial element of attentional deployment. Fourth, attention and eye movements tightly interplay, posing computational challenges with respect to the coordinate system used to control attention. And last, scene understanding and object recognition strongly constrain the selection of attended locations. Insights from these five key areas provide a framework for a computational and neurobiological understanding of visual attention.},
author = {Itti, L and Koch, C},
doi = {10.1038/35058500},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Nature reviews. Neuroscience/2001/Itti, Koch - 2001.pdf:pdf},
issn = {1471-003X},
journal = {Nature reviews. Neuroscience},
keywords = {Animals,Attention,Attention: physiology,Computer Simulation,Humans,Models,Neurological,Neurons,Neurons: metabolism,Visual Cortex,Visual Cortex: physiology,Visual Perception,Visual Perception: physiology},
number = {3},
pages = {194--203},
pmid = {11256080},
title = {{Computational modelling of visual attention}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11256080},
volume = {2},
year = {2001}
}
@article{Liu2014,
author = {Liu, Zhi and Zou, Wenbin and Li, Lina and Shen, Liquan and {Le Meur}, Olivier},
doi = {10.1109/LSP.2013.2292873},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/IEEE Signal Processing Letters/2014/Liu et al. - 2014.pdf:pdf},
issn = {1070-9908},
journal = {IEEE Signal Processing Letters},
number = {1},
pages = {88--92},
title = {{Co-Saliency Detection Based on Hierarchical Segmentation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6675796},
volume = {21},
year = {2014}
}
@article{Zendehrouh2015,
author = {Zendehrouh, Sareh},
doi = {10.1016/j.neunet.2015.08.006},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neural Networks/2015/Zendehrouh - 2015.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
pages = {112--123},
publisher = {Elsevier Ltd},
title = {{A new computational account of cognitive control over reinforcement-based decision-making: Modeling of a probabilistic learning task}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608015001604},
volume = {71},
year = {2015}
}
@article{Ring2011,
abstract = {This paper addresses the problem of continual learning [1] in a new way, combining multi-modular reinforcement learning with inspiration from the motor cortex to produce a unique perspective on hierarchical behavior. Most reinforcement-learning agents represent policies monolithically using a single table or function approximator. In those cases where the policies are split among a few different modules, these modules are related to each other only in that they work together to produce the agent's overall policy. In contrast, the brain appears to organize motor behavior in a two-dimensional map, where nearby locations represent similar behaviors. This representation allows the brain to build hierarchies of motor behavior that correspond not to hierarchies of subroutines but to regions of the map such that larger regions correspond to more general behaviors. Inspired by the benefits of the brain's representation, the system presented here is a first step and the first attempt toward the two-dimensional organization of learned policies according to behavioral similarity. We demonstrate a fully autonomous multi-modular system designed for the constant accumulation of ever more sophisticated skills (the continual-learning problem). The system can split up a complex task among a large number of simple modules such that nearby modules correspond to similar policies. The eventual goal is to develop and use the resulting organization hierarchically, accessing behaviors by their location and extent in the map.},
author = {Ring, Mark and Schaul, Tom and Schmidhuber, Juergen},
doi = {10.1109/DEVLRN.2011.6037326},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/2011 IEEE International Conference on Development and Learning, ICDL 2011/2011/Ring, Schaul, Schmidhuber - 2011.pdf:pdf},
isbn = {9781612849904},
issn = {2161-9476},
journal = {2011 IEEE International Conference on Development and Learning, ICDL 2011},
pages = {1--8},
title = {{The two-dimensional organization of behavior}},
volume = {2},
year = {2011}
}
@article{Dong2011a,
abstract = {We present a new model of sensorimotor learning in a systems-level cognitive model, LIDA. Sensorimotor learning helps an agent properly interact with its environment using past experi- ences. This new model stores and updates the rewards of pairs of data, motor commands and their contexts, using the concept of reinforcement learning; thus the agent is able to generate (output) effective commands in certain contexts based on its reward history. Following Global Workspace Theory, the primary basis of LIDA, the process of updating rewards in sensorimotor learning is cued by the agent's conscious content, the most salient portion of the agent's under- standing of the current situation, issued by the Global Workspace module of LIDA. Furthermore, we add a dynamic learning rate to control the extent to which a newly arriving reward may affect the reward update. This learning rate control mechanism is inspired by a hypothesis from neuroscience regarding memory of errors. Our experimental results show that sensorimotor learning using a dynamic learning rate improves performance in a simulated movement of push- ing a box.},
author = {Dong, Daqi and Franklin, Stan},
doi = {10.1016/j.bica.2015.09.005},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Biologically Inspired Cognitive Architectures/2015/Dong, Franklin - 2015.pdf:pdf},
issn = {2212683X},
journal = {Biologically Inspired Cognitive Architectures},
keywords = {a learning rate control,action,cognitive modeling,execution,in lida and add,in this paper,learning rate,lida 1,lida model,mechanism,sensorimotor learning,this is,we implement sensorimotor learning},
pages = {1--9},
publisher = {Elsevier B.V.},
title = {{Modeling Sensorimotor Learning in LIDA using a Dynamic Learning Rate}},
volume = {14},
year = {2015}
}
@article{Frintrop2010,
author = {Frintrop, Simone and Rome, Erich and Christensen, Henrik I.},
doi = {10.1145/1658349.1658355},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/ACM Transactions on Applied Perception/2010/Frintrop, Rome, Christensen - 2010.pdf:pdf},
issn = {15443558},
journal = {ACM Transactions on Applied Perception},
number = {1},
pages = {1--39},
title = {{Computational visual attention systems and their cognitive foundations}},
url = {http://portal.acm.org/citation.cfm?doid=1658349.1658355},
volume = {7},
year = {2010}
}
@article{Fan2016,
abstract = {Hierarchical temporal memory (HTM) tries to mimic the computing in cerebral-neocortex. It identifies spatial and temporal patterns in the input for making inferences. This may require large number of computationally expensive tasks like, dot-product evaluations. Nano-devices that can provide direct mapping for such primitives are of great interest. In this work we show that the computing blocks for HTM can be mapped using low-voltage, fast-switching, magneto-metallic spin-neurons combined with emerging resistive cross-bar network (RCN). Results show possibility of more than 200x lower energy as compared to 45nm CMOS ASIC design},
archivePrefix = {arXiv},
arxivId = {1402.2902},
author = {Fan, Deliang and Sharad, Mrigank and Sengupta, Abhronil and Roy, Kaushik},
doi = {10.1109/TNNLS.2015.2462731},
eprint = {1402.2902},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/IEEE Transactions on Neural Networks and Learning Systems/2016/Fan et al. - 2016.pdf:pdf},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Hierarchical temporal memory (HTM),magnetic domain walls (DWs),memristors,neural network hardware,spin Hall effect (SHE),spin transfer torque},
number = {9},
pages = {1907--1919},
title = {{Hierarchical Temporal Memory Based on Spin-Neurons and Resistive Memory for Energy-Efficient Brain-Inspired Computing}},
volume = {27},
year = {2016}
}
@article{Baars2005a,
author = {Baars, Bernard J.},
journal = {Progress in Brain Research},
pages = {45--53},
title = {{Global workspace theory of consciousness: toward a cognitive neuroscience of human experience}},
volume = {150},
year = {2005}
}
@article{Griffiths2010,
abstract = {Cognitive science aims to reverse-engineer the mind, and many of the engineering challenges the mind faces involve induction. The probabilistic approach to modeling cognition begins by identifying ideal solutions to these inductive problems. Mental processes are then modeled using algorithms for approximating these solutions, and neural processes are viewed as mechanisms for implementing these algorithms, with the result being a top-down analysis of cognition starting with the function of cognitive processes. Typical connectionist models, by contrast, follow a bottom-up approach, beginning with a characterization of neural mechanisms and exploring what macro-level functional phenomena might emerge. We argue that the top-down approach yields greater flexibility for exploring the representations and inductive biases that underlie human cognition.},
author = {Griffiths, Thomas L and Chater, Nick and Kemp, Charles and Perfors, Amy and Tenenbaum, Joshua B},
doi = {10.1016/j.tics.2010.05.004},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Trends in cognitive sciences/2010/Griffiths et al. - 2010.pdf:pdf},
issn = {1879-307X},
journal = {Trends in cognitive sciences},
keywords = {Bias (Epidemiology),Brain,Brain: physiology,Cognition,Cognition: physiology,Humans,Models,Predictive Value of Tests,Probability,Psychological},
number = {8},
pages = {357--64},
pmid = {20576465},
publisher = {Elsevier Ltd},
title = {{Probabilistic models of cognition: exploring representations and inductive biases}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20576465},
volume = {14},
year = {2010}
}
@article{Ivanitsky1997,
author = {Ivanitsky, A. M.},
journal = {Neuroscience and Behavioral Physiology},
number = {4},
pages = {414--426},
title = {{Information synthesis in key parts of the cerebral cortex as the basis of subjective experience}},
volume = {27},
year = {1997}
}
@book{Kober2014,
address = {Cham},
author = {Kober, Jens and Peters, Jan},
doi = {10.1007/978-3-319-03194-1},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2014/Kober, Peters - 2014.pdf:pdf},
isbn = {978-3-319-03193-4},
publisher = {Springer International Publishing},
series = {Springer Tracts in Advanced Robotics},
title = {{Learning Motor Skills}},
url = {http://link.springer.com/10.1007/978-3-319-03194-1},
volume = {97},
year = {2014}
}
@inproceedings{Rasmussen1998,
author = {Rasmussen, Daniel and Eliasmith, Chris},
booktitle = {Proceedings of the 36th Annual Conference of the Cognitive Science Society},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceedings of the 36th Annual Conference of the Cognitive Science Society/2014/Rasmussen, Eliasmith - 2014.pdf:pdf},
keywords = {brain could achieve its,hierarchical,hrl into a theory,neural engineering framework,neural model,of neural function,providing a hypothe-,reinforcement learning,sis for how the,strengths in scaling},
number = {1},
pages = {1252--1257},
title = {{A neural model of hierarchical reinforcement learning}},
year = {2014}
}
@article{Rawlinson2012,
abstract = {The Memory-Prediction Framework (MPF) and its Hierarchical-Temporal Memory implementation (HTM) have been widely applied to unsupervised learning problems, for both classification and prediction. To date, there has been no attempt to incorporate MPF/HTM in reinforcement learning or other adaptive systems; that is, to use knowledge embodied within the hierarchy to control a system, or to generate behaviour for an agent. This problem is interesting because the human neocortex is believed to play a vital role in the generation of behaviour, and the MPF is a model of the human neocortex.We propose some simple and biologically-plausible enhancements to the Memory-Prediction Framework. These cause it to explore and interact with an external world, while trying to maximize a continuous, time-varying reward function. All behaviour is generated and controlled within the MPF hierarchy. The hierarchy develops from a random initial configuration by interaction with the world and reinforcement learning only. Among other demonstrations, we show that a 2-node hierarchy can learn to successfully play "rocks, paper, scissors" against a predictable opponent.},
author = {Rawlinson, David and Kowadlo, Gideon},
doi = {10.1371/journal.pone.0029264},
editor = {Vasilaki, Eleni},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/PloS one/2012/Rawlinson, Kowadlo - 2012.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
keywords = {Adaptation,Algorithms,Behavior,Behavior: physiology,Humans,Learning,Learning: physiology,Memory,Memory: physiology,Models,Neocortex,Neocortex: physiology,NeuroModels,Neurological,Pattern Recognition,Psychological,Psychological: physiology,Reinforcement (Psychology),Reward,User-Computer Interface,Visual,Visual: physiology},
mendeley-tags = {NeuroModels},
number = {1},
pages = {e29264},
pmid = {22272231},
publisher = {Public Library of Science},
title = {{Generating adaptive behaviour within a memory-prediction framework}},
url = {http://dx.plos.org/10.1371/journal.pone.0029264},
volume = {7},
year = {2012}
}
@article{Sokolov2004,
author = {Соколов, Е. Н.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Психология. Журнал Высшей школы экономики/2004/Соколов - 2004.pdf:pdf},
journal = {Психология. Журнал Высшей школы экономики},
language = {russian},
number = {2},
pages = {3--15},
title = {{Нейроны сознания}},
volume = {1},
year = {2004}
}
@article{Rafferty2015a,
author = {Rafferty, Anna N. and Brunskill, Emma and Griffiths, Thomas L. and Shafto, Patrick},
doi = {10.1111/cogs.12290},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive Science/2016/Rafferty et al. - 2016.pdf:pdf},
issn = {03640213},
journal = {Cognitive Science},
keywords = {automated teaching,concept learning,partially observable markov decision,process},
month = {aug},
number = {6},
pages = {1290--1332},
title = {{Faster Teaching via POMDP Planning}},
url = {http://doi.wiley.com/10.1111/cogs.12290},
volume = {40},
year = {2016}
}
@inproceedings{Ragni2012,
author = {Ragni, Marco and Neubert, Stefanie},
booktitle = {ECAI 2012: 20h European Conference on Artificial Intelligence: Proceedings},
doi = {10.3233/978-1-61499-098-7-666},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/ECAI 2012 20h European Conference on Artificial Intelligence Proceedings/2012/Ragni, Neubert - 2012.pdf:pdf},
isbn = {9781614990987},
pages = {666--671},
title = {{Solving Raven's IQ-tests : An AI and Cognitive Modeling Approach}},
year = {2012}
}
@article{Grossberg2014,
author = {Grossberg, Stephen},
doi = {10.1016/j.brainres.2014.11.018},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Brain Research/2014/Grossberg - 2014.pdf:pdf},
issn = {0006-8993},
journal = {Brain Research},
keywords = {3D vision,Adaptive resonance theory,Adaptively controlled conditioning,Attention,Autism,Category learning,Cognitive working memory,Eye movement,Grid cell,Laminar cortical circuits,Learning,Medial temporal amnesia,Memory,NeuroModels,Place cell,Predictive remapping,Spatial navigation,Speech perception,Time cell,mGluR},
mendeley-tags = {NeuroModels},
pages = {1--24},
publisher = {Elsevier},
title = {{From brain synapses to systems for learning and memory: Object recognition, spatial navigation, timed conditioning, and movement control}},
url = {http://dx.doi.org/10.1016/j.brainres.2014.11.018},
year = {2014}
}
@article{Parkhurst2002,
abstract = {A biologically motivated computational model of bottom-up visual selective attention was used to examine the degree to which stimulus salience guides the allocation of attention. Human eye movements were recorded while participants viewed a series of digitized images of complex natural and artificial scenes. Stimulus dependence of attention, as measured by the correlation between computed stimulus salience and fixation locations, was found to be significantly greater than that expected by chance alone and furthermore was greatest for eye movements that immediately follow stimulus onset. The ability to guide attention of three modeled stimulus features (color, intensity and orientation) was examined and found to vary with image type. Additionally, the effect of the drop in visual sensitivity as a function of eccentricity on stimulus salience was examined, modeled, and shown to be an important determiner of attentional allocation. Overall, the results indicate that stimulus-driven, bottom-up mechanisms contribute significantly to attentional guidance under natural viewing conditions.},
author = {Parkhurst, Derrick and Law, Klinton and Niebur, Ernst},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Vision research/2002/Parkhurst, Law, Niebur - 2002.pdf:pdf},
issn = {0042-6989},
journal = {Vision research},
keywords = {Analysis of Variance,Attention,Attention: physiology,Biological,Computer Simulation,Eye Movements,Eye Movements: physiology,Female,Humans,Male,Models,Normal Distribution,Visual Perception,Visual Perception: physiology},
number = {1},
pages = {107--23},
pmid = {11804636},
title = {{Modeling the role of salience in the allocation of overt visual attention}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11804636},
volume = {42},
year = {2002}
}
@inproceedings{Chen2016,
address = {Austin},
author = {Chen, Stephanie Y and Bartels, Daniel M and Urminsky, Oleg},
booktitle = {Proceedings of the 38th Annual Conference of the Cognitive Science Society},
editor = {Papafragou, A. and Grodner, D. and Mirman, D. and Trueswell, J.C.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceedings of the 38th Annual Conference of the Cognitive Science Society/2016/Chen, Bartels, Urminsky - 2016.pdf:pdf},
keywords = {causal,concepts and categories,personal identity,reasoning,self-concept},
pages = {1727--1732},
publisher = {Cognitive Science Society},
title = {{Is the Self - Concept like other Concepts ? The Causal Structure of Identity}},
year = {2016}
}
@phdthesis{Rasmussen2014,
author = {Rasmussen, Daniel},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2014/Rasmussen - 2014.pdf:pdf},
pages = {175},
school = {Unversetu of Waterloo},
title = {{Hierarchical reinforcement learning in a biologically plausible neural architecture}},
year = {2014}
}
@article{Schneider1999,
abstract = {This paper addresses the issue of how visual-spatial working memory, attention, and scene representation are related. The first section introduces a modified two-stage conception of visual-spatial processing. "Stage one" refers to low-level visual-spatial processing and computes in parallel for the currently available retinal information "object candidates," here called "visual-spatial units." An attentional process called "unit selection" allows access to stage two for one of these units at a time. Stage two contains high-level visual-spatial information that can be used for goal-directions (e.g., verbal report, grasping). It consists of three parallel processing streams. First, the currently selected unit is recognized; second, a spatial-motor program for the selected unit is computed; and third, an "object file" is set up for the selected unit. An object file contains temporary episodic representations of detailed high-level visual-spatial attributes of an "object" plus an "index." An index acts as a pointer and is bound via temporary connections to the attributes of the file. Section two of this paper specifies one part of stage two in more detail, namely visual-spatial working memory (VSWM). It can contain up to four object files. A first central claim is that during sensory-based processing for working memory ("access"), one object file is always "on-line," and up to three other object files are "off-line". A second central claim is that the process of setting up an object file depends on the number and the activation level of already stored files. Based on the concept of activation-based competition between object files, it is postulated that the more files that are stored and the higher their activation is, the longer it takes for a newly set up object file to reach a sufficient level of activation. Activation-based competition is also used to explain "short-term forgetting" by "interference." A third central claim about VSWM is that a "refreshment" process exists that increases the activation level of an index of an object file in order to prevent forgetting or in order to bring the file back to the state of controlling the current action. Finally, section three gives a selective look at a number of experimental data such as the attentional blink, backward masking, dwell time effects, transsaccadic memory, and change blindness. New explanations are offered and new predictions made.},
author = {Schneider, W X},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Psychological research/1999/Schneider - 1999.pdf:pdf},
issn = {0340-0727},
journal = {Psychological research},
keywords = {Attention,Attention: physiology,Cognitive Science,Humans,Memory,Models,Perceptual Masking,Psychological,Psychophysics,Short-Term,Short-Term: physiology,Space Perception,Space Perception: physiology,Visual Perception,Visual Perception: physiology},
number = {2-3},
pages = {220--36},
pmid = {10472201},
title = {{Visual-spatial working memory, attention, and scene representation: a neuro-cognitive theory}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10472201},
volume = {62},
year = {1999}
}
@article{Subagdja2015,
author = {Subagdja, Budhitama and Tan, Ah-Hwee},
doi = {10.1016/j.neucom.2015.02.038},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neurocomputing/2015/Subagdja, Tan - 2015.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Adaptive resonance theory,Episodic memory,Transitive inference},
pages = {1--14},
publisher = {Elsevier},
title = {{Neural modeling of sequential inferences and learning over episodic memory}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231215001873},
year = {2015}
}
@incollection{Dobnik2013,
author = {Dobnik, Simon and Cooper, Robin},
booktitle = {Constraint Solving and Language Processing},
doi = {10.1007/978-3-642-41578-4_5},
editor = {Duchier, Denys and Parmentier, Yannick},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Constraint Solving and Language Processing/2013/Dobnik, Cooper - 2013.pdf:pdf},
keywords = {action,formal semantics,language,learning and classification,perception,scriptions,spatial de-},
pages = {70--91},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Modelling language, action, and perception in Type Theory with Records}},
year = {2013}
}
@article{Madl2014,
author = {Madl, Tamas and Chen, Ke and Montaldi, Daniela and Trappl, Robert},
doi = {10.1016/j.neunet.2015.01.002},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neural Networks/2014/Madl et al. - 2014.pdf:pdf},
issn = {0893-6080},
journal = {Neural Networks},
keywords = {computational cognitive modeling,spatial memory models},
pages = {18--43},
publisher = {Elsevier Ltd},
title = {{Computational cognitive models of spatial memory: a review}},
url = {http://dx.doi.org/10.1016/j.neunet.2015.01.002},
volume = {65},
year = {2014}
}
@book{Vityaev2006,
address = {Новосибирск},
author = {Витяев, Е. Е.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2006/Витяев - 2006.pdf:pdf},
language = {russian},
pages = {293},
publisher = {Новосиб. гос. ун-т},
title = {{Извлечение знаний из данных. Компьютерное познание. Модели когнитивных процессов: Монография}},
year = {2006}
}
@article{Treur2016,
author = {Treur, Jan},
doi = {10.1016/j.bica.2016.02.002},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Biologically Inspired Cognitive Architectures/2016/Treur - 2016.pdf:pdf},
issn = {2212683X},
journal = {Biologically Inspired Cognitive Architectures},
pages = {131--168},
publisher = {Elsevier B.V.},
title = {{Dynamic modeling based on a temporal–causal network modeling approach}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S2212683X16300147},
volume = {16},
year = {2016}
}
@article{Raymond1992,
author = {Raymond, J. E. and Shapiro, K. L.},
journal = {Journal of Experimental Psychology: Human Perception and Performance},
number = {18},
pages = {849--860},
title = {{Temporary suppression of visual processing in an RSVP task: an attentional blink?}},
year = {1992}
}
@article{Borisyuk2004,
abstract = {We develop a new oscillatory model that combines consecutive selection of objects and discrimination between new and familiar objects. The model works with visual information and fulfils the following operations: (1) separation of different objects according to their spatial connectivity; (2) consecutive selection of objects located in the visual field into the attention focus; (3) extraction of features; (4) representation of objects in working memory; (5) novelty detection of objects. The functioning of the model is based on two main principles: the synchronization of oscillators through phase-locking and resonant increase of the amplitudes of oscillators if they work in-phase with other oscillators. The results of computer simulation of the model are described for visual stimuli representing printed words.},
author = {Borisyuk, Roman M and Kazanovich, Yakov B},
doi = {10.1016/j.neunet.2004.03.005},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neural networks the official journal of the International Neural Network Society/2004/Borisyuk, Kazanovich - 2004.pdf:pdf},
issn = {0893-6080},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {Attention,Attention: physiology,Computer Simulation,Discrimination Learning,Discrimination Learning: physiology,Humans,Mathematics,Memory,Memory: physiology,Models,Neurological,Photic Stimulation,Photic Stimulation: methods,Psychological,Signal Detection,Visual Perception,Visual Perception: physiology},
number = {7},
pages = {899--915},
pmid = {15312834},
title = {{Oscillatory model of attention-guided object selection and novelty detection}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15312834},
volume = {17},
year = {2004}
}
@article{Chersi2014,
abstract = {A growing body of evidence in cognitive psychology and neuroscience suggests a deep interconnection between sensory-motor and language systems in the brain. Based on recent neurophysiological findings on the anatomo-functional organization of the fronto-parietal network, we present a computational model showing that language processing may have reused or co-developed organizing principles, functionality, and learning mechanisms typical of premotor circuit. The proposed model combines principles of Hebbian topological self-organization and prediction learning. Trained on sequences of either motor or linguistic units, the network develops independent neuronal chains, formed by dedicated nodes encoding only context-specific stimuli. Moreover, neurons responding to the same stimulus or class of stimuli tend to cluster together to form topologically connected areas similar to those observed in the brain cortex. Simulations support a unitary explanatory framework reconciling neurophysiological motor data with established behavioral evidence on lexical acquisition, access, and recall.},
author = {Chersi, Fabian and Ferro, Marcello and Pezzulo, Giovanni and Pirrelli, Vito},
doi = {10.1111/tops.12094},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Topics in cognitive science/2014/Chersi et al. - 2014.pdf:pdf},
issn = {1756-8765},
journal = {Topics in cognitive science},
keywords = {computational modeling,lexical chains,motor chains,prediction,self-organizing maps,serial working memory,somatotopic organization},
number = {3},
pages = {476--91},
pmid = {24935737},
title = {{Topological self-organization and prediction learning support both action and lexical chains in the brain}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24935737},
volume = {6},
year = {2014}
}
@article{Taatgen2005,
abstract = {Emerging parallel processing and increased flexibility during the acquisition of cognitive skills form a combination that is hard to reconcile with rule-based models that often produce brittle behavior. Rule-based models can exhibit these properties by adhering to 2 principles: that the model gradually learns task-specific rules from instructions and experience, and that bottom-up processing is used whenever possible. In a model of learning perfect time-sharing in dual tasks (Schumacher et al., 2001), speedup learning and bottom-up activation of instructions can explain parallel behavior. In a model of a complex dynamic task (Carnegie Mellon University Aegis Simulation Program [CMU-ASP], Anderson et al., 2004), parallel behavior is explained by the transition from serially organized instructions to rules that are activated by both top-down (goal-driven) and bottom-up (perceptually driven) factors. Parallelism lets the model opportunistically reorder instructions, leading to the gradual emergence of new task strategies.},
author = {Taatgen, Niels},
doi = {10.1207/s15516709cog0000_23},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive science/2005/Taatgen - 2005.pdf:pdf},
isbn = {0364-0213},
issn = {0364-0213},
journal = {Cognitive science},
keywords = {cog{\_}arch,cognitive architecture,complex systems,computer,dual tasking,human,instruction,interaction,knowledge,learning,psychology,representation,situated cognition,skill acquisition and learning,symbolic computational modeling},
mendeley-tags = {cog{\_}arch},
number = {3},
pages = {421--455},
pmid = {21702780},
title = {{Modeling parallelization and flexibility improvements in skill acquisition: from dual tasks to complex dynamic skills}},
volume = {29},
year = {2005}
}
@unpublished{Intelligence2012,
author = {Chella, Antonio and Manzotti, Ricardo},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2012/Chella, Manzotti - 2012.pdf:pdf},
pages = {2--3},
title = {{Strong Artificial Intelligence and Consciousness}},
year = {2012}
}
@incollection{Palomino2016,
abstract = {This paper presents a novel attention-based cognitive architecture for a social robot. This architecture aims to join perception and reasoning considering a double interplay: the current task biases the perceptual process whereas perceived items determine the behaviours to be accomplished, considering the present context and role of the agent. Therefore, the proposed architecture represents a bidirectional solution to the perception-reasoning-action loop closing problem. The proposal is divided into two levels of performance, employing anObject-BasedVisual Attention model as perception system and a general purpose Planning Framework at the top deliberative level. The architecture has been tested using a real and unrestricted environment that involves a real robot, time-varying tasks and daily life situations.},
author = {Palomino, Antonio Jes{\'{u}}s and Marfil, Rebeca and Bandera, Juan Pedro and Bandera, Antonio},
booktitle = {Robot 2015: Second Iberian Robotics Conference},
doi = {10.1007/978-3-319-27149-1},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Robot 2015 Second Iberian Robotics Conference/2016/Palomino et al. - 2016.pdf:pdf},
isbn = {978-3-319-27148-4},
keywords = {attention,attention model,bidirectional,cog{\_}arch,cognitive architecture,social robot},
mendeley-tags = {attention,cog{\_}arch},
pages = {721--732},
series = {Advances in Intelligent Systems and Computing},
title = {{A New Cognitive Architecture for Bidirectional Loop Closing}},
url = {http://link.springer.com/10.1007/978-3-319-27149-1},
year = {2016}
}
@inproceedings{Howard2015,
abstract = {We have created a high-fidelity model of 9 regions of the brain involved in making sense of complex and uncertain situations. Sense making is a proactive form of situation awareness requiring sifting through information of various types to form hypotheses about evolving situations. The MINDS model (Mirroring Intelligence in a Neural Description of Sensemaking) reveals the neural principles and cognitive tradeoffs that explain weaknesses in human reasoning and decision-making.},
author = {Howard, Michael D. and Bhattacharyya, Rajan and Chelian, Suhas E. and Phillips, Matthew E. and Pilly, Praveen K. and Ziegler, Matthias D.},
booktitle = {2015 IEEE Aerospace Conference},
doi = {10.1109/AERO.2015.7118968},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/2015 IEEE Aerospace Conference/2015/Howard et al. - 2015.pdf:pdf},
isbn = {978-1-4799-5379-0},
keywords = {Brain modeling,Cognition,Computational modeling,Decision making,MINDS computational model,Semantics,Tuning,Uncertainty,brain,cognition,decision making,decision-making,human computer interaction,human-system interaction,mirroring intelligence-in-a-neural description-of-,neural nets,neural principle,situation awareness},
pages = {1--16},
publisher = {IEEE},
shorttitle = {Aerospace Conference, 2015 IEEE},
title = {{The neural basis of decision-making during sensemaking: Implications for human-system interaction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7118968},
year = {2015}
}
@book{Baars1988,
address = {New York},
author = {Baars, Bernard J.},
publisher = {Cambridge University Press},
title = {{A cognitive theory of consciousness}},
year = {1988}
}
@article{Sun2004,
abstract = {This paper explores the interaction between implicit and explicit processes during skill learning, in terms of top-down learning (that is, learning that goes from explicit to implicit knowledge) versus bottom-up learning (that is, learning that goes from implicit to explicit knowledge). Instead of studying each type of knowledge (implicit or explicit) in isolation, we stress the interaction between the two types, especially in terms of one type giving rise to the other, and its effects on learning. The work presents an integrated model of skill learning that takes into account both implicit and explicit processes and both top-down and bottom-up learning. We examine and simulate human data in the Tower of Hanoi task. The paper shows how the quantitative data in this task may be captured using either top-down or bottom-up approaches, although top-down learning is a more apt explanation of the human data currently available. These results illustrate the two different directions of learning (top-down versus bottom-up), and thereby provide a new perspective on skill learning. ?? 2003 Elsevier B.V. All rights reserved.},
author = {Sun, Ron and Zhang, Xi},
doi = {10.1016/j.cogsys.2003.07.001},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive Systems Research/2004/Sun, Zhang - 2004.pdf:pdf},
issn = {13890417},
journal = {Cognitive Systems Research},
pages = {63--89},
title = {{Top-down versus bottom-up learning in cognitive skill acquisition}},
volume = {5},
year = {2004}
}
@article{Keramati2011a,
abstract = {Reinforcement learning models address animal's behavioral adaptation to its changing “external” environment, and are based on the assumption that Pavlo- vian, habitual and goal-directed responses seek to maximize reward acquisition. Negative-feedback models of homeostatic regulation, on the other hand, are con- cerned with behavioral adaptation in response to the “internal” state of the animal, and assume that animals' behavioral objective is to minimize deviations of some key physiological variables from their hypothetical setpoints. Building upon the drive-reduction theory of reward, we propose a new analytical framework that in- tegrates learning and regulatory systems, such that the two seemingly unrelated objectives of reward maximization and physiological-stability prove to be identi- cal. The proposed theory shows behavioral adaptation to both internal and external states in a disciplined way. We further show that the proposed framework allows for a unified explanation of some behavioral pattern like motivational sensitivity of different associative learning mechanism, anticipatory responses, interaction among competing motivational systems, and risk aversion.},
author = {Keramati, Mehdi and Gutkin, Boris},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Nips/2011/Keramati, Gutkin - 2011.pdf:pdf},
isbn = {9781618395993},
journal = {Nips},
pages = {82--90},
title = {{A Reinforcement Learning theory for homeostatic regulation}},
year = {2011}
}
@article{Damerow2016,
abstract = {Most current approaches to scene understanding lack the capability to adapt object and situation models to behavioral needs not anticipated by the human system designer. Here, we give a detailed description of a system architecture for self-referential autonomous learning which enables the refinement of object and situation models during operation in order to optimize behavior. This includes structural learning of hierarchical models for situations and behaviors that is triggered by a mismatch between expected and actual action outcome. Besides proposing architectural concepts, we also describe a first implementation of our system within a simulated traffic scenario to demonstrate the feasibility of our approach.},
author = {Damerow, Florian and Knoblauch, Andreas and K{\"{o}}rner, Ursula and Eggert, Julian and K{\"{o}}rner, Edgar},
doi = {10.1007/s12559-016-9407-7},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive Computation/2016/Damerow et al. - 2016.pdf:pdf},
issn = {1866-9956},
journal = {Cognitive Computation},
keywords = {autonomous learning {\'{a}} hierarchical,self-referential control {\'{a}} scene,situation model,understanding {\'{a}}},
pages = {1--17},
title = {{Toward Self-Referential Autonomous Learning of Object and Situation Models}},
url = {http://link.springer.com/10.1007/s12559-016-9407-7},
year = {2016}
}
@article{Itti1998,
author = {Itti, Laurent and Koch, Christof and Niebur, Ernst},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/IEEE Transactions on pattern analysis and machine intelligence/1998/Itti, Koch, Niebur - 1998.pdf:pdf},
journal = {IEEE Transactions on pattern analysis and machine intelligence},
number = {11},
pages = {1254--1259},
title = {{A model of Siliency-Based Visual Attention for Rapid Scene Analysis}},
volume = {20},
year = {1998}
}
@article{Tabor2013,
abstract = {Human participants and recurrent ("connectionist") neural networks were both trained on a categorization system abstractly similar to natural language systems involving irregular ("strong") classes and a default class. Both the humans and the networks exhibited staged learning and a generalization pattern reminiscent of the Elsewhere Condition (Kiparsky, 1973). Previous connectionist accounts of related phenomena have often been vague about the nature of the networks' encoding systems. We analyzed our network using dynamical systems theory, revealing topological and geometric properties that can be directly compared with the mechanisms of non-connectionist, rule-based accounts. The results reveal that the networks "contain" structures related to mechanisms posited by rule-based models, partly vindicating the insights of these models. On the other hand, they support the one mechanism (OM), as opposed to the more than one mechanism (MOM), view of symbolic abstraction by showing how the appearance of MOM behavior can arise emergently from one underlying set of principles. The key new contribution of this study is to show that dynamical systems theory can allow us to explicitly characterize the relationship between the two perspectives in implemented models.},
author = {Tabor, Whitney and Cho, Pyeong W and Dankowicz, Harry},
doi = {10.1111/cogs.12072},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive science/2013/Tabor, Cho, Dankowicz - 2013.pdf:pdf},
issn = {1551-6709},
journal = {Cognitive science},
keywords = {Adult,Computer Simulation,Concept Formation,Concept Formation: physiology,Female,Generalization (Psychology),Generalization (Psychology): physiology,Humans,Learning,Learning: physiology,Male,Models,Problem Solving,Problem Solving: physiology,Psychological},
number = {7},
pages = {1193--227},
pmid = {23931713},
title = {{Birth of an abstraction: a dynamical systems account of the discovery of an elsewhere principle in a category learning task}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23931713},
volume = {37},
year = {2013}
}
@article{Chernavsky2012a,
author = {Чернавская, О. Д. and Чернавский, Д. С. and Карп, В. П. and Никитин, А. П. and Рожило, Я. А.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Сложные системы/2012/Чернавская et al. - 2012(2).pdf:pdf},
journal = {Сложные системы},
language = {russian},
number = {3},
pages = {46--65},
title = {{Процесс мышления в контексте динамической теории информации. Часть II: понятие «образ» и «символ» как инструменты моделирования процесса мышления средствами нейрокомпьютинга}},
volume = {2},
year = {2012}
}
