Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@phdthesis{George2008,
author = {George, Dileep},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/George/George - 2008 - How the Brain Might Work a Hierarchical and Temporal Model for Learning and Recognition.pdf:pdf},
number = {June},
pages = {191},
school = {Stanford University},
title = {{How the Brain Might Work: a Hierarchical and Temporal Model for Learning and Recognition}},
year = {2008}
}
@article{Sabater2005,
abstract = {The scientific research in the area of computational mechanisms for trust and reputation in virtual societies is a recent discipline oriented to increase the reliability and performance of electronic communities. Computer science has moved from the paradigm of isolated machines to the paradigm of networks and distributed computing. Likewise, artificial intelligence is quickly moving from the paradigm of isolated and non-situated intelligence to the paradigm of situated, social and collective intelligence. The new paradigm of the so called intelligent or autonomous agents and multi-agent systems (MAS) together with the spectacular emergence of the information society technologies (specially reflected by the popularization of electronic commerce) are responsible for the increasing interest on trust and reputation mechanisms applied to electronic societies. This review wants to offer a panoramic view on current computational trust and reputation models.},
author = {Sabater, Jordi and Sierra, Carles},
doi = {10.1007/s10462-004-0041-5},
file = {::},
isbn = {02692821 (ISSN)},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
keywords = {Reputation,Trust},
month = {sep},
number = {1},
pages = {33--60},
title = {{Review on Computational Trust and Reputation Models}},
url = {http://link.springer.com/10.1007/s10462-004-0041-5},
volume = {24},
year = {2005}
}
@article{Barto2003,
abstract = {Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of {\{}semi-Markov{\}} decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.},
author = {Barto, Andrew G. and Mahadevan, Sridhar},
doi = {10.1023/A:1025696116075},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Discrete Event Dynamic Systems/Barto, Mahadevan/Barto, Mahadevan - 2003 - Recent Advances in Hierarchical Reinforcement Learning.pdf:pdf},
isbn = {0924-6703},
issn = {09246703},
journal = {Discrete Event Dynamic Systems},
pages = {341--379},
title = {{Recent Advances in Hierarchical Reinforcement Learning}},
url = {http://dx.doi.org/10.1023/A:1025696116075},
volume = {13},
year = {2003}
}
@article{Panov2015e,
abstract = {В работе рассмотрен ряд вопросов, возникающих в области автоматизации управления малыми беспилотными летательными аппаратами (БПЛА) мультироторного типа. Предложена совокупность методов планирования и управления, рассмотрена задача организации взаимодействия различных методов и алгоритмов (авторских и известных) в единую интеллектуальную систему управления БПЛА. Предлагается использование трех уровней управления – стратегического, тактического и реактивного, описывается соответствующая архитектура – STRL (от англ. strategic, tactical, reactive, layered). Использование этой архитектуры позволит автоматизировать управление коалициями БПЛА при решении широкого круга задач в различных средах.},
author = {Макаров, Д. А. and Панов, А. И. and Яковлев, К. С.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Искусственный интеллект и принятие решений/Макаров, Панов, Яковлев/Макаров, Панов, Яковлев - 2015 - Архитектура многоуровневой интеллектуальной системы управления беспилотными летательными аппаратами.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Искусственный интеллект и принятие решений/Макаров, Панов, Яковлев/Макаров, Панов, Яковлев - 2015 - Архитектура многоуровневой интеллектуальной системы управления беспилотными летательными аппаратами.doc:doc;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Искусственный интеллект и принятие решений/Макаров, Панов, Яковлев/Макаров, Панов, Яковлев - 2015 - Архитектура многоуровневой интеллектуальной системы управления беспилотными летательными аппаратами(2).pdf:pdf},
journal = {Искусственный интеллект и принятие решений},
keywords = {14-11-00692,elibrary,mypub,vak,беспилотные летательные аппараты,знаковая картина мира,интеллектуальная система управления,многоуровневая архитектура,нелинейное управление,планирование траектории,уравнение Риккати},
language = {russian},
mendeley-tags = {14-11-00692,elibrary,mypub,vak},
number = {3},
pages = {18--33},
title = {{Архитектура многоуровневой интеллектуальной системы управления беспилотными летательными аппаратами}},
year = {2015}
}
@inproceedings{Mehta2008a,
abstract = {We present an algorithm, HI-MAT (Hierar- chy Induction via Models And Trajectories), that discovers MAXQ task hierarchies by ap- plying dynamic Bayesian network models to a successful trajectory from a source rein- forcement learning task. HI-MAT discovers subtasks by analyzing the causal and tem- poral relationships among the actions in the trajectory. Under appropriate assumptions, HI-MAT induces hierarchies that are consis- tent with the observed trajectory and have compact value-function tables employing safe state abstractions. We demonstrate empir- ically that HI-MAT constructs compact hi- erarchies that are comparable to manually- engineered hierarchies and facilitate signifi- cant speedup in learning when transferred to a target task.},
address = {New York, New York, USA},
author = {Mehta, Neville and Ray, Soumya and Tadepalli, Prasad and Dietterich, Thomas},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390238},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 25th international conference on Machine learning - ICML '08/Mehta et al/Mehta et al. - 2008 - Automatic discovery and transfer of MAXQ hierarchies.pdf:pdf},
isbn = {9781605582054},
pages = {648--655},
publisher = {ACM Press},
title = {{Automatic discovery and transfer of MAXQ hierarchies}},
url = {http://portal.acm.org/citation.cfm?id=1390238 http://portal.acm.org/citation.cfm?doid=1390156.1390238},
year = {2008}
}
@misc{Presentation2016c,
author = {Panov, Aleksandr I.},
file = {::},
pages = {53},
title = {{Biologically and psychologically inspired modelling in BICA}},
year = {2016}
}
@article{Wang2012a,
abstract = {Reinforcement learning has been an important category of machine learning approaches exhibiting self-learning and online learning characteristics. Using reinforcement learning, an agent can learn its behaviors through trial-and-error interactions with a dynamic environment and finally come up with an optimal strategy. Reinforcement learning suffers the curse of dimensionality, though there has been significant progress to overcome this issue in recent years. MAXQ is one of the most common approaches for reinforcement learning. To function properly, MAXQ requires a decomposition of the agent's task into a task hierarchy. Previously, the decomposition can only be done manually. In this paper, we propose a mechanism for automatic subtask discovery. The mechanism applies clustering to automatically construct task hierarchy required by MAXQ, such that MAXQ can be fully automated. We present the design of our mechanism, and demonstrate its effectiveness through theoretical analysis and an extensive experimental evaluation. {\textcopyright} 2012 IEEE.},
author = {Wang, Hongbing and Li, Wenya and Zhou, Xuan},
doi = {10.1109/ICTAI.2012.165},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of International Conference on Tools with Artificial Intelligence, ICTAI/Wang, Li, Zhou/Wang, Li, Zhou - 2012 - Automatic discovery and transfer of MAXQ hierarchies in a complex system.pdf:pdf},
isbn = {9780769549156},
issn = {10823409},
journal = {Proceedings of International Conference on Tools with Artificial Intelligence, ICTAI},
keywords = {Clustering,MAXQ,Reinforcement Learning,System of Systems},
pages = {1157--1162},
title = {{Automatic discovery and transfer of MAXQ hierarchies in a complex system}},
volume = {1},
year = {2012}
}
@article{James2018a,
author = {James, Steven and Rosman, Benjamin and Africa, South and Konidaris, George},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/James et al/James et al. - 2018 - Learning to Plan with Portable Symbols.pdf:pdf},
number = {July},
title = {{Learning to Plan with Portable Symbols}},
year = {2018}
}
@article{Wahlster1989,
abstract = {This chapter surveys the field of user modeling in artificial intelligence dialog systems. First, reasons why user modeling has become so important in the last few years are pointed out, and definitions are proposed for the terms 'user model' and 'user modeling component'. Research within and outside of artificial intelligence which is related to user modeling in dialog systems is discussed. In Section 2, techniques for constructing user models in the course of a dialog are presented and, in Section 3, recent proposals for representing a wide range of assumptions about a user's beliefs and goals in a system's knowledge base are surveyed. Examples for the application of user models in systems developed to date are then given, and some social implications discussed. Finally, unsolved problems like coping with collective beliefs or resource-limited processes are investigated, and prospects for application- oriented research are outlined. Although the survey is restricted to user models in natural- language dialog systems, most of the concepts and methods discussed can be extended to AI dialog systems in general.},
author = {Wahlster, Wolfgang and Kobsa, Alfred},
doi = {10.1007/978-3-642-83230-7},
isbn = {0-387-18380-9; 3-540-18380-9 Print},
journal = {User Models in Dialog Systems},
number = {Sfb 314},
pages = {4--34},
pmid = {3383829},
title = {{User models in dialog systems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.9428{\&}rep=rep1{\&}type=pdf{\%}5Cnhttp://link.springer.com/chapter/10.1007/978-3-642-83230-7{\_}1},
year = {1989}
}
@article{Chen2016a,
abstract = {Abstract Lifelong Machine Learning (or Lifelong Learning) is an advanced machine learning paradigm that learns continuously, accumulates the knowledge learned in previous tasks, and uses it to help future learning. In the process, the learner becomes more and more knowledgeable and effective at learning. This learning ability is one of the hallmarks of human intelligence. However, the current dominant machine learning paradigm learns in isolation: given a training dataset, it runs a machine learning algorithm on the dataset to produce a model. It makes no attempt to retain the learned knowledge and use it in future learning. Although this isolated learning paradigm has been very successful, it requires a large number of training examples, and is only suitable for well-defined and narrow tasks. In comparison, we humans can learn effectively with a few examples because we have accumulated so much knowledge in the past which enables us to learn with little data or effort. Lifelong learning aims to achieve th...},
author = {Chen, Zhiyuan and Liu, Bing},
doi = {10.2200/S00737ED1V01Y201610AIM033},
file = {::},
isbn = {9781627055017},
issn = {1939-4608},
journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
number = {3},
pages = {1--145},
title = {{Lifelong Machine Learning}},
url = {http://www.morganclaypool.com/doi/10.2200/S00737ED1V01Y201610AIM033},
volume = {10},
year = {2016}
}
@phdthesis{Mehta2011,
abstract = {Acting intelligently to efficiently solve sequential decision problems requires the ability to extract hierarchical structure from the underlying domain dynamics, exploit it for optimal or near-optimal decision-making, and transfer it to related problems instead of solving every problem in isolation. This dissertation makes three contributions toward this goal. The first contribution is the introduction of two frameworks for the transfer of hi- erarchical structure in sequential decision problems. The MASH framework facilitates transfer among multiple agents coordinating within a domain. The VRHRL framework allows an agent to transfer its knowledge across a family of domains that share the same transition dynamics but have differing reward dynamics. Both MASH and VRHRL are validated empirically in large domains and the results demonstrate significant speedup in the solutions due to transfer. The second contribution is a new approach to the discovery of hierarchical structure in sequential decision problems. HI-MAT leverages action models to analyze the relevant dependencies in a hierarchically-generated trajectory and it discovers hierarchical struc- ture that transfers to all problems whose actions share the same relevant dependencies as the single source problem. HierGen advances HI-MAT by learning simple action models, leveraging these models to analyze non-hierarchically-generated trajectories from mul- tiple source problems in a robust causal fashion, and discovering hierarchical structure that transfers to all problems whose actions share the same causal dependencies as those in the source problems. Empirical evaluations in multiple domains demonstrate that the discovered hierarchical structures are comparable to manually-designed structures in quality and performance. Action models are essential to hierarchical structure discovery and other aspects of intelligent behavior. The third contribution of this dissertation is the introduction of two general frameworks for learning action models in sequential decision problems. In the MBP framework, learning is user-driven; in the PLEX framework, the learner generates its own problems. The frameworks are formally analyzed and reduced to concept learning with one-sided error. A general action-modeling language is shown to be efficiently learnable in both frameworks.},
author = {Mehta, Neville},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Mehta/Mehta - 2011 - Hierarchical Structure Discovery and Transfer in Sequential Decision Problems.pdf:pdf},
pages = {180},
title = {{Hierarchical Structure Discovery and Transfer in Sequential Decision Problems}},
year = {2011}
}
@article{Crosby2015,
author = {Crosby, Matthew},
file = {::},
journal = {ICAPS Proceedings of the Competition of Distributed and Multi-Agent Planners (CoDMAP-15)},
pages = {4--7},
title = {{ADP an Agent Decomposition Planner CoDMAP 2015}},
year = {2015}
}
@article{Kaelbling1993,
abstract = {This paper presents the HDG learning algorithm, which uses a hierarchical decomposition of the state space to make learning to achieve goals more efficient with a small penalty in path qual-ity. Special care must be taken when performing hierarchical planning and learning in stochastic domains, because macro-operators cannot be ex-ecuted ballistically. The HDG algorithm, which is a descendent of Watkins' Q-learning algorithm, is described here and preliminary empirical re-sults are presented.},
author = {Kaelbling, Leslie},
file = {::},
journal = {Proceedings of the Tenth International Conference on Machine Learning},
pages = {167--173},
title = {{Hierarchical Learning in Stochastic Domains: Preliminary Results}},
url = {http://people.csail.mit.edu/lpk/papers/ml93.ps},
year = {1993}
}
@inproceedings{Makarov2016,
abstract = {Работа посвящена архитектуре системы управления сложными техническими объектами, рассматриваемыми как интеллектуальные агенты. В качестве таких объектов взяты малые беспилотные летательные аппараты (БПЛА) мультикоптерного типа. Архитектура состоит из трех уровней: стратегического, тактического и реактивного. Её применение позволит автоматизировать управление как отдельными БПЛА, так и их коалициями БПЛА при решении широкого круга задач.},
address = {Смоленск},
author = {Макаров, Д. А. and Панов, А. И. and Яковлев, К. С.},
booktitle = {Пятнадцатая национальная конференция по искусственному интеллекту с международным участием КИИ-2016 (3-7 октября 2016г., г.Смоленск, Россия): Труды конференции},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Пятнадцатая национальная конференция по искусственному интеллекту с международным участием КИИ-2016 (3-7.Смоленск, Россия) Труды конференции/Макаров, Панов, Яковлев/Макаров, Панов, Яковлев - 2016 - STRL много.doc:doc;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Пятнадцатая национальная конференция по искусственному интеллекту с международным участием КИИ-2016 (3-7.Смоленск, Россия) Труды конференции/Макаров, Панов, Яковлев/Макаров, Панов, Яковлев - 2016 - STRL много.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Пятнадцатая национальная конференция по искусственному интеллекту с международным участием КИИ-2016 (3-7.Смоленск, Россия) Труды конференции/Макаров, Панов, Яковлев/Макаров, Панов, Яковлев - 2016 - STRL мн(2).pdf:pdf},
isbn = {978-5-91412-316-8},
keywords = {14-11-00692,elibrary,myconf,беспилотные летательные аппараты,знаковая картина мира,интеллектуальная система управления,многоуровневая архитектура,нелинейное управление,планирование поведения,планирование траектории,уравнение Риккати},
language = {russian},
mendeley-tags = {14-11-00692,elibrary,myconf},
pages = {179--188},
publisher = {Универсум},
title = {{STRL: многоуровневая система управления интеллектуальными агентами}},
volume = {1},
year = {2016}
}
@article{Pardo2017,
abstract = {In reinforcement learning, it is common to let an agent interact for a fixed amount of time with its environment before resetting it and repeating the process in a series of episodes. The task that the agent has to learn can either be to maximize its performance over (i) that fixed period, or (ii) an indefinite period where time limits are only used during training to diversify experience. In this paper, we provide a formal account for how time limits could effectively be handled in each of the two cases and explain why not doing so can cause state-aliasing and invalidation of experience replay, leading to suboptimal policies and training instability. In case (i), we argue that the terminations due to time limits are in fact part of the environment, and thus a notion of the remaining time should be included as part of the agent's input to avoid violation of the Markov property. In case (ii), the time limits are not part of the environment and are only used to facilitate learning. We argue that this insight should be incorporated by bootstrapping from the value of the state at the end of each partial episode. For both cases, we illustrate empirically the significance of our considerations in improving the performance and stability of existing reinforcement learning algorithms, showing state-of-the-art results on several control tasks.},
archivePrefix = {arXiv},
arxivId = {1712.00378},
author = {Pardo, Fabio and Tavakoli, Arash and Levdik, Vitaly and Kormushev, Petar},
eprint = {1712.00378},
file = {::},
issn = {1938-7228},
title = {{Time Limits in Reinforcement Learning}},
url = {http://arxiv.org/abs/1712.00378},
year = {2017}
}
@article{George2009,
abstract = {The theoretical setting of hierarchical Bayesian inference is gaining acceptance as a framework for understanding cortical computation. In this paper, we describe how Bayesian belief propagation in a spatio-temporal hierarchical model, called Hierarchical Temporal Memory (HTM), can lead to a mathematical model for cortical circuits. An HTM node is abstracted using a coincidence detector and a mixture of Markov chains. Bayesian belief propagation equations for such an HTM node define a set of functional constraints for a neuronal implementation. Anatomical data provide a contrasting set of organizational constraints. The combination of these two constraints suggests a theoretically derived interpretation for many anatomical and physiological features and predicts several others. We describe the pattern recognition capabilities of HTM networks and demonstrate the application of the derived circuits for modeling the subjective contour effect. We also discuss how the theory and the circuit can be extended to explain cortical features that are not explained by the current model and describe testable predictions that can be derived from the model.},
author = {George, Dileep and Hawkins, Jeff},
doi = {10.1371/journal.pcbi.1000532},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/PLoS computational biology/George, Hawkins/George, Hawkins - 2009 - Towards a mathematical theory of cortical micro-circuits.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Artificial Intelligence,Automated,Automated: methods,Bayes Theorem,Cerebral Cortex,Cerebral Cortex: physiology,Feedback,Markov Chains,Memory,Memory: physiology,Models,Neurological,Pattern Recognition,Pyramidal Cells,Pyramidal Cells: physiology},
number = {10},
pages = {e1000532},
pmid = {19816557},
title = {{Towards a mathematical theory of cortical micro-circuits}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2749218{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {5},
year = {2009}
}
@article{Dubey2018,
abstract = {What makes humans so good at solving seemingly complex video games? Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors on human performance. We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors. We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play. Videos and the game manipulations are available at https://rach0012.github.io/humanRL{\_}website/},
archivePrefix = {arXiv},
arxivId = {1802.10217},
author = {Dubey, Rachit and Agrawal, Pulkit and Pathak, Deepak and Griffiths, Thomas L. and Efros, Alexei A.},
doi = {arXiv:1802.10217v1},
eprint = {1802.10217},
file = {::},
title = {{Investigating Human Priors for Playing Video Games}},
url = {http://arxiv.org/abs/1802.10217},
year = {2018}
}
@article{Pechoucek2002,
author = {Pechoucek, Michal and Marik, Vladimir and Barta, Jaroslav},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IEEE Intelligent Systems/Pechoucek, Marik, Barta/Pechoucek, Marik, Barta - 2002 - A Knowledge-Based Approach to Coalition Formation.pdf:pdf},
journal = {IEEE Intelligent Systems},
number = {3},
pages = {17--25},
title = {{A Knowledge-Based Approach to Coalition Formation}},
volume = {17},
year = {2002}
}
@article{Potamianos2005,
author = {Potamianos, Alexandros and Narayanan, Shrikanth and Member, Senior and Riccardi, Giuseppe},
doi = {10.1007/978-3-540-49127-9_35},
isbn = {9781598295993},
issn = {1947-4040},
journal = {Audio},
number = {3},
pages = {321--329},
title = {{Spoken Dialogue Systems}},
volume = {13},
year = {2005}
}
@article{Roncone2017,
abstract = {— Collaborative robots represent a clear added value to manufacturing, as they promise to increase productivity and improve working conditions of such environments. Although modern robotic systems have become safe and reliable enough to operate close to human workers on a day-to-day basis, the workload is still skewed in favor of a limited contribution from the robot's side, and a significant cognitive load is allotted to the human. We believe the transition from robots as recipients of human instruction to robots as capable collaborators hinges around the implementation of transparent systems, where mental models about the task are shared between peers, and the human partner is freed from the responsibility of taking care of both actors. In this work, we implement a transparent task planner able to be deployed in realistic, near-future applications. The proposed framework is capable of basic reasoning capabilities for what concerns role assignment and task allocation, and it interfaces with the human partner at the level of abstraction he is most comfortable with. The system is readily available to non-expert users, and programmable with high-level commands in an intuitive interface. Our results demonstrate an overall improvement in terms of completion time, as well as a reduced cognitive load for the human partner.},
author = {Roncone, Alessandro and Mangin, Olivier and Scassellati, Brian},
doi = {10.1109/ICRA.2017.7989122},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings - IEEE International Conference on Robotics and Automation/Roncone, Mangin, Scassellati/Roncone, Mangin, Scassellati - 2017 - Transparent role assignment and task allocation in human robot collabo.pdf:pdf},
isbn = {9781509046331},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {Social Human-Robot Interaction,Human Factors and H},
pages = {1014--1021},
title = {{Transparent role assignment and task allocation in human robot collaboration}},
year = {2017}
}
@article{Milford2014,
abstract = {Mobile robots and animals alike must effectively navigate their environments in order to achieve their goals. For animals goal-directed navigation facilitates finding food, seeking shelter or migration; similarly robots perform goal-directed navigation to find a charging station, get out of the rain or guide a person to a destination. This similarity in tasks extends to the environment as well; increasingly, mobile robots are operating in the same underwater, ground and aerial environments that animals do. Yet despite these similarities, goal-directed navigation research in robotics and biology has proceeded largely in parallel, linked only by a small amount of interdisciplinary research spanning both areas. Most state-of-the-art robotic navigation systems employ a range of sensors, world representations and navigation algorithms that seem far removed from what we know of how animals navigate; their navigation systems are shaped by key principles of navigation in 'real-world' environments including dealing with uncertainty in sensing, landmark observation and world modelling. By contrast, biomimetic animal navigation models produce plausible animal navigation behaviour in a range of laboratory experimental navigation paradigms, typically without addressing many of these robotic navigation principles. In this paper, we attempt to link robotics and biology by reviewing the current state of the art in conventional and biomimetic goal-directed navigation models, focusing on the key principles of goal-oriented robotic navigation and the extent to which these principles have been adapted by biomimetic navigation models and why.},
author = {Milford, M. and Schulz, R.},
doi = {10.1098/rstb.2013.0484},
file = {::},
isbn = {1471-2970},
issn = {0962-8436},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
number = {1655},
pages = {20130484--20130484},
pmid = {25267826},
title = {{Principles of goal-directed spatial robot navigation in biomimetic models}},
url = {http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2013.0484},
volume = {369},
year = {2014}
}
@article{Ring1994,
abstract = {Continual learning$\backslash$n$\backslash$nis the constant development of complex behaviors with no nal end in$\backslash$n$\backslash$nmind. It is the process of learning ever more complicated skills by$\backslash$nbuilding on those skills al-$\backslash$n$\backslash$nready developed. In order for learning at one stage of development$\backslash$nto serve as the foundation$\backslash$n$\backslash$nfor later learning, a continual-learning agent should learn hierarchically.$\backslash$nCHILD, an agent$\backslash$n$\backslash$ncapable of$\backslash$n$\backslash$nContinual, Hierarchical, Incremental Learning$\backslash$n$\backslash$nand$\backslash$n$\backslash$nDevelopment$\backslash$n$\backslash$nis proposed,$\backslash$n$\backslash$ndescribed, tested, and evaluated in this dissertation. CHILD accumulates$\backslash$nuseful behaviors$\backslash$n$\backslash$nin reinforcement environments by using the$\backslash$n$\backslash$nTemporal Transition Hierarchies$\backslash$n$\backslash$nlearning algo-$\backslash$n$\backslash$nrithm, also derived in the dissertation. This constructive algorithm$\backslash$ngenerates a hierarchical,$\backslash$n$\backslash$nhigher-order neural network that can be used for predicting context-dependent$\backslash$ntemporal se-$\backslash$n$\backslash$nquences and can learn sequential-task benchmarks more than two orders$\backslash$nof magnitude faster$\backslash$n$\backslash$nthan competing neural-network systems. Consequently, CHILD can quickly$\backslash$nsolve compli-$\backslash$n$\backslash$ncated non-Markovian reinforcement-learning tasks and can then transfer$\backslash$nits skills to similar$\backslash$n$\backslash$nbut even more complicated tasks, learning these faster still. This$\backslash$ncontinual-learning ap-$\backslash$n$\backslash$nproach is made possible by the unique properties of Temporal Transition$\backslash$nHierarchies, which$\backslash$n$\backslash$nallow existing skills to be amended and augmented in precisely the$\backslash$nsame way that they were$\backslash$n$\backslash$nconstructed in the rst place.},
author = {Ring, M B},
file = {::},
journal = {Psychology},
pages = {44},
title = {{Continual learning in reinforcement environments}},
volume = {1},
year = {1994}
}
@article{Nikolaidis2014,
abstract = {We present a framework for learning human user models from joint-action demonstrations that enables the robot to compute a robust policy for a collaborative task with a human. The learning takes place completely automatically, without any human intervention. First, we describe the clustering of demonstrated action sequences into different human types using an unsupervised learning algorithm. These demonstrated sequences are also used by the robot to learn a reward function that is representative for each type, through the employment of an inverse reinforcement learning algorithm. The learned model is then used as part of a Mixed Observability Markov Decision Process formulation, wherein the human type is a partially observable variable. With this framework, we can infer, either offline or online, the human type of a new user that was not included in the training set, and can compute a policy for the robot that will be aligned to the preference of this new user and will be robust to deviations of the human actions from prior demonstrations. Finally we validate the approach using data collected in human subject experiments, and conduct proof-of-concept demonstrations in which a person performs a collaborative task with a small industrial robot.},
archivePrefix = {arXiv},
arxivId = {1405.6341},
author = {Nikolaidis, Stefanos and Gu, Keren and Ramakrishnan, Ramya and Shah, Julie},
doi = {10.1145/2696454.2696455},
eprint = {1405.6341},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/arXiv/Nikolaidis et al/Nikolaidis et al. - 2014 - Efficient Model Learning for Human-Robot Collaborative Tasks.pdf:pdf},
isbn = {9781450328838},
issn = {21672148},
journal = {arXiv},
pages = {1--9},
title = {{Efficient Model Learning for Human-Robot Collaborative Tasks}},
url = {http://arxiv.org/abs/1405.6341},
year = {2014}
}
@article{Gorodotsky2015,
author = {Городецкий, В. И. and Самойлов, В. В. and Троцкий, Д. В.},
doi = {10.7868/S0002338815030087},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Известия РАН. Теория и системы управления/Городецкий, Самойлов, Троцкий/Городецкий, Самойлов, Троцкий - 2015 - Базовая онтология коллективного поведения автономных агентов и ее расширения.pdf:pdf},
isbn = {0002338815},
issn = {0002-3388},
journal = {Известия РАН. Теория и системы управления},
language = {russian},
number = {5},
pages = {102--121},
title = {{Базовая онтология коллективного поведения автономных агентов и ее расширения}},
url = {http://elibrary.ru/item.asp?doi=10.7868/S0002338815030087},
year = {2015}
}
@article{Madl2018,
abstract = {Computational cognitive models of spatial memory often neglect difficulties posed by the real world, such as sensory noise, uncertainty, and high spatial complexity. On the other hand, robotics is unconcerned with understanding biological cognition. Here, we describe a computational framework for robotic architectures aiming to function in realistic environments, as well as to be cognitively plausible. We motivate and describe several mechanisms towards achieving this despite the sensory noise and spatial complexity inherent in the physical world. We tackle error accumulation during path integration by means of Bayesian localization, and loop closing with sequential gradient descent. Finally, we outline a method for structuring spatial representations using metric learning and clustering. Crucially, unlike the algorithms of traditional robotics, we show that these mechanisms can be implemented in neuronal or cognitive models. We briefly outline a concrete implementation of the proposed framework as part of the LIDA cognitive architecture, and argue that this kind of probabilistic framework is well-suited for use in cognitive robotic architectures aiming to combine spatial functionality and psychological plausibility.},
author = {Madl, Tamas and Franklin, Stan and Chen, Ke and Trappl, Robert},
doi = {10.1016/j.cogsys.2017.08.002},
file = {::},
issn = {13890417},
journal = {Cognitive Systems Research},
keywords = {Bayesian brain,Cognitive architecture,Computational cognitive modeling,LIDA,Spatial memory},
pages = {147--172},
publisher = {Elsevier B.V.},
title = {{A computational cognitive framework of spatial memory in brains and robots}},
url = {https://doi.org/10.1016/j.cogsys.2017.08.002},
volume = {47},
year = {2018}
}
@article{Konidaris2018,
abstract = {{\textless}p{\textgreater}We consider the problem of constructing abstract representations for planning in high-dimensional, continuous environments. We assume an agent equipped with a collection of high-level actions, and construct representations provably capable of evaluating plans composed of sequences of those actions. We first consider the deterministic planning case, and show that the relevant computation involves set operations performed over sets of states. We define the specific collection of sets that is necessary and sufficient for planning, and use them to construct a grounded abstract symbolic representation that is provably suitable for deterministic planning. The resulting representation can be expressed in PDDL, a canonical high-level planning domain language; we construct such a representation for the Playroom domain and solve it in milliseconds using an off-the-shelf planner. We then consider probabilistic planning, which we show requires generalizing from sets of states to distributions over states. We identify the specific distributions required for planning, and use them to construct a grounded abstract symbolic representation that correctly estimates the expected reward and probability of success of any plan. In addition, we show that learning the relevant probability distributions corresponds to specific instances of probabilistic density estimation and probabilistic classification. We construct an agent that autonomously learns the correct abstract representation of a computer game domain, and rapidly solves it. Finally, we apply these techniques to create a physical robot system that autonomously learns its own symbolic representation of a mobile manipulation task directly from sensorimotor data---point clouds, map locations, and joint angles---and then plans using that representation. Together, these results establish a principled link between high-level actions and abstract representations, a concrete theoretical foundation for constructing abstract representations with provable properties, and a practical mechanism for autonomously learning abstract high-level representations.{\textless}/p{\textgreater}},
author = {Konidaris, George and Kaelbling, Leslie Pack and Lozano-Perez, Tomas},
doi = {10.1613/jair.5575},
file = {::},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {215--289},
title = {{From skills to symbols: Learning symbolic representations for abstract high-level planning}},
volume = {61},
year = {2018}
}
@article{Pellom2001,
author = {Pellom, B. and Ward, W. and Hansen, J. and Cole, R. and Hacioglu, K. and Zhang, J. and Yu, X. and Pradhan, S.},
doi = {10.3115/1072133.1072225},
journal = {Proceedings of the first international conference on Human language technology research - HLT '01},
pages = {1--6},
title = {{University of Colorado dialog systems for travel and navigation}},
url = {http://portal.acm.org/citation.cfm?doid=1072133.1072225},
year = {2001}
}
@inproceedings{Hengst2002,
abstract = {An open problem in reinforcement learning is discovering hierarchical structure. HEXQ, an algorithm which automatically attempts to decompose and solve a model-free fac- tored MDP hierarchically is described. By searching for aliased Markov sub-space re- gions based on the state variables the algo- rithm uses temporal and state abstraction to construct a hierarchy of interlinked smaller MDPs.},
author = {Hengst, Bernhard},
booktitle = {Proceedings of the International Conference on Machine Learning ICML 2002},
doi = {10.1.1.9.5839},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the International Conference on Machine Learning ICML 2002/Hengst/Hengst - 2002 - Discovering hierarchy in reinforcement learning with HEXQ.pdf:pdf},
isbn = {1-55860-873-7},
pages = {243--250},
title = {{Discovering hierarchy in reinforcement learning with HEXQ}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.5839},
year = {2002}
}
@article{Erdem2014,
abstract = {We propose an extended version of our previous goal directed navigation model based on forward planning of trajectories in a network of head direction cells, persistent spiking cells, grid cells, and place cells. In our original work the animat incrementally creates a place cell map by random exploration of a novel environment. After the exploration phase, the animat decides on its next movement direction towards a goal by probing linear look-ahead trajectories in several candidate directions while stationary and picking the one activating place cells representing the goal location. In this work we present several improvements over our previous model. We improve the range of linear look-ahead probes significantly by imposing a hierarchical structure on the place cell map consistent with the experimental findings of differences in the firing field size and spacing of grid cells recorded at different positions along the dorsal to ventral axis of entorhinal cortex. The new model represents the environment at different scales by populations of simulated hippocampal place cells with different firing field sizes. Among other advantages this model allows simultaneous constant duration linear look-ahead probes at different scales while significantly extending each probe range. The extension of the linear look-ahead probe range while keeping its duration constant also limits the degrading effects of noise accumulation in the network. We show the extended model's performance using an animat in a large open field environment. {\textcopyright} 2013 Elsevier Ltd.},
author = {Erdem, Uǧur M. and Hasselmo, Michael E.},
doi = {10.1016/j.jphysparis.2013.07.002},
file = {::},
isbn = {1769-7115 (Electronic)$\backslash$r0928-4257 (Linking)},
issn = {09284257},
journal = {Journal of Physiology Paris},
keywords = {Entorhinal cortex,Grid cell,Hippocampus,Navigation,Place cell},
number = {1},
pages = {28--37},
pmid = {23891644},
title = {{A biologically inspired hierarchical goal directed navigation model}},
volume = {108},
year = {2014}
}
@inproceedings{Kumar2017,
abstract = {We present a framework combining hierarchical and multi-agent deep reinforcement learning approaches to solve coordination problems among a multitude of agents using a semi-decentralized model. The framework extends the multi-agent learning setup by introducing a meta-controller that guides the communication between agent pairs, enabling agents to focus on communicating with only one other agent at any step. This hierarchical decomposition of the task allows for efficient exploration to learn policies that identify globally optimal solutions even as the number of collaborating agents increases. We show promising initial experimental results on a simulated distributed scheduling problem.},
archivePrefix = {arXiv},
arxivId = {1712.08266},
author = {Kumar, Saurabh and Shah, Pararth and Hakkani-Tur, Dilek and Heck, Larry},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
eprint = {1712.08266},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Kumar et al/Kumar et al. - 2017 - Federated Control with Hierarchical Multi-Agent Deep Reinforcement Learning.pdf:pdf},
title = {{Federated Control with Hierarchical Multi-Agent Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1712.08266},
year = {2017}
}
@article{Panov2016d,
abstract = {В работе исследуется задача автоматического планирования в контексте более общей проблемы создания интеллектуальных систем управления сложными техническими объектами (мобильными роботами, беспилотными транспортными средствами и др.). Основное внимание уделяется задачам, которые не могут быть решены без взаимодействия методов стратегического (символьного) и тактического (субсимвольного) планирования. В работе предлагаются оригинальные методы планирования на символьном (психологически правдоподобное планирование поведения агента) и субсимвольном (планирование траектории) уровнях. Описываются способы их взаимной увязки и возникающие прямые и обратные связи. Приводится описание применения указанных алгоритмов на модельном примере задачи коллективного перемещения в динамической среде с разрушаемыми препятствиями.},
author = {Панов, А. И. and Яковлев, К. С.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Искусственный интеллект и принятие решений/Панов, Яковлев/Панов, Яковлев - 2016 - Взаимодействие стратегического и тактического планирования поведения коалиций агентов в динамической среде.doc:doc;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Искусственный интеллект и принятие решений/Панов, Яковлев/Панов, Яковлев - 2016 - Взаимодействие стратегического и тактического планирования поведения коалиций агентов в динамической среде.pdf:pdf},
journal = {Искусственный интеллект и принятие решений},
keywords = {15-37-20893,16-37-60055,A*,JPS,LIAN,elibrary,mypub,vak,автоматическое планирование,знаковая картина мира,значение,интеллектуальное планирование,личностный смысл,образ,планирование,планирование поведения,планирование траектории,эвристический поиск},
language = {russian},
mendeley-tags = {15-37-20893,16-37-60055,elibrary,mypub,vak},
number = {4},
pages = {68--78},
title = {{Взаимодействие стратегического и тактического планирования поведения коалиций агентов в динамической среде}},
url = {http://aidt.ru/index.php?option=com{\_}content{\&}view=article{\&}id=740:a-i-panov-k-s-yakovlev-vzaimodejstvie-strategicheskogo-i-takticheskogo-planirovaniya-povedeniya-koalitsii-agentov-v-dinamicheskoj-srede{\&}catid=291:modelirovanie-povedeniya-i-upravlenie{\&}Itemid=},
year = {2016}
}
@article{Dayan1993,
abstract = {One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time. This paper shows how to create a Q-learning managerial hierarchy in which high level managers learn how to set tasks to their sub-managers who, in turn, learn how to satisfy them. Sub-managers need not initially understand their managers' commands. They simply learn to maximise their reinforcement in the context of the current command. We illustrate the system using a simple maze task.. As the system learns how to get around, satisfying commands at the multiple levels, it explores more efficiently than standard, flat, Q-learning and builds a more comprehensive map.},
author = {Dayan, Peter and Hinton, Geoffrey},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Advances in neural information processing systems/Dayan, Hinton/Dayan, Hinton - 1993 - Feudal Reinforcement Learning.pdf:pdf},
isbn = {1-55860-274-7},
issn = {1049-5258},
journal = {Advances in neural information processing systems},
keywords = {Reinforcement Learning},
pages = {271--278},
title = {{Feudal Reinforcement Learning}},
url = {http://www.cs.utoronto.ca/{~}hinton/absps/dh93.pdf},
year = {1993}
}
@article{Co-Reyes2018a,
abstract = {In this work, we take a representation learning perspective on hierarchical reinforcement learning, where the problem of learning lower layers in a hierarchy is transformed into the problem of learning trajectory-level generative models. We show that we can learn continuous latent representations of trajectories, which are effective in solving temporally extended and multi-stage problems. Our proposed model, SeCTAR, draws inspiration from variational autoencoders, and learns latent representations of trajectories. A key component of this method is to learn both a latent-conditioned policy and a latent-conditioned model which are consistent with each other. Given the same latent, the policy generates a trajectory which should match the trajectory predicted by the model. This model provides a built-in prediction mechanism, by predicting the outcome of closed loop policy behavior. We propose a novel algorithm for performing hierarchical RL with this model, combining model-based planning in the learned latent space with an unsupervised exploration objective. We show that our model is effective at reasoning over long horizons with sparse rewards for several simulated tasks, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration.},
archivePrefix = {arXiv},
arxivId = {1806.02813},
author = {Co-Reyes, John D. and Liu, YuXuan and Gupta, Abhishek and Eysenbach, Benjamin and Abbeel, Pieter and Levine, Sergey},
eprint = {1806.02813},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Co-Reyes et al/Co-Reyes et al. - 2018 - Self-Consistent Trajectory Autoencoder Hierarchical Reinforcement Learning with Trajectory Embeddings.pdf:pdf},
issn = {1938-7228},
title = {{Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings}},
url = {http://arxiv.org/abs/1806.02813},
year = {2018}
}
@book{Leontiev1977,
address = {М.},
author = {Леонтьев, Алексей Николаевич},
edition = {Изд. 2-е},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Леонтьев/Леонтьев - 1977 - Деятельность. Сознание. Личность.pdf:pdf},
keywords = {psycho},
language = {russian},
mendeley-tags = {psycho},
pages = {304},
publisher = {Политиздат},
title = {{Деятельность. Сознание. Личность}},
year = {1977}
}
@book{Szepesvari2010,
abstract = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective.What distin- guishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the al- gorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book,we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming.We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.},
author = {Szepesv{\'{a}}ri, Csaba},
booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
doi = {10.2200/S00268ED1V01Y201005AIM009},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Synthesis Lectures on Artificial Intelligence and Machine Learning/Szepesv{\'{a}}ri/Szepesv{\'{a}}ri - 2010 - Algorithms for Reinforcement Learning.pdf:pdf},
isbn = {9781608454921},
issn = {1939-4608},
number = {1},
pages = {1--103},
title = {{Algorithms for Reinforcement Learning}},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009},
volume = {4},
year = {2010}
}
@article{Epstein2013,
abstract = {This paper describes how a cognitive architecture builds a spatial model and navigates from it without a map. Each con- structed model is a collage of spatial affordances that de- scribes how the environment has been sensed and traversed. The system exploits the evolving model while it directs an agent to explore the environment. Effective models are learned quickly during travel. Moreover, when combined with simple heuristics, the learned spatial model supports effective navigation. In three simple environments, these learned mod- els describe space in ways familiar to people, and often pro- duce near-optimal travel times.},
author = {Epstein, Susan L. and Aroor, Anoop and Sklar, Elizabeth I. and Parsons, Simon},
file = {::},
keywords = {affordances,cognitive architecture,exploration,learning,spatial,spatial cognition},
pages = {1--6},
title = {{Navigation with Learned Spatial Affordances}},
url = {http://www.compsci.hunter.cuny.edu/{~}epstein/papers/CogSciFinal.Epstein.pdf},
year = {2013}
}
@article{Panov2015g,
abstract = {Рассматриваются особенности представления пространственных и временных знаний для планирования согласованного перемещения коалиции интеллектуальных агентов. В качестве примера подобной задачи рассмотрен перелет группы беспилотных летательных аппаратов в заданную область на местности с препятствиями. Рассматривается знаковый подход к описанию знаний агента, основанный на психологической теории деятельности и биологически правдоподобной иерархической модели распознавания. Особое внимание уделено описанию действий и процессам обучения на основе поступающей первичной информации.},
author = {Панов, А. И.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Робототехника и техническая кибернетика/Панов/Панов - 2015 - Представление знаний автономных агентов, планирующих согласованные перемещения.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Робототехника и техническая кибернетика/Панов/Панов - 2015 - Представление знаний автономных агентов, планирующих согласованные перемещения.docx:docx},
journal = {Робототехника и техническая кибернетика},
keywords = {14-07-31194,elibrary,mypub,группа БПЛА,знак,иерархия автоматов,когнитивные процессы,представление знаний},
language = {russian},
mendeley-tags = {14-07-31194,elibrary,mypub},
number = {4(9)},
pages = {34--40},
title = {{Представление знаний автономных агентов, планирующих согласованные перемещения}},
url = {https://elibrary.ru/item.asp?id=25085187},
year = {2015}
}
@article{Hayes2016,
abstract = {Collaboration between humans and robots requires solutions to an array of challenging problems, including multi-agent planning, state estimation, and goal inference. There already exist feasible solutions for many of these challenges, but they depend upon having rich task models. In this work we detail a novel type of Hierarchical Task Network we call a Clique/Chain HTN (CC-HTN), alongside an algorithm for autonomously constructing them from topological properties derived from graphical task representations. As the presented method relies on the structure of the task itself, our work imposes no particular type of symbolic insight into motor primitives or environmental representation, making it applicable to a wide variety of use cases critical to human-robot interaction. We present evaluations within a multi-resolution goal inference task and a transfer learning application showing the utility of our approach.},
author = {Hayes, Bradley and Scassellati, Brian},
doi = {10.1109/ICRA.2016.7487760},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings - IEEE International Conference on Robotics and Automation/Hayes, Scassellati/Hayes, Scassellati - 2016 - Autonomously constructing hierarchical task networks for planning and human-robot collabor.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {5469--5476},
title = {{Autonomously constructing hierarchical task networks for planning and human-robot collaboration}},
year = {2016}
}
@article{Garrett2015,
abstract = {In this paper we address planning problems in high-dimensional hybrid configuration spaces, with a particular focus on manipulation planning problems involving many objects. We present the hybrid backward-forward (HBF) planning algorithm that uses a backward identification of constraints to direct the sampling of the infinite action space in a forward search from the initial state towards a goal configuration. The resulting planner is probabilistically complete and can effectively construct long manipulation plans requiring both prehensile and nonprehensile actions in cluttered environments.},
author = {Garrett, Caelan Reed and Lozano-P{\'{e}}rez, Tom{\'{a}}s and Kaelbling, Leslie Pack},
doi = {10.1109/IROS.2015.7354287},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IEEE International Conference on Intelligent Robots and Systems/Garrett, Lozano-P{\'{e}}rez, Kaelbling/Garrett, Lozano-P{\'{e}}rez, Kaelbling - 2015 - Backward-forward search for manipulation planning.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Heuristic algorithms,Intelligent robots,Ovens,Planning,Probabilistic logic,Search problems},
number = {grant 1420927},
pages = {6366--6373},
title = {{Backward-forward search for manipulation planning}},
volume = {2015-Decem},
year = {2015}
}
@article{Kaplan2004,
abstract = {This chapter presents a generic internal reward system that drives an agent to increase the complexity of its behavior. This reward system does not reinforce a predefined task. Its purpose is to drive the agent to progress in learning given its embodiment and the environment in which it is placed. The dynamics created by such a system are studied first in a simple environment and then in the context of active vision.},
author = {Kaplan, Frederic and Oudeyer, Pierre-Yves},
doi = {10.1007/b99075},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Embodied artificial intelligence/Kaplan, Oudeyer/Kaplan, Oudeyer - 2004 - Maximizing learning progress An internal reward system for development.pdf:pdf},
isbn = {3-540-22484-X},
issn = {03029743},
journal = {Embodied artificial intelligence},
keywords = {Active vision,Artificial intelligence,Reward},
pages = {259--270},
title = {{Maximizing learning progress: An internal reward system for development}},
volume = {3139},
year = {2004}
}
@article{Vien2016,
abstract = {Reinforcement learning (RL) is an area of machine learning that is concerned with how an agent learns to make decisions sequentially in order to optimize a par-ticular performance measure. For achieving such a goal, the agent has to choose either 1) exploiting previously known knowledge that might end up at local optimality or 2) exploring to gather new knowledge that expects to improve the current performance. Among other RL algo-rithms, Bayesian model-based RL (BRL) is well-known to be able to trade-off between exploitation and explo-ration optimally via belief planning, i.e. partially observ-able Markov decision process (POMDP). However, solving that POMDP often suffers from curse of dimensionality and curse of history. In this paper, we make two major contributions which are: 1) an integration framework of temporal abstraction into BRL that eventually results in a hierarchical POMDP formulation, which can be solved online using a hierarchical sample-based planning solver; 2) a subgoal discovery method for hierarchical BRL that automatically discovers useful macro actions to accelerate learning. In the experiment section, we demonstrate that the proposed approach can scale up to much larger prob-lems. On the other hand, the agent is able to discover useful subgoals for speeding up Bayesian reinforcement learning.},
author = {Vien, Ngo Anh and Lee, Seung Gwan and Chung, Tae Choong},
doi = {10.1007/s10489-015-0742-2},
file = {::},
issn = {15737497},
journal = {Applied Intelligence},
keywords = {Bayesian reinforcement learning,Hierarchical Monte-Carlo planning,Hierarchical reinforcement learning,MDP,Monte-Carlo tree search,POMDP,POSMDP,Reinforcement learning},
number = {1},
pages = {112--126},
publisher = {Applied Intelligence},
title = {{Bayes-adaptive hierarchical MDPs}},
url = {http://dx.doi.org/10.1007/s10489-015-0742-2},
volume = {45},
year = {2016}
}
@article{DeCarolis2001,
abstract = {The aim of our research is to build a Reflexive Agent, that is able to either manifest an emotion it is feeling or to hide it. If the Agent decides to manifest its emotion, it can establish what verbal or nonverbal signals to employ communication in its and how to combine and synchronize them. In the decision of whether to express an emotion in a given context, a number of factors are considered, such as the Agent's own personality and goals, the Interlocutor's characteristics and the context. In planning how to communicate an emotion, various factors are considered as well: the available modalities (face, gaze, voice etc); the cognitive ease in producing and processing the various the expressiveness of every signal in communicating specific meanings; and, finally, the appropriateness of signals to social situations.},
author = {{De Carolis}, Berardina and Pelachaud, Catherine and Poggi, Isabella and {De Rosis}, Fiorella},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IJCAI International Joint Conference on Artificial Intelligence/De Carolis et al/De Carolis et al. - 2001 - Behavior planning for a reflexive agent.pdf:pdf},
isbn = {1-55860-812-5, 978-1-558-60812-2},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {1059--1064},
title = {{Behavior planning for a reflexive agent}},
year = {2001}
}
@article{Andre2002,
abstract = {Hierarchical reinforcement learning ALisp - a language to support represent hierarchical model. The authors present way to organize information in hierarchical fashion. In which, at each level, each state contains only necessary information for RL, and speed up learning process then. That state is called abstract state in the paper.},
author = {Andre, David and Russell, Stuart},
file = {::},
isbn = {0-262-51129-0},
issn = {1049-5258},
journal = {Aaai},
number = {Dietterich 2000},
pages = {119--125},
title = {{State Abstraction for Programmable Reinforcement Learning Agents}},
url = {http://www.aaai.org/Papers/AAAI/2002/AAAI02-019.pdf},
year = {2002}
}
@article{Vezhnevets2017a,
abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.},
archivePrefix = {arXiv},
arxivId = {1703.01161},
author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
eprint = {1703.01161},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Vezhnevets et al/Vezhnevets et al. - 2017 - FeUdal Networks for Hierarchical Reinforcement Learning.pdf:pdf},
issn = {1938-7228},
title = {{FeUdal Networks for Hierarchical Reinforcement Learning}},
url = {http://arxiv.org/abs/1703.01161},
year = {2017}
}
@article{Vig2007,
author = {Vig, Lovekesh and Adams, Julie a.},
doi = {10.1007/s10846-007-9150-0},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Journal of Intelligent and Robotic Systems/Vig, Adams/Vig, Adams - 2007 - Coalition Formation From Software Agents to Robots.pdf:pdf},
issn = {0921-0296},
journal = {Journal of Intelligent and Robotic Systems},
number = {1},
pages = {85--118},
title = {{Coalition Formation: From Software Agents to Robots}},
url = {http://link.springer.com/10.1007/s10846-007-9150-0},
volume = {50},
year = {2007}
}
@inproceedings{Alexander2016,
abstract = {We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub- sequences by learning for how long the plan can be committed to - i.e. followed without re-planing. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro- actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.},
archivePrefix = {arXiv},
arxivId = {1606.04695},
author = {Vezhnevets, Alexander and Mnih, Volodymyr and Agapiou, John and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Kavukcuoglu, Koray},
booktitle = {Proceeding of NIPS 2016},
doi = {10.3847/0004-637X/832/1/56},
eprint = {1606.04695},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/arXiv/Alexander et al/Alexander et al. - 2016 - Strategic Attentive Writer for Learning Macro-Actions.pdf:pdf},
issn = {10495258},
title = {{Strategic Attentive Writer for Learning Macro-Actions}},
url = {http://arxiv.org/abs/1606.04695},
year = {2016}
}
@article{Bai2017a,
abstract = {In the context of hierarchical reinforcement learn-ing, the idea of hierarchies of abstract machines (HAMs) is to write a partial policy as a set of hierar-chical finite state machines with unspecified choice states, and use reinforcement learning to learn an optimal completion of this partial policy. Given a HAM with deep hierarchical structure, there of-ten exist many internal transitions where a machine calls another machine with the environment state unchanged. In this paper, we propose a new hier-archical reinforcement learning algorithm that au-tomatically discovers such internal transitions, and shortcircuits them recursively in the computation of Q values. The resulting HAMQ-INT algorithm outperforms the state of the art significantly on the benchmark Taxi domain and a much more complex RoboCup Keepaway domain.},
author = {Bai, Aijun and Russell, Stuart},
file = {::},
isbn = {9780999241103},
issn = {10450823},
journal = {Proc. of the 26th International Joint Conference on Artificial Intelligence},
keywords = {2017,HAM,HRL,Machine Learning: Reinforcement Learning,Planning and Scheduling: Markov Decisions Processe,RL,Robotics and Vision: Multi-Robot Systems,Robotics and Vision: Robotics},
mendeley-tags = {2017,HAM,HRL,RL},
pages = {1418--1424},
title = {{Efficient Reinforcement Learning with Hierarchies of Machines by Leveraging Internal Transitions}},
year = {2017}
}
@article{Fang2014,
author = {Fang, Rui and Doering, Malcolm and Chai, Jy},
isbn = {9781577356783},
journal = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
keywords = {NLP and Machine Learning},
number = {Dale 1995},
pages = {1544--1550},
title = {{Collaborative Models for Referring Expression Generation in Situated Dialogue}},
url = {http://cse.msu.edu/{\%}7B{~}{\%}7Dfangrui/Papers/aaai2014-REG.pdf},
year = {2014}
}
@article{Digney1998,
abstract = {While the need for hierarchies within control systems is apparent, it is also clear to many researchers that such hierarchies should be learned. Learning both the structure and the component behaviors is a difficult task. The benefit of learning the hierarchical structures of behaviors is that the decomposition of the control structure into smaller transportable chunks allows previously learned knowledge to be applied to new but related tasks. Presented in this paper are improvements to Nested Q-learning (NQL) that allow more realistic learning of control hierarchies in reinforcement environments. Also presented is a simulation of a simple robot performing a series of related tasks that is used to compare both hierarchical and non-hierarchal learning techniques.},
author = {Digney, Bruce L},
file = {::},
isbn = {0-262-66144-6},
journal = {From Animals to Animats 5: Proceedings of the Fifth International Conference on the Simulation of Adaptive Behavior},
keywords = {Options,hierarchical skills,sub-goal,subgoal},
pages = {321--330},
title = {{Learning Hierarchical Control Structures for Multiple Tasks and Changing Environments}},
volume = {5},
year = {1998}
}
@article{Lee2018a,
abstract = {Value Iteration Networks (VINs) are effective differentiable path planning modules that can be used by agents to perform navigation while still maintaining end-to-end differentiability of the entire architecture. Despite their effectiveness, they suffer from several disadvantages including training instability, random seed sensitivity, and other optimization problems. In this work, we reframe VINs as recurrent-convolutional networks which demonstrates that VINs couple recurrent convolutions with an unconventional max-pooling activation. From this perspective, we argue that standard gated recurrent update equations could potentially alleviate the optimization issues plaguing VIN. The resulting architecture, which we call the Gated Path Planning Network, is shown to empirically outperform VIN on a variety of metrics such as learning speed, hyperparameter sensitivity, iteration count, and even generalization. Furthermore, we show that this performance gap is consistent across different maze transition types, maze sizes and even show success on a challenging 3D environment, where the planner is only provided with first-person RGB images.},
archivePrefix = {arXiv},
arxivId = {1806.06408},
author = {Lee, Lisa and Parisotto, Emilio and Chaplot, Devendra Singh and Xing, Eric and Salakhutdinov, Ruslan},
eprint = {1806.06408},
file = {::},
issn = {1938-7228},
title = {{Gated Path Planning Networks}},
url = {http://arxiv.org/abs/1806.06408},
year = {2018}
}
@article{Chai2001,
abstract = {This paper describes a web-based dialog system – Natural Language Sales Assistant (NLSA) – that helps users find relevant information about products and services in e-commerce sites. The system leverages technologies in natural language processing and human computer interaction to create a faster and more intuitive way of interacting with websites. By combining traditional AI rule-based technology with taxonomy mapping, the system is able to accommodate both customer and business requirements. Our user studies have demonstrated that, in the context of e-commerce, users preferred the natural language enabled navigation over menu-driven navigation (79{\%} to 21{\%} users). In addition, compared to a menu driven system, the average number of clicks used in the natural language system was reduced by 63.2{\%} and the average time was reduced by 33.3{\%}. The NLSA system is currently deployed by IBM as a live pilot and we are collecting real user feedback. We believe that conversational interfaces like that of NLSA offer the ultimate personalization and can greatly enhance the user experience for websites.},
author = {Chai, Joyce Yue and Budzikowska, Malgorzata and Horvath, Veronika and Nicolov, Nicolas and Kambhatla, Nanda and Zadrozny, Wlodek and Stys, Margo and Kambhatla, Nanda and Zadrozny, Wlodek and Melville, Prem},
doi = {10.1145/345124.345164},
journal = {Iaai},
keywords = {engines usually require that,however,indexing,jargon so that the,keyword,keyword search,keywords could possibly match,product catalog or documents,search capabilities,terms used in the,users know domain specific},
number = {2},
pages = {19--26},
title = {{Natural Language Sales Assistant-A Web-Based Dialog System for Online Sales.}},
volume = {23},
year = {2001}
}
@article{Li2013a,
abstract = {Simultaneous localization and consistent mapping in dynamic environments is a fundamental and unsolved problem in the mobile robotics community. Most of the algorithms for this problem heavily rely on discriminating dynamic objects from static objects. Because these recursive filters based discrimination algorithms always have lag before the model selection parameters converge to the steady states, they have a period of time that the filter could identify a dynamic target as static or vice versa. Mis-classifications decrease precision and consistence, and induce filter divergence.A brain interacts with dynamic environments. The biological basis of this adaptability is provided by the connectivity and the dynamic properties of neurons. Biologically inspired by the adaptability, the paper proposes a shunting STM (Short Term Memory) based method to solve the simultaneous localization and consistent mapping problem, especially in dynamic environments. The proposed method utilizes a shunting STM neural network to represent environments and to probabilistically reflect the probability of existence of an object; it adapts a scan matching scheme to localize robot based on the map representation. Dynamic properties of the neural network are used to reflect environmental changes, therefore, the proposed method does not require explicit discrimination of objects. As a result, the proposed method does not have the lag of convergence, and it has high utilization ratio of observation information. Theoretical analyses in the paper show the proposed method has Lyapunov stability and its computational complexity does not depend on the size of the environment. The paper compares the proposed method with the classification based Extend Kalman Filter on a classical outdoor dataset, in simulated environments and in real indoor environments. The results show the proposed method outperforms the classification based EKF on precision and consistence in both static environments and dynamic environments. {\textcopyright} 2012 Elsevier B.V..},
author = {Li, Yangming and Li, Shuai and Ge, Yunjian},
doi = {10.1016/j.neucom.2012.10.011},
file = {::},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Biological inspiration,Dynamic environment,Scan matching,Shunting short term memory model,Simultaneous localization and consistent mapping},
pages = {170--179},
publisher = {Elsevier},
title = {{A biologically inspired solution to simultaneous localization and consistent mapping in dynamic environments}},
url = {http://dx.doi.org/10.1016/j.neucom.2012.10.011},
volume = {104},
year = {2013}
}
@article{Dietterich1998,
abstract = {This paper presents a new approach to hier- archical reinforcement learning based on the MAXQ decomposition of the value function. The MAXQ decomposition has both a procedu- ral semantics—as a subroutine hierarchy—and a declarative semantics—as a representation of the value function of a hierarchical policy. MAXQ unifies and extends previouswork on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. Conditions under which the MAXQ decomposition can represent the optimal value function are derived. The paper defines a hierarchicalQlearning algorithm, proves its con- vergence, and shows experimentally that it can learn much faster than ordinary “flat” Q learn- ing. Finally, the paper discusses some interest- ing issues that arise in hierarchical reinforcement learning including the hierarchical credit assign- ment problem and non-hierarchical execution of theMAXQ hierarchy.},
author = {Dietterich, Thomas G},
file = {::},
isbn = {1-55860-556-8},
journal = {Proc. of the fifteenth international conference on machine learning},
number = {c},
pages = {118--126},
title = {{The MAXQ Method for Hierarchical Reinforcement Learning}},
year = {1998}
}
@article{Fikes1971,
abstract = {We describe a new problem solver called STRIPS that attempts to find a sequence of operators in a space of world models to transform a given initial world model in which a given goal formula can be proven to be true. STRIPS represents a world model as an arbitrary collection in first-order predicate calculus formulas and is designed to work with models consisting of large numbers of formula. It employs a resolution theorem prover to answer questions of particular models and uses means-ends analysis to guide it to the desired goal-satisfying model.},
author = {Fikes, Richard E. and Nilsson, Nils J.},
doi = {10.1016/0004-3702(71)90010-5},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Artificial Intelligence/Fikes, Nilsson/Fikes, Nilsson - 1971 - STRIPS A new approach to the application of theorem proving to problem solving.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {plan},
mendeley-tags = {plan},
number = {3-4},
pages = {189--208},
title = {{STRIPS: A new approach to the application of theorem proving to problem solving}},
volume = {2},
year = {1971}
}
@article{Johnston2013,
abstract = {We explore the plausibility of using automated spoken dialog systems (SDS) for administer- ing survey interviews. Because the goals of a survey dialog system differ from more tradi- tional information-seeking and transactional applications, different measures of task accu- racy and success may be warranted. We report a large-scale experimental evaluation of an SDS that administered survey interviews with questions drawn from government and social scientific surveys. We compare two dialog confirmation strategies: (1) a traditional strate- gy of explicit confirmation on low-confidence recognition; and (2) no confirmation. With ex- plicit confirmation, the small percentage of re- sidual errors had little to no impact on survey data measurement. Even without confirmation, while there are significantly more errors, im- pact on the substantive conclusions of the sur- vey is still very limited.},
author = {Johnston, Michael and Ehlen, Patrick and Conrad, Frederick G and Schober, Michael F and Antoun, Christopher and Fail, Stefanie and Hupp, Andrew and Vickers, Lucas and Yan, Huiying and Zhang, Chan},
isbn = {9781937284954},
journal = {Proceedings of the SIGdial 2013 Conference: The 14th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
number = {August},
pages = {329--333},
title = {{Spoken Dialog Systems for Automated Survey Interviewing}},
url = {http://mfschober.net/Johnstonetal13.pdf},
year = {2013}
}
@article{Gupta2017,
abstract = {We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the planner, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. Our experiments demonstrate that CMP outperforms both reactive strategies and standard memory-based architectures and performs well in novel environments. Furthermore, we show that CMP can also achieve semantically specified goals, such as "go to a chair".},
archivePrefix = {arXiv},
arxivId = {1702.03920},
author = {Gupta, Saurabh and Davidson, James and Levine, Sergey and Sukthankar, Rahul and Malik, Jitendra},
eprint = {1702.03920},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/ArXiv 1702.03920/Gupta et al/Gupta et al. - 2017 - Cognitive Mapping and Planning for Visual Navigation.pdf:pdf},
journal = {ArXiv: 1702.03920},
month = {feb},
title = {{Cognitive Mapping and Planning for Visual Navigation}},
url = {http://arxiv.org/abs/1702.03920},
year = {2017}
}
@misc{Presentation2016b,
author = {Панов, А. И.},
file = {::},
pages = {23},
title = {{Теория когнитома Анохина}},
year = {2016}
}
@inproceedings{Panov2012c,
abstract = {В работе представлена архитектура интеллектуального агента, поведение которого моделирует процесс принятия решения специалистом в некоторой предметной области. Знания агента хранятся в знаковой форме, коммуникации агентов между собой осуществляются с помощью знакового опосредования. Описывается система проведения экспериментов с такими интеллектуальными агентами, реализованная на основе мультиагентной системы Jadex. Также представлен анализ данных серии экспермиентов, проведенных в построенной системе для интеллектуальных агентов, моделирующих процесс интерпретации жалоб психологом-консультантом.},
address = {Рыбинск},
author = {Панов, А. И.},
booktitle = {Теория и практика системного анализа: Труды II Всероссийской научной конференции молодых учёных с международным участием},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Теория и практика системного анализа Труды II Всероссийской научной конференции молодых учёных с международным участием/Панов/Панов - 2012 - Моделирование процесса принятия решения агентом со знаковой картино.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Теория и практика системного анализа Труды II Всероссийской научной конференции молодых учёных с международным участием/Панов/Панов - 2012 - Моделирование процесса принятия решения агентом со знаковой карт(2).pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Теория и практика системного анализа Труды II Всероссийской научной конференции молодых учёных с международным участием/Панов/Панов - 2012 - Моделирование процесса принятия решения агентом со знаковой картино.doc:doc},
keywords = {myconf},
language = {russian},
mendeley-tags = {myconf},
pages = {126--137},
publisher = {РГАТУ имени П.А. Соловьева},
title = {{Моделирование процесса принятия решения агентом со знаковой картиной мира}},
volume = {I},
year = {2012}
}
@article{Woudenberg2014,
abstract = {Conversational software, that is software with which a user can con- verse in a natural language such as English or Dutch, can be classified into two distinct categories: chatbots and dialogue systems. Chatbots work according to the principle of pattern matching where user input is matched to a fixed response. In dialogue systems, user input is parsed into some semantical rep- resentation. This representation is then used by a component called a dialogue manager to determine what the response should be. A dia- logue manager maintains a state of the conversation by tracking who spoke last, what knowledge is private and shared, the current goal, what plan should be followed to resolve an open issue and how much of the user input was understood. Both categories of systems have their advantages and disadvan- tages. Chatbot systems are trivial to build and maintain, but are too simplistic for applications that do more than answering frequently asked questions. Dialogue systems on the other hand are harder to develop and maintain, can deal with less variety in user input, but are capable of handling and generating all kinds of linguistic phenomena such as grounding and information revision. This thesis presents a system that combines ideas from chatbots and dialogue system. This hybrid system doesn't parse natural language into a semantic presentation, but contains a dialogue manager that can handle natural language directly. This system would make it easy to implement and maintain new domains even when the developer has little or no knowledge of computational linguistics. The system would also be able to deal with a variety of linguistic phenomena. A statistics tutor has been implemented using this hybrid system with which three students interacted. This shows that it's reasonably easy to build and maintain new domains using this approach. It is also shown that the statistics tutor is able to deal with phenomena such as grounding, question accommodation and information revision.},
author = {Woudenberg, A F Van},
title = {{A Chatbot Dialogue Manager Chatbots and Dialogue Systems : A Hybrid Approach}},
year = {2014}
}
@article{Daniel2016,
abstract = {Tasks that require many sequential decisions are hard to solve using conventional rein- forcement learning algorithms. Based on the option framework, we propose a model which aims to alleviate these concerns. Instead of learning a single monolithic policy, the agent learns a set of simpler sub-policies as well as the initiation and termination probabili- ties for each of those sub-policies. While ex- isting option learning algorithms frequently require manual specification of components such as the sub-policies, we present an algo- rithm which infers all relevant components of the option framework from data. We present results on SMDPs with discrete as well as continuous state-action spaces. The results show that the presented algorithm can combine simple sub-policies to solve complex tasks and can improve learning performance on simpler tasks.},
author = {Daniel, Christian and van Hoof, Herke and Peters, Jan and Neumann, Gerhard},
doi = {10.1007/s10994-016-5580-x},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Machine Learning/Daniel et al/Daniel et al. - 2016 - Probabilistic inference for determining options in reinforcement learning.pdf:pdf},
issn = {15730565},
journal = {Machine Learning},
keywords = {Options,Reinforcement learning,Robot learning,Semi Markov decision process},
number = {2-3},
pages = {337--357},
publisher = {Springer US},
title = {{Probabilistic inference for determining options in reinforcement learning}},
volume = {104},
year = {2016}
}
@article{Gorodetsky2007,
abstract = {Данная работа представляет реализованную авторами P2P агентскую плат- форму, экземпляры которой, установленные в узлах сети поверх стандартного P2P серви- са, образуют распределенную базу знаний, предназначенную для организации семантическо- го P2P взаимодействия агентов. Прикладные агенты, в свою очередь, устанавливаются в узлах сети поверх экземпляров агентской платформы. В основу разработки положены функциональная архитектура, разработанная рабочей группой FIPA в качестве предложе- ния для последующей программной реализации и стандартизации. Разработанная про- граммная реализация платформы поддерживается также механизмом парных взаимодейст- вий агентов на основе сообщений разработанных форматов, а также парных коммуникаций узлов сети. Такой механизмом парных взаимодействий агентов также разработан автора- ми. Роль, функции и существо процессов функционирования этой платформы поясняется на примерах двух приложений, которые сами по себе являются достаточно важными с практи- ческой точки зрения. Эти же приложения использованы для верификации основных решений, предложенных в работе.},
author = {Городецкий, В. И. and Карсаев, О. В. and Самойлов, В. В. and Серебряков, С. В.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Труды СПИИРАН/Городецкий et al/Городецкий et al. - 2007 - Открытые сети агентов.pdf:pdf},
journal = {Труды СПИИРАН},
language = {russian},
number = {4},
pages = {11--35},
title = {{Открытые сети агентов}},
year = {2007}
}
@article{Duffy2008,
author = {Duffy, Brian},
journal = {Processing},
pages = {1--12},
title = {{Conversational Agents}},
year = {2008}
}
@article{Of2010,
author = {Of, Foundations and Agents, Computational},
file = {::},
isbn = {9780977858231},
issn = {0004-3702},
number = {95},
pages = {19--20},
title = {{Artificial Intelligence}},
volume = {86},
year = {2010}
}
@article{Emelyanov2016,
abstract = {Extensive use of unmanned aerial vehicles (UAVs) in recent years has induced the rapid growth of research areas related to UAV production. Among these, the design of control systems capable of automating a wide range of UAV activities is one of the most actively explored and evolving. Currently, researchers and developers are interested in designing control systems that can be referred to as intelligent, e.g. the systems which are suited to solve such tasks as planning, goal prioritization, coalition formation etc. and thus guarantee high levels of UAV autonomy. One of the principal problems in intelligent control system design is tying together various methods and models traditionally used in robotics and aimed at solving such tasks as dynamics modelling, control signal genera- tion, location and mapping, path planning etc. with the methods of behaviour modelling and planning which are thoroughly studied in cognitive science. Our work is aimed at solving this problem. We propose layered architecture — STRL (strategic, tactical, reactive, layered) — of the control system that au- tomates the behaviour generation using a cognitive approach while taking into account complex dynamics and kinematics of the control object (UAV).We use a special type of knowledge representation — sign world model — that is based on the psychological activity theory to describe individual behaviour planning and coalition formation processes. We also propose path planning methodology which serves as the mediator between the high-level cognitive activities and the reactive control signals generation. To generate these signals we use a state-dependent Riccati equation and specific method for solving it. We believe that utilization of the proposed architecture will broaden the spectrum of tasks which can be solved by the UAV's coalition automatically, as well as raise the autonomy level of each individual member of that coalition.},
author = {Emel'yanov, Stanislav and Makarov, Dmitry and Panov, Aleksandr I. and Yakovlev, Konstantin},
doi = {10.1016/j.cogsys.2015.12.008},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Cognitive Systems Research/Emel'yanov et al/Emel'yanov et al. - 2016 - Multilayer cognitive architecture for UAV control(2).pdf:pdf},
issn = {1389-0417},
journal = {Cognitive Systems Research},
keywords = {14-11-00692,cognitive architecture,elibrary,intelligent control system,mypub,nonlinear control,path planning,scopus,sign world model,state-dependent Riccati equation,unmanned aerial vehicle,wos{\_}core},
mendeley-tags = {14-11-00692,elibrary,mypub,scopus,wos{\_}core},
pages = {58--72},
title = {{Multilayer cognitive architecture for UAV control}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1389041716000048},
volume = {39},
year = {2016}
}
@inproceedings{Rasmussen1998,
author = {Rasmussen, Daniel and Eliasmith, Chris},
booktitle = {Proceedings of the 36th Annual Conference of the Cognitive Science Society},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 36th Annual Conference of the Cognitive Science Society/Rasmussen, Eliasmith/Rasmussen, Eliasmith - 2014 - A neural model of hierarchical reinforcement learning.pdf:pdf},
keywords = {brain could achieve its,hierarchical,hrl into a theory,neural engineering framework,neural model,of neural function,providing a hypothe-,reinforcement learning,sis for how the,strengths in scaling},
number = {1},
pages = {1252--1257},
title = {{A neural model of hierarchical reinforcement learning}},
year = {2014}
}
@article{Gorodetsky2009,
abstract = {В работе рассматривается технология построения прикладных систем группового управления, со- стоящих из большого числа автономных подсистем, организованных в сеть, узлы которой могут работать под управлением различных операционных систем и в различных коммуникационных средах. Технология интегрирует подходы распределенного принятия решений, многоагентных систем, ориентированной на сервис архитектуры и вычислений на основе парных взаимодействий. Технология поддерживается инструментальными средствами, ко- торые обеспечивают эффективную разработку агентов и механизмов их взаимодействия. Приводятся примеры использования технологии в ряде приложений, в частности, для автономного управления воздушным движением в районе аэропорта.},
author = {Городецкий, В. И. and Карсаев, О. В. and Самойлов, В. В. and Серебряков, С. В.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Искусственный интеллект и принятие решений/Городецкий et al/Городецкий et al. - 2009 - Прикладные многоагентные системы группового управления.pdf:pdf},
journal = {Искусственный интеллект и принятие решений},
language = {russian},
number = {2},
pages = {3--24},
title = {{Прикладные многоагентные системы группового управления}},
year = {2009}
}
@article{Frank2012,
abstract = {Growing evidence suggests that the prefrontal cortex (PFC) is organized hierarchically, with more anterior regions having increasingly abstract representations. How does this organization support hierarchical cognitive control and the rapid discovery of abstract action rules? We present computational models at different levels of description. A neural circuit model simulates interacting corticostriatal circuits organized hierarchically. In each circuit, the basal ganglia gate frontal actions, with some striatal units gating the inputs to PFC and others gating the outputs to influence response selection. Learning at all of these levels is accomplished via dopaminergic reward prediction error signals in each corticostriatal circuit. This functionality allows the system to exhibit conditional if-then hypothesis testing and to learn rapidly in environments with hierarchical structure. We also develop a hybrid Bayesian-reinforcement learning mixture of experts (MoE) model, which can estimate the most likely hypothesis state of individual participants based on their observed sequence of choices and rewards. This model yields accurate probabilistic estimates about which hypotheses are attended by manipulating attentional states in the generative neural model and recovering them with the MoE model. This 2-pronged modeling approach leads to multiple quantitative predictions that are tested with functional magnetic resonance imaging in the companion paper.},
author = {Frank, Michael J. and Badre, David},
doi = {10.1093/cercor/bhr114},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Cerebral cortex (New York, N.Y. 1991)/Frank, Badre/Frank, Badre - 2012 - Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1 Computational analysis.pdf:pdf},
issn = {1460-2199},
journal = {Cerebral cortex (New York, N.Y. : 1991)},
keywords = {Computer Simulation,Corpus Striatum,Corpus Striatum: cytology,Corpus Striatum: physiology,Humans,Learning,Learning: physiology,Models,Neural Pathways,Neural Pathways: cytology,Neural Pathways: physiology,Neurological,Neurons,Neurons: physiology,Prefrontal Cortex,Prefrontal Cortex: cytology,Prefrontal Cortex: physiology,Reinforcement (Psychology)},
number = {3},
pages = {509--26},
pmid = {21693490},
title = {{Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: Computational analysis}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3278315{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {22},
year = {2012}
}
@article{M.Garcia-Serrano2002,
author = {{M. Garc{\'{i}}a-Serrano}, Ana and Rodrigo-Aguado, Luis and Javier, Calle},
journal = {Proceedings of the Third International Conference on Language Resources and Evaluation (LREC'02)},
number = {January 2002},
title = {{Natural Language Dialogue in a Virtual Assistant Interface.}},
url = {http://aclweb.org/anthology/L02-1253},
year = {2002}
}
@article{Panov2018a,
abstract = {В работе рассматривается знаковый подход к проблеме моделирования процесса целеполагания и его интеграции с методами синтеза плана поведения. На основе психологически правдоподобной модели знаковой картины мира когнитивного агента предложен алгоритм GoalMAP, который на основе формальной сетевой модели реализует итеративный процесс иерархического планирования с выделенным этапом постановки или выбора новой цели. Рассмотрена оценка сложности алгоритма планирования поведения, проведены модельные эксперименты с программной реализацией построенных алгоритмов, демонстрирующей ключевые особенности используемого подхода.},
author = {Панов, А. И.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Искусственный интеллект и принятие решений/Панов/Панов - 2018 - Целеполагание и синтез плана поведения когнитивным агентом.docx:docx},
journal = {Искусственный интеллект и принятие решений},
keywords = {16-37-60055,17-07-00281,GoalMAP,frccsc,mypub,vak,знак,знаковая картина мира,когнитивный агент,культурно-исторический подход,планирование,планирование поведения,теория деятельности,целеполагание},
language = {russian},
mendeley-tags = {16-37-60055,17-07-00281,frccsc,mypub,vak},
number = {2},
pages = {(В печати)},
title = {{Целеполагание и синтез плана поведения когнитивным агентом}},
year = {2018}
}
@article{Huang2017a,
abstract = {Our ultimate goal is to efficiently enable end-users to correctly anticipate a robot's behavior in novel situations. This behavior is often a direct result of the robot's underlying objective function. Our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, and then selecting those behaviors that are maximally informative. The problem of computing a posterior over objectives from observed behavior is known as Inverse Reinforcement Learning (IRL), and has been applied to robots learning human objectives. We consider the problem where the roles of human and robot are swapped. Our main contribution is to recognize that unlike robots, humans will not be $\backslash$emph{\{}exact{\}} in their IRL inference. We thus introduce two factors to define candidate approximate-inference models for human learning in this setting, and analyze them in a user study in the autonomous driving domain. We show that certain approximate-inference models lead to the robot generating example behaviors that better enable users to anticipate what the robot will do in test situations. Our results also suggest, however, that additional research is needed in modeling how humans extrapolate from examples of robot behavior.},
archivePrefix = {arXiv},
arxivId = {1702.03465},
author = {Huang, Sandy H. and Held, David and Abbeel, Pieter and Dragan, Anca D.},
doi = {10.15607/RSS.2017.XIII.059},
eprint = {1702.03465},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Huang et al/Huang et al. - 2017 - Enabling Robots to Communicate their Objectives.pdf:pdf},
isbn = {1234567245},
issn = {15737527},
title = {{Enabling Robots to Communicate their Objectives}},
url = {http://arxiv.org/abs/1702.03465},
year = {2017}
}
@article{Osipov2015d,
abstract = {Рассматриваются процедуры формирования элемента картины мира субъекта деятельности (знака), введенные в первой части работы. Исследуется процесс формирование пары образ – значение знака с учетом современных представлений о строении и функционировании коры головного мозга человека. Строится алгоритм синтеза плана поведения и предлагается новая архитектура интеллектуальных агентов, обладающих, в частности, способностями к распределению ролей в коалициях.},
author = {Осипов, Г. С. and Панов, А. И. and Чудова, Н. В.},
doi = {10.7868/S0002338815050108},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Известия Российский академии наук. Теория и системы управления/Осипов, Панов, Чудова/Осипов, Панов, Чудова - 2015 - Управление поведением как функция сознания. II. Синтез плана поведения.pdf:pdf},
isbn = {0002-3388},
journal = {Известия Российский академии наук. Теория и системы управления},
keywords = {14-11-00692,15-07-06214,elibrary,mypub,vak},
language = {russian},
mendeley-tags = {14-11-00692,15-07-06214,elibrary,mypub,vak},
number = {6},
pages = {47--61},
title = {{Управление поведением как функция сознания. II. Синтез плана поведения}},
year = {2015}
}
@article{Jaderberg2018,
abstract = {Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.},
archivePrefix = {arXiv},
arxivId = {1807.01281},
author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
doi = {arXiv:1807.01281},
eprint = {1807.01281},
file = {:C$\backslash$:/Users/panov/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaderberg et al. - 2018 - Human-level performance in first-person multiplayer games with population-based deep reinforcement learning(2).pdf:pdf},
title = {{Human-level performance in first-person multiplayer games with population-based deep reinforcement learning}},
url = {http://arxiv.org/abs/1807.01281},
year = {2018}
}
@article{Abel2018b,
abstract = {We consider the problem of how best to use prior experience to bootstrap lifelong learning, where an agent faces a series of task instances drawn from some task distribution. First, we identify the initial policy that optimizes expected performance over the distribution of tasks for increasingly complex classes of policy and task distributions. We empirically demonstrate the relative performance of each policy class' optimal element in a variety of simple task distributions. We then consider value-function initialization methods that preserve PAC guarantees while simultaneously minimizing the learning required in two learning algorithms, yielding MaxQInit, a practical new method for value-function-based transfer. We show that MaxQInit performs well in simple lifelong RL experiments.},
author = {Abel, David and Jinnai, Yuu and Guo, Sophie Yue and Konidaris, George and Littman, Michael},
file = {::},
journal = {Proceedings of the 35th International Conference on Machine Learning},
pages = {20--29},
title = {{Policy and Value Transfer in Lifelong Reinforcement Learning}},
url = {http://proceedings.mlr.press/v80/abel18b.html},
volume = {80},
year = {2018}
}
@article{Botvinick2012,
abstract = {The hierarchical structure of human and animal behavior has been of critical interest in neuroscience for many years. Yet understanding the neural processes that give rise to such structure remains an open challenge. In recent research, a new perspective on hierarchical behavior has begun to take shape, inspired by ideas from machine learning, and in particular the framework of hierarchical reinforcement learning. Hierarchical reinforcement learning builds on traditional reinforcement learning mechanisms, extending them to accommodate temporally extended behaviors or subroutines. The resulting computational paradigm has begun to influence both theoretical and empirical work in neuroscience, conceptually aligning the study of hierarchical behavior with research on other aspects of learning and decision making, and giving rise to some thought-provoking new findings. ?? 2012.},
author = {Botvinick, Matthew Michael},
doi = {10.1016/j.conb.2012.05.008},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Current Opinion in Neurobiology/Botvinick/Botvinick - 2012 - Hierarchical reinforcement learning and decision making.pdf:pdf},
isbn = {0818653302},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
number = {6},
pages = {956--962},
pmid = {22695048},
publisher = {Elsevier Ltd},
title = {{Hierarchical reinforcement learning and decision making}},
url = {http://dx.doi.org/10.1016/j.conb.2012.05.008},
volume = {22},
year = {2012}
}
@article{Panov2015g,
abstract = {Рассматриваются особенности представления пространственных и временных знаний для планирования согласованного перемещения коалиции интеллектуальных агентов. В качестве примера подобной задачи рассмотрен перелет группы беспилотных летательных аппаратов в заданную область на местности с препятствиями. Рассматривается знаковый подход к описанию знаний агента, основанный на психологической теории деятельности и биологически правдоподобной иерархической модели распознавания. Особое внимание уделено описанию действий и процессам обучения на основе поступающей первичной информации.},
author = {Панов, А. И.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Робототехника и техническая кибернетика/Панов/Панов - 2015 - Представление знаний автономных агентов, планирующих согласованные перемещения.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Робототехника и техническая кибернетика/Панов/Панов - 2015 - Представление знаний автономных агентов, планирующих согласованные перемещения.docx:docx},
journal = {Робототехника и техническая кибернетика},
keywords = {14-07-31194,elibrary,mypub,группа БПЛА,знак,иерархия автоматов,когнитивные процессы,представление знаний},
language = {russian},
mendeley-tags = {14-07-31194,elibrary,mypub},
number = {4(9)},
pages = {34--40},
title = {{Представление знаний автономных агентов, планирующих согласованные перемещения}},
url = {https://elibrary.ru/item.asp?id=25085187},
year = {2015}
}
@incollection{Panov2016c,
abstract = {In this paper we outline the approach of solving special type of navigation tasks for robotic systems, when a coalition of robots (agents) acts in the 2D environment, which can be modified by the actions, and share the same goal location. The latter is originally unreachable for some members of the coalition, but the common task still can be accomplished as the agents can assist each other (e.g. by modifying the environment). We call such tasks smart relocation tasks (as the can not be solved by pure path planning methods) and study spatial and behavior interaction of robots while solving them. We use cognitive approach and introduce semiotic knowledge representation — sign world model which underlines behavioral planning methodology. Planning is viewed as a recursive search process in the hierarchical state-space induced by sings with path planning signs reside on the lowest level. Reaching this level triggers path planning which is accomplished by state of the art grid-based planners focused on producing smooth paths (e.g. LIAN) and thus indirectly guarantying feasibility of that paths against agent's dynamic constraints.},
author = {Panov, Aleksandr I. and Yakovlev, Konstantin},
booktitle = {Robot Intelligence Technology and Applications 4},
doi = {10.1007/978-3-319-31293-4_1},
editor = {Kim, Jong-Hwan and Karray, Fakhri and Jo, Jun and Sincak, Peter and Myung, Hyun},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Robot Intelligence Technology and Applications 4/Panov, Yakovlev/Panov, Yakovlev - 2016 - Behavior and Path Planning for the Coalition of Cognitive Robots in Smart Relocation Tasks(2).pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Robot Intelligence Technology and Applications 4/Panov, Yakovlev/Panov, Yakovlev - 2016 - Behavior and Path Planning for the Coalition of Cognitive Robots in Smart Relocation Tasks.pdf:pdf},
keywords = {14-07-31194,15-37-20893,behavior planning,coalition,elibrary,knowledge representation,lian,myconf,path planning,scopus,semiotic model,sign world model,task planning,wos{\_}core},
mendeley-tags = {14-07-31194,15-37-20893,elibrary,myconf,scopus,wos{\_}core},
pages = {3--20},
publisher = {Springer International Publishing},
series = {Advances in Intelligent Systems and Computing},
title = {{Behavior and Path Planning for the Coalition of Cognitive Robots in Smart Relocation Tasks}},
url = {http://link.springer.com/10.1007/978-3-319-31293-4{\_}1},
year = {2016}
}
@incollection{Hengst2012,
abstract = {Hierarchical decomposition tackles complex problems by reducing them to a smaller set of interrelated problems. The smaller problems are solved separately and the results re-combined to find a solution to the original problem. It is well known that the na{\"{i}}ve application of reinforcement learning (RL) techniques fails to scale to more complex domains. This Chapter introduces hierarchical approaches to reinforcement learning that hold out the promise of reducing a reinforcement learning problems to a manageable size. Hierarchical Reinforcement Learning (HRL) rests on finding good re-usable temporally extended actions that may also provide opportunities for state abstraction. Methods for reinforcement learning can be extended to work with abstract states and actions over a hierarchy of subtasks that decompose the original problem, potentially reducing its computational complexity. We use a four-room task as a running example to illustrate the various concepts and approaches, including algorithms that can automatically learn the hierarchical structure from interactions with the domain.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.02971},
author = {Hengst, Bernhard},
booktitle = {Reinforcement Learning},
doi = {10.1007/978-3-642-27645-3_9},
eprint = {arXiv:1509.02971},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Reinforcement Learning/Hengst/Hengst - 2012 - Hierarchical Approaches.pdf:pdf},
isbn = {978-3-642-27644-6},
issn = {18726240},
pages = {293--323},
title = {{Hierarchical Approaches}},
url = {http://dx.doi.org/10.1007/978-3-642-27645-3{\_}9 http://link.springer.com/10.1007/978-3-642-27645-3{\_}9},
year = {2012}
}
@article{Riedmiller2018a,
abstract = {We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.},
archivePrefix = {arXiv},
arxivId = {1802.10567},
author = {Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and {Van de Wiele}, Tom and Mnih, Volodymyr and Heess, Nicolas and Springenberg, Jost Tobias},
eprint = {1802.10567},
file = {::},
issn = {1938-7228},
title = {{Learning by Playing - Solving Sparse Reward Tasks from Scratch}},
url = {http://arxiv.org/abs/1802.10567},
year = {2018}
}
@inproceedings{Panov2010b,
abstract = {В данной работе предлагается новая методика выявления причинно-следственных связей в психологических данных, получаемых при проведении тестирования. Используется сочетание методов статистической обработки и методов машинного обучения, в частности ДСМ-метода и метода AQ покрытий. Предлагаются способы дополнительного анализа и визуализации полученных гипотез о причинно-следственных связях.},
address = {Рыбинск},
author = {Панов, А. И.},
booktitle = {Теория и практика системного анализа: Труды I Всероссийской научной конференции молодых учёных},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Теория и практика системного анализа Труды I Всероссийской научной конференции молодых учёных/Панов/Панов - 2010 - Методика интеллектуального анализа результатов психологического тестирования.pdf:pdf},
keywords = {myconf},
language = {russian},
mendeley-tags = {myconf},
pages = {39--45},
publisher = {РГАТА имени П.А. Соловьева},
title = {{Методика интеллектуального анализа результатов психологического тестирования}},
volume = {I},
year = {2010}
}
@article{Botvinick2009a,
abstract = {Research on human and animal behavior has long emphasized its hierarchical structure-the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior.},
author = {Botvinick, Matthew M. and Niv, Yael and Barto, Andrew C.},
doi = {10.1016/j.cognition.2008.08.011},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Cognition/Botvinick, Niv, Barto/Botvinick, Niv, Barto - 2009 - Hierarchically organized behavior and its neural foundations a reinforcement learning perspective.pdf:pdf},
issn = {1873-7838},
journal = {Cognition},
keywords = {Animals,Humans,Models,Nerve Net,Nerve Net: physiology,Prefrontal Cortex,Prefrontal Cortex: physiology,Problem Solving,Problem Solving: physiology,Psychological,Reinforcement (Psychology)},
number = {3},
pages = {262--80},
pmid = {18926527},
title = {{Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2783353{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {113},
year = {2009}
}
@article{Jennings1995a,
abstract = {One reason why Distributed AI (DAI) technology has been deployed in relatively few real-size applications is that it lacks a clear and implementable model of cooperative problem solving which specifies how agents should operate and interact in complex, dynamic and unpredictable environments. As a consequence of the experience gained whilst building a number of DAI systems for industrial applications, a new principled model of cooperation has been developed. This model, called Joint Responsibility, has the notion of joint intentions at its core. It specifies pre-conditions which must be attained before collaboration can commence and prescribes how individuals should behave both when joint activity is progressing satisfactorily and also when it runs into difficulty. The theoretical model has been used to guide the implementation of a general-purpose cooperation framework and the qualitative and quantitative benefits of this implementation have been assessed through a series of comparative experiments in the real-world domain of electricity transportation management. Finally, the success of the approach of building a system with an explicit and grounded representation of cooperative problem solving is used to outline a proposal for the next generation of multi-agent systems. {\textcopyright} 1995.},
author = {Jennings, N. R.},
doi = {10.1016/0004-3702(94)00020-2},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Artificial Intelligence/Jennings/Jennings - 1995 - Controlling cooperative problem solving in industrial multi-agent systems using joint intentions.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
number = {2},
pages = {195--240},
title = {{Controlling cooperative problem solving in industrial multi-agent systems using joint intentions}},
volume = {75},
year = {1995}
}
@article{Bonnet2015,
author = {Bonnet, Jonathan and Gleizes, Marie Pierre and Kaddoum, Elsy and Rainjonneau, Serge and Flandin, Gregory},
doi = {10.1109/SASO.2015.9},
file = {::},
isbn = {9781467375351},
issn = {19493681},
journal = {International Conference on Self-Adaptive and Self-Organizing Systems, SASO},
keywords = {adaptive multi-agent system,multisatellite,planning},
pages = {11--20},
title = {{Multi-satellite Mission Planning Using a Self-Adaptive Multi-agent System}},
volume = {2015-Octob},
year = {2015}
}
@article{Konidaris2012,
abstract = {We present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to significantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that significantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-specific skills.},
author = {Konidaris, George and Scheidwasser, Ilya and Barto, Andrew G.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Journal of Machine Learning Research/Konidaris, Scheidwasser, Barto/Konidaris, Scheidwasser, Barto - 2012 - Transfer in reinforcement learning via shared features.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {reinforcement learning,shaping,skills,transfer},
number = {1},
pages = {1333--1371},
title = {{Transfer in reinforcement learning via shared features}},
url = {http://dl.acm.org/citation.cfm?id=2503308.2343689{\%}5Cnhttp://dl.acm.org/ft{\_}gateway.cfm?id=2343689{\&}type=pdf},
volume = {13},
year = {2012}
}
@article{Rabinowitz2018a,
abstract = {Theory of mind (ToM) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We design a Theory of Mind neural network {\{}–{\}} a ToMnet {\{}–{\}} which uses meta-learning to build such models of the agents it encounters. The ToMnet learns a strong prior model for agents' future behaviour, and, using only a small number of behavioural observations, can bootstrap to richer predictions about agents' characteristics and mental states. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep RL agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test of recognising that others can hold false beliefs about the world.},
archivePrefix = {arXiv},
arxivId = {1802.07740},
author = {Rabinowitz, Neil and Perbet, Frank and Song, Francis and Zhang, Chiyuan and Eslami, S M Ali and Botvinick, Matthew},
eprint = {1802.07740},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Rabinowitz et al/Rabinowitz et al. - 2018 - Machine Theory of Mind.pdf:pdf},
issn = {1938-7228},
journal = {Proceedings of the 35th International Conference on Machine Learning},
pages = {4215--4224},
title = {{Machine Theory of Mind}},
url = {http://proceedings.mlr.press/v80/rabinowitz18a.html},
volume = {80},
year = {2018}
}
@inproceedings{Barto2004,
abstract = {Humans and other animals often engage in activities for their own sakes rather than as steps toward solving practical problems. Psychologists call these intrinsically motivated behaviors. What we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy. At the core of the model are recent theoretical and algorithmic advances in computational reinforcement learning, specifically, new concepts related to skills and new learning algorithms for learning with skill hierarchies. 1},
author = {Barto, Andrew G. and Singh, Satinder},
booktitle = {Proceedings of the 3rd International Conference on Development and Learning (ICDL 2004)},
doi = {10.1.1.123.395},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 3rd International Conference on Development and Learning (ICDL 2004)/Barto, Singh/Barto, Singh - 2004 - Intrinsically motivated learning of hierarchical collections of skills.pdf:pdf},
pages = {112--119},
title = {{Intrinsically motivated learning of hierarchical collections of skills}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.117.6436{\%}5Cnhttp://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A083E47D2FE080D11716EC249E464BE2?doi=10.1.1.117.6436{\&}rep=rep1{\&}type=pdf},
year = {2004}
}
@article{Dann2017a,
abstract = {In platform videogames, players are frequently tasked with solving medium-term navigation prob-lems in order to gather items or powerups. Arti-ficial agents must generally obtain some form of direct experience before they can solve such tasks. Experience is gained either through training runs, or by exploiting knowledge of the game's physics to generate detailed simulations. Human players, on the other hand, seem to look ahead in high-level, abstract steps. Motivated by human play, we intro-duce an approach that leverages not only abstract " skills " , but also knowledge of what those skills can and cannot achieve. We apply this approach to Infinite Mario, where despite facing randomly generated, maze-like levels, our agent is capable of deriving complex plans in real-time, without rely-ing on perfect knowledge of the game's physics.},
author = {Dann, Michael and Zambetta, Fabio and Thangarajah, John},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IJCAI International Joint Conference on Artificial Intelligence/Dann, Zambetta, Thangarajah/Dann, Zambetta, Thangarajah - 2017 - Real-time navigation in classical platform games via skill reuse.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning: Reinforcement Learning,Multidisciplinary Topics and Applications: Compute},
pages = {1582--1588},
title = {{Real-time navigation in classical platform games via skill reuse}},
year = {2017}
}
@inproceedings{Primeau2016,
author = {Primeau, Nicolas and Falcon, Rafael and Abielmona, Rami and Groza, Voicu and Petriu, Emil},
booktitle = {2016 IEEE 20th Jubilee International Conference on Intelligent Engineering Systems (INES)},
doi = {10.1109/INES.2016.7555136},
file = {::},
isbn = {978-1-5090-1216-9},
month = {jun},
pages = {21--26},
publisher = {IEEE},
title = {{Improving task allocation in risk-aware robotic sensor networks via auction protocol selection}},
url = {http://ieeexplore.ieee.org/document/7555136/},
year = {2016}
}
@article{Traum,
abstract = {SLIDES!},
author = {Traum, David},
title = {{Approaches to dialogue systems and dialogue management}}
}
@incollection{Kiselev2018c,
abstract = {The paper discusses the interaction between methods of modeling reasoning and behavior planning in a sign-based world model for the task of synthesizing a hierarchical plan of relocation. Such inter- action is represented by the formalism of intelligent rule-based dynamic systems in the form of alternate use of transition functions (planning) and closure functions (reasoning). Particular attention is paid to the ways of information representation of the object spatial relationships on the local map and the methods of organizing pseudo-physical reasoning in a sign-based world model. The paper presents a number of model exper- iments on the relocation of a cognitive agent in different environments and replenishment of the state description by means of the variants of logical inference.},
author = {Kiselev, Gleb and Kovalev, Alexey and Panov, Aleksandr I},
booktitle = {Artificial Intelligence},
doi = {10.1007/978-3-030-00617-4_1},
editor = {Kuznetsov, Sergei and Osipov, Gennady S. and Stefanuk, Vadim},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Artificial Intelligence/Kiselev, Kovalev, Panov/Kiselev, Kovalev, Panov - 2018 - Spatial reasoning and planning in sign-based world model(2).pdf:pdf},
isbn = {978-3-030-00616-7},
keywords = {17-29-07051,18-07-01011,ing modelling,mipt,myconf,pseudophysical logic,reason-,relocation planning,sign,sign-based world model},
mendeley-tags = {17-29-07051,18-07-01011,mipt,myconf},
pages = {1--10},
publisher = {Springer International Publishing},
series = {Communications in Computer and Information Science},
title = {{Spatial reasoning and planning in sign-based world model}},
url = {https://link.springer.com/chapter/10.1007/978-3-030-00617-4{\_}1},
year = {2018}
}
@article{Granatyr2015,
abstract = {Finding reliable partners to interact with in open environments is a challenging task for software agents, and trust and reputation mechanisms are used to handle this issue. From this viewpoint, we can observe the growing body of research on this subject, which indicates that these mechanisms can be considered key elements to design multiagent systems (MASs). Based on that, this article presents an extensive but not exhaustive review about the most significant trust and reputation models published over the past two decades, and hundreds of models were analyzed using two perspectives. The first one is a combination of trust dimensions and principles proposed by some relevant authors in the field, and the models are discussed using an MAS perspective. The second one is the discussion of these dimensions taking into account some types of interaction found in MASs, such as coalition, argumentation, negotiation, and recommendation. By these analyses, we aim to find significant relations between trust dimensions and types of interaction so it would be possible to construct MASs using the most relevant dimensions according to the types of interaction, which may help developers in the design of MASs. es, and Fabr{\'{i}}cio Enembreck. 2015. Trust and reputation models for multiagent systems. ACM Comput. Surv. 48, 2, Article 27 (October 2015), 42 pages.},
author = {Granatyr, Jones and Botelho, Vanderson and Lessing, Otto Robert and Scalabrin, Edson Em{\'{i}}lio and Barth{\`{e}}s, Jean-Paul and Enembreck, Fabr{\'{i}}cio},
doi = {10.1145/2816826},
file = {::},
isbn = {0360-0300},
issn = {03600300},
journal = {ACM Computing Surveys},
month = {oct},
number = {2},
pages = {1--42},
title = {{Trust and Reputation Models for Multiagent Systems}},
url = {http://dl.acm.org/citation.cfm?doid=2830539.2816826},
volume = {48},
year = {2015}
}
@article{Verma2018a,
abstract = {We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural "oracle". We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.},
archivePrefix = {arXiv},
arxivId = {1804.02477},
author = {Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
eprint = {1804.02477},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Verma et al/Verma et al. - 2018 - Programmatically Interpretable Reinforcement Learning.pdf:pdf},
issn = {1938-7228},
title = {{Programmatically Interpretable Reinforcement Learning}},
url = {http://arxiv.org/abs/1804.02477},
year = {2018}
}
@inproceedings{Shu2017,
abstract = {Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task corresponds to a human language description. Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices. In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills. We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.},
archivePrefix = {arXiv},
arxivId = {1712.07294},
author = {Shu, Tianmin and Xiong, Caiming and Socher, Richard},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
doi = {10.1051/0004-6361/201527329},
eprint = {1712.07294},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Shu, Xiong, Socher/Shu, Xiong, Socher - 2017 - Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pmid = {23459267},
title = {{Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning}},
url = {http://arxiv.org/abs/1712.07294},
year = {2017}
}
@article{Osipov2014c,
abstract = {Рассматриваются функции, которые в психологии принято относить к функциям сознания. К числу таких функций относятся рефлексия, осознание мотива деятельности, целеполагание, синтез целенаправленного поведения и некоторые иные. Описание опирается на понятие знака, достаточно широко используемое в психологии, в частности, в культурно-исторической теории Л.С.Выготского, где знак понимается неформальным образом. В настоящей работе уточняется понятие знака, рассматриваются механизмы формирования знаков и некоторые процедуры самоорганизации на множестве знаков. Благодаря работе механизмов самоорганизации возникает новый способ представления картины мира субъекта деятельности. Вводится понятие семиотической сети, которое используется для исследования картин мира субъектов. Строятся модели некоторых из указанных выше функций. Вторая часть статьи посвящена функциям самосознания и применению построенных моделей для задачи синтеза плана и построения новых архитектур интеллектуальных агентов, обладающих, в частности, способностями к распределению ролей в коалициях.},
author = {Осипов, Г. С. and Панов, А. И. and Чудова, Н. В.},
doi = {10.7868/S000233881404012X},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Известия Российской академии наук. Теория и системы управления/Осипов, Панов, Чудова/Осипов, Панов, Чудова - 2014 - Управление поведением как функция сознания. I. Картина мира и целеполагание.doc:doc;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Известия Российской академии наук. Теория и системы управления/Осипов, Панов, Чудова/Осипов, Панов, Чудова - 2014 - Управление поведением как функция сознания. I. Картина мира и целеполагание.pdf:pdf},
issn = {0002-3388},
journal = {Известия Российской Академии Наук. Теория и системы управления},
keywords = {12-07-00611,elibrary,mypub,vak},
language = {russian},
mendeley-tags = {12-07-00611,elibrary,mypub,vak},
number = {4},
pages = {49--62},
title = {{Управление поведением как функция сознания. I. Картина мира и целеполагание}},
url = {http://elibrary.ru/item.asp?doi=10.7868/S000233881404012X},
year = {2014}
}
@article{Crosby2014,
abstract = {In this paper we present a novel approach to multiagent planning in domains with concurrent actions and associated concurrent action constraints. In these domains, we associate the actions of individual agents with subsets of objects, which allows for a transformation of the problems into single-agent planning problems that are considerably easier to solve. The transformation forces agents to select joint actions associated with a single subset of objects at a time, and ensures that the concurrency constraints on this subset are satisfied. Joint actions are serialised such that each agent performs their part of the action separately. The number of actions in the resulting single-agent planning problem turns out to be manageable in many real-world domains, thus allowing the problem to be solved efficiently using a standard single-agent planner. We also describe a cost-optimal algorithm for compressing the resulting plan, i.e. merging individual actions in order to reduce the total number of joint actions. Results show that our approach can handle large problems that are impossible to solve for most multiagent planners.},
author = {Crosby, Matthew and Jonsson, Anders and Rovatsos, Michael},
doi = {10.3233/978-1-61499-419-0-237},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Frontiers in Artificial Intelligence and Applications/Crosby, Jonsson, Rovatsos/Crosby, Jonsson, Rovatsos - 2014 - A single-agent approach to multiagent planning.pdf:pdf},
isbn = {9781614994183},
issn = {09226389},
journal = {Frontiers in Artificial Intelligence and Applications},
pages = {237--242},
title = {{A single-agent approach to multiagent planning}},
volume = {263},
year = {2014}
}
@inproceedings{Roderick2017,
abstract = {We examine the problem of learning and planning on high-dimensional domains with long horizons and sparse rewards. Recent approaches have shown great successes in many Atari 2600 domains. However, domains with long horizons and sparse rewards, such as Montezuma's Revenge and Venture, remain challenging for existing methods. Methods using abstraction (Dietterich 2000; Sutton, Precup, and Singh 1999) have shown to be useful in tackling long-horizon problems. We combine recent techniques of deep reinforcement learning with existing model-based approaches using an expert-provided state abstraction. We construct toy domains that elucidate the problem of long horizons, sparse rewards and high-dimensional inputs, and show that our algorithm significantly outperforms previous methods on these domains. Our abstraction-based approach outperforms Deep Q-Networks (Mnih et al. 2015) on Montezuma's Revenge and Venture, and exhibits backtracking behavior that is absent from previous methods.},
archivePrefix = {arXiv},
arxivId = {1710.00459},
author = {Roderick, Melrose and Grimm, Christopher and Tellex, Stefanie},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
eprint = {1710.00459},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Roderick, Grimm, Tellex/Roderick, Grimm, Tellex - 2017 - Deep Abstract Q-Networks.pdf:pdf},
title = {{Deep Abstract Q-Networks}},
url = {http://arxiv.org/abs/1710.00459},
year = {2017}
}
@article{Abdul-Kader2015,
abstract = {—Human-Computer Speech is gaining momentum as a technique of computer interaction. There has been a recent upsurge in speech based search engines and assistants such as Siri, Google Chrome and Cortana. Natural Language Processing (NLP) techniques such as NLTK for Python can be applied to analyse speech, and intelligent responses can be found by designing an engine to provide appropriate human like responses. This type of programme is called a Chatbot, which is the focus of this study. This paper presents a survey on the techniques used to design Chatbots and a comparison is made between different design techniques from nine carefully selected papers according to the main methods adopted. These papers are representative of the significant improvements in Chatbots in the last decade. The paper discusses the similarities and differences in the techniques and examines in particular the Loebner prize-winning Chatbots.},
archivePrefix = {arXiv},
arxivId = {1310.4774},
author = {Abdul-Kader, Sameera A and Woods, John},
eprint = {1310.4774},
journal = {IJACSA) International Journal of Advanced Computer Science and Applications},
keywords = {Chatbot,Loebner Prize,NLP,NLTK,SQL,Turing Test,—AIML},
number = {7},
pages = {72--80},
title = {{Survey on Chatbot Design Techniques in Speech Conversation Systems}},
url = {www.ijacsa.thesai.org},
volume = {6},
year = {2015}
}
@article{Panov2017a,
abstract = {Behavior planning is an important function of any complex technical facility intelligent control system. Presently, a symbol paradigm of artificial intelligence offers a variety of planning algorithms, including those that use precedent information, i.e. algorithms based on acquired knowledge. A symbol grounding problem within the exiting approaches of knowledge representation does not allow effective use the developed algorithms together with learning mechanisms for the purpose of solving a wide variety of applied problems by actual intelligent agents (robotics systems). This article presents the original planning algorithm (MAP Planner), which uses a sign world model as the basis for acquisition and maintenance of knowledge for future use in behavior planning. the sign problem approach describes planning as a cognitive function actualized by the world model of a subject of activity. Apart from solving symbol grounding problems and ensuring psychological and biological plausibility, a sign planning process model allows interaction of an intelligent agent with other participants in solving a cooperative task. The article presents the description of the knowledge representation method used, a MAP planning algorithm, and a model experiment in a “block world”.},
annote = {NULL},
author = {Panov, Aleksandr I.},
doi = {10.1016/j.bica.2016.12.001},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Biologically Inspired Cognitive Architectures/Panov/Panov - 2017 - Behavior Planning of Intelligent Agent with Sign World Model.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Biologically Inspired Cognitive Architectures/Panov/Panov - 2017 - Behavior Planning of Intelligent Agent with Sign World Model.pdf:pdf},
issn = {2212-683X},
journal = {Biologically Inspired Cognitive Architectures},
keywords = {16-37-60055,behavior planning,causal matrix,elibrary,frccsc,hse,map algorithm,mypub,scopus,semiotic network,sign image,sign personal meaning,sign significance,sign world model,wos{\_}core},
mendeley-tags = {16-37-60055,elibrary,frccsc,hse,mypub,scopus,wos{\_}core},
pages = {21--31},
title = {{Behavior Planning of Intelligent Agent with Sign World Model}},
url = {http://www.sciencedirect.com/science/article/pii/S2212683X16300913},
volume = {19},
year = {2017}
}
@article{Jong2008,
abstract = {Hierarchical decomposition promises to help scale reinforcement learning algorithms naturally to real-world problems by exploiting their under- lying structure. Model-based algorithms, which provided the first finite-time convergence guaran- tees for reinforcement learning, may also play an important role in copingwith the relative scarcity of data in large environments. In this paper, we introduce an algorithm that fully integrates mod- ern hierarchical and model-learning methods in the standard reinforcement learning setting. Our algorithm, R-MAXQ, inherits the efficientmodel- based exploration of the R-MAX algorithm and the opportunities for abstraction provided by the MAXQframework. We analyze the sample com- plexity of our algorithm, and our experiments in a standard simulation environment illustrate the advantages of combining hierarchies andmodels},
author = {Jong, Nicholas K and Stone, Peter},
doi = {10.1145/1390156.1390211},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/25th International Conference on Machine Learning/Jong, Stone/Jong, Stone - 2008 - Hierarchical Model-Based Reinforcement Learning R- MAX MAXQ.pdf:pdf},
isbn = {9781605582054},
journal = {25th International Conference on Machine Learning},
number = {July},
title = {{Hierarchical Model-Based Reinforcement Learning : R- MAX + MAXQ}},
year = {2008}
}
@phdthesis{Panov2015g,
abstract = {Построена модель компонент знака—элемента картины мира субъекта деятельности в рамках сегодняшних представлений о функционировании мозга и психики человека. Построены четыре типа операторов распознавания (два статических оператора, динамический и иерархический операторы) в терминах алгебраической теории для образной компоненты знака. Доказаны теоремы корректности линейных замыканий множеств построенных в работе операторов распознавания (статических, динамического и иерархического).},
author = {Панов, А. И.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Панов/Панов - 2015 - Исследование методов, разработка моделей и алгоритмов формирования элементов знаковой картины мира субъекта деятельности.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Панов/Панов - 2015 - Исследование методов, разработка моделей и алгоритмов формирования элементов знаковой картины мира субъекта деятельнос(2).pdf:pdf},
keywords = {14-07-31194,myth},
language = {russian},
mendeley-tags = {14-07-31194,myth},
pages = {118},
school = {Федеральное государственное бюджетное учреждение науки Институт системного анализа Российской академии наук},
title = {{Исследование методов, разработка моделей и алгоритмов формирования элементов знаковой картины мира субъекта деятельности}},
year = {2015}
}
@inproceedings{Jonsson2001,
abstract = {Learning a complex task can be significantly facilitated by defining a hierarchy of subtasks. An agent can learn to choose between various temporally abstract actions, each solving an assigned subtask, to accomplish the overall task. In this paper, we study hierarchical learning using the framework of options. We argue that to take full advantage of hierarchical structure, one should perform option-specific state abstraction, and that if this is to scale to larger tasks, state abstraction ...},
author = {Jonsson, Anders and Barto, Andrew G.},
booktitle = {Proceedings of NIPS 2001},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of NIPS 2001/Jonsson, Barto/Jonsson, Barto - 2001 - Automated State Abstraction for Options using the U-Tree Algorithm.pdf:pdf},
isbn = {0262122413},
issn = {1049-5258},
pages = {1054--1060},
title = {{Automated State Abstraction for Options using the U-Tree Algorithm}},
year = {2001}
}
@article{Chudova2012b,
author = {Чудова, Н. В.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Искусственный интеллект и принятие решений/Чудова/Чудова - 2012 - Концептуальное описание картины мира для задачи моделирования поведения, основанного на сознании.pdf:pdf},
journal = {Искусственный интеллект и принятие решений},
keywords = {12-07-00611,psycho},
language = {russian},
mendeley-tags = {12-07-00611,psycho},
number = {2},
pages = {51--62},
title = {{Концептуальная модель картины мира для задачи моделирования поведения, основанного на сознании}},
year = {2012}
}
@article{Zubarev2013,
abstract = {В статье рассматриваются и анализируются различные подходы к построению архитектуры системы управления беспилотным летательным аппаратом типа «вертолет». На основании проведенного анализа делается вывод о целесообразности применения иерархической, трехуровневой схемы – стратегический уровень, тактический уровень, уровень управления. Описываются решаемые на каждом уровне задачи и дается краткая характеристика методов, подходов и алгоритмов, потенциально применимых для их решения, выделяются наиболее перспективные на взгляд авторов методы и подходы.},
author = {Зубарев, Д. В. and Макаров, Д. А. and Панов, А. И. and Яковлев, К. С.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Авиакосмическое приборостроение/Зубарев et al/Зубарев et al. - 2013 - Принципы построения многоуровневых архитектур систем управления беспилотными летательными аппаратами.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Авиакосмическое приборостроение/Зубарев et al/Зубарев et al. - 2013 - Принципы построения многоуровневых архитектур систем управления беспилотными летательными аппаратами.docx:docx},
journal = {Авиакосмическое приборостроение},
keywords = {12-07-00611,12-07-31057,12-07-31058,elibrary,mypub,алгоритмы управления,архитектура системы управления,беспилотные летательные аппараты,вертолеты,когнитивное компьютерное моделирование,методы планирования траекторий,регуляторы},
language = {russian},
mendeley-tags = {12-07-00611,12-07-31057,12-07-31058,elibrary,mypub},
number = {4},
pages = {10--28},
title = {{Принципы построения многоуровневых архитектур систем управления беспилотными летательными аппаратами}},
year = {2013}
}
@incollection{Jones2006,
abstract = {As we progress towards a world where robots play an integral role in society, a critical problem that remains to be solved is the pickup team challenge; that is, dynamically formed heterogeneous robot teams executing coordinated tasks where little information is known a priori about the tasks, the robots, and the environments in which they would operate. Successful solutions to forming pickup teams would enable researchers to experiment with larger numbers of robots and enable industry to efficiently and cost-effectively integrate new robot technology with existing legacy teams. In this paper, we define the challenge of pickup teams and propose the treasure hunt domain for evaluating the performance of pickup teams. Additionally, we describe a basic implementation of a pickup team that can search and discover treasure in a previously unknown environment. We build on prior approaches in market-based task allocation and plays for synchronized task execution, to allocate roles amongst robots in the pickup team, and to execute synchronized team actions to accomplish the treasure hunt task},
author = {Jones, E. Gil and Browning, Brett and Dias, M. Bernardine and Argall, Brenna and Veloso, Manuela and Stentz, Anthony},
booktitle = {Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006},
doi = {10.1109/ROBOT.2006.1641771},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006/Jones et al/Jones et al. - 2006 - Dynamically formed heterogeneous robot teams performing tightly-coordinated tasks.pdf:pdf},
isbn = {0780395069},
issn = {10504729},
pages = {570--575},
title = {{Dynamically formed heterogeneous robot teams performing tightly-coordinated tasks}},
year = {2006}
}
@article{Singh2010,
abstract = {There is great interest in building intrinsic motivation into artificial systems using the reinforcement learning framework. Yet, what intrinsic motivation may mean computationally, and how it may differ from extrinsic motivation, remains a murky and controversial subject. In this paper, we adopt an evolutionary perspective and define a new optimal reward framework that captures the pressure to design good primary reward functions that lead to evolutionary success across environments. The results of two computational experiments show that optimal primary reward signals may yield both emergent intrinsic and extrinsic motivation. The evolutionary perspective and the associated optimal reward framework thus lead to the conclusion that there are no hard and fast features distinguishing intrinsic and extrinsic reward computationally. Rather, the directness of the relationship between rewarding behavior and evolutionary success varies along a continuum.},
author = {Singh, Satinder and Lewis, Richard L. and Barto, Andrew G. and Sorg, Jonathan},
doi = {10.1109/TAMD.2010.2051031},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IEEE Transactions on Autonomous Mental Development/Singh et al/Singh et al. - 2010 - Intrinsically Motivated Reinforcement Learning An Evolutionary Perspective.pdf:pdf},
isbn = {0262195348, 9780262195348},
issn = {1943-0604},
journal = {IEEE Transactions on Autonomous Mental Development},
number = {2},
pages = {70--82},
title = {{Intrinsically Motivated Reinforcement Learning: An Evolutionary Perspective}},
url = {http://ieeexplore.ieee.org/document/5471106/},
volume = {2},
year = {2010}
}
@article{Kiselev2018a,
abstract = {В настоящей работе рассмотрена задача распределения ролей при составлении общего плана действий в коалиции когнитивных агентов. Когнитивные агенты реализуют основные функции интеллектуального агента с использованием моделей когнитивных функций человека. В качестве психологических оснований построения моделей когнитивных функций использованы теория деятельности и формальная модель знаковой картины мира. В работе представлен оригинальный метод распределения ролей – алгоритм MultiMAP, основанный на знаковом способе планирования поведения агента. Представлены основные особенности описываемого подхода, включающие способы представления знаний агента о себе и о других агентах, способы знаковой коммуникации и сохранения опыта кооперации с другими агентами. Описаны модельные эксперименты, демонстрирующие основные преимущества представленного подхода и некоторые недостатки, на устранение которых направлена будущая работа в данном направлении.},
author = {Киселев, Г. А. and Панов, А. И.},
doi = {10.15622/sp.57.7},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Труды СПИИРАН/Киселев, Панов/Киселев, Панов - 2018 - Знаковый подход к задаче распределения ролей в коалиции когнитивных агентов.docx:docx;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Труды СПИИРАН/Киселев, Панов/Киселев, Панов - 2018 - Знаковый подход к задаче распределения ролей в коалиции когнитивных агентов.pdf:pdf},
issn = {2078-9181},
journal = {Труды СПИИРАН},
keywords = {16-37-60055,elibrary,frccsc,hse,mypub,vak},
language = {russian},
mendeley-tags = {16-37-60055,elibrary,frccsc,hse,mypub,vak},
number = {2},
pages = {161--187},
title = {{Знаковый подход к задаче распределения ролей в коалиции когнитивных агентов}},
url = {http://proceedings.spiiras.nw.ru/ojs/index.php/sp/article/view/3665},
year = {2018}
}
@article{Hexmoor2006,
abstract = {Agent teaming and autonomy are foundational themes in multi-agent systems. Agents may work as singletons or they may work in environments where other agents exist. In multi-agent systems, agents may form teams by sharing common goals with other agents. Cooperation is essential for any collaborative, group activity. Beyond coordination and judicious role assignment, cooperation enables members of a team to be aware and account for collection of their goals as well as the performance of agents on individual goals. This paper presents a general model of cooperation and illustrates how it may enhance group performance. In this paper, we present results of an application of the concept of cooperation in a simulated swarm of reconnaissance urban UAVs that are tracking vehicles in an urban environment.},
author = {Hexmoor, Henry and Eluru, Swetha and Sabaa, Hadi},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Informatica (Ljubljana)/Hexmoor, Eluru, Sabaa/Hexmoor, Eluru, Sabaa - 2006 - Plan sharing Showcasing coordinated UAV formation flight.pdf:pdf},
issn = {03505596},
journal = {Informatica (Ljubljana)},
keywords = {Agents,Benevolence,Collaboration,Help,UAV},
number = {2},
pages = {183--192},
title = {{Plan sharing: Showcasing coordinated UAV formation flight}},
volume = {30},
year = {2006}
}
@article{Sutton2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sutton, Richard S. and Singh, Satinder},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Artificial Intelligence/Sutton, Precup, Singh/Sutton, Precup, Singh - 1999 - Between MDPs and Semi-MDPs A Framework for Temporal Abstraction in Reinforcement Learning.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {PhD Proposal},
keywords = {Option},
number = {1999},
pages = {181--211},
pmid = {25246403},
title = {{Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning}},
volume = {1},
year = {1999}
}
@article{Mueller2013,
abstract = {The hippocampus has long been thought to be critical in learning and representing the cognitive map, and thus support functions such as search, pathfinding and route planning. This work aims to demonstrate the utility of hippocampus-based neural networks in modeling human search task behavior. Human solutions to pathfinding problems are generally fast but approximate, in contrast to traditional AI approaches. In this paper, we report data on a human search task, and then examine a set of models, based upon the structure of the hippocampus, which use a goal scent mechanism similar to the optimal pathfinding algorithms used in artificial intelligence systems. We compare five distinct search models, and conclude that a goal scent model driven by multiple goals spread throughout the search space provides the best and most accurate account of the human data. This research suggests a convergence in traditional AI and biologically- inspired approaches to pathfinding that may be mutually beneficial. {\textcopyright} 2013 Elsevier B.V.},
author = {Mueller, Shane T. and Perelman, Brandon S. and Simpkins, Benjamin G.},
doi = {10.1016/j.bica.2013.05.002},
file = {::},
issn = {2212683X},
journal = {Biologically Inspired Cognitive Architectures},
keywords = {Hippocampus,Pathfinding},
pages = {94--111},
publisher = {Elsevier B.V.},
title = {{Pathfinding in the cognitive map: Network models of mechanisms for search and planning}},
url = {http://dx.doi.org/10.1016/j.bica.2013.05.002},
volume = {5},
year = {2013}
}
@article{Yang2007,
author = {Yang, Jingan and Luo, Zhenghu},
doi = {10.1016/j.asoc.2006.04.004},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Applied Soft Computing/Yang, Luo/Yang, Luo - 2007 - Coalition formation mechanism in multi-agent systems based on genetic algorithms.pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing},
keywords = {agent coalition formation,chromosome encoding,crossover and mutation,genetic algorithm,multiagent system},
number = {2},
pages = {561--568},
title = {{Coalition formation mechanism in multi-agent systems based on genetic algorithms}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1568494606000421},
volume = {7},
year = {2007}
}
@article{Nichol2018,
abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
archivePrefix = {arXiv},
arxivId = {1803.02999},
author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
eprint = {1803.02999},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Nichol, Achiam, Schulman/Nichol, Achiam, Schulman - 2018 - On First-Order Meta-Learning Algorithms.pdf:pdf},
pages = {1--15},
title = {{On First-Order Meta-Learning Algorithms}},
url = {http://arxiv.org/abs/1803.02999},
year = {2018}
}
@article{Potamianos2005a,
author = {Potamianos, Alexandros and Narayanan, Shrikanth and Member, Senior and Riccardi, Giuseppe},
doi = {10.1007/978-3-540-49127-9_35},
isbn = {9781598295993},
issn = {1947-4040},
journal = {Audio},
number = {3},
pages = {321--329},
title = {{Spoken Dialogue Systems}},
volume = {13},
year = {2005}
}
@article{Chandrasekaran2017,
author = {Chandrasekaran, Muthukumaran and Doshi, Prashant and Zeng, Yifeng and Chen, Yingke},
doi = {10.1007/s10458-016-9354-4},
file = {::},
issn = {1387-2532},
journal = {Autonomous Agents and Multi-Agent Systems},
keywords = {Multiagent systems,Ad hoc teamwork,Sequential deci},
number = {4},
pages = {821--860},
publisher = {Springer US},
title = {{Can bounded and self-interested agents be teammates? Application to planning in ad hoc teams}},
url = {http://link.springer.com/10.1007/s10458-016-9354-4},
volume = {31},
year = {2017}
}
@article{Tamar2016,
abstract = {We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.},
archivePrefix = {arXiv},
arxivId = {1602.02867},
author = {Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
eprint = {1602.02867},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/arXiv/Tamar et al/Tamar et al. - 2016 - Value Iteration Networks.pdf:pdf},
journal = {arXiv},
month = {feb},
pages = {1--14},
title = {{Value Iteration Networks}},
url = {http://arxiv.org/abs/1602.02867},
year = {2016}
}
@article{Parr1998,
abstract = {We present a new approach to reinforcement learning in which the poli- cies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework inwhich knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learn- ing and “behavior-based” or “teleo-reactive” approaches to control. We present provably convergent algorithms for problem-solving and learn- ing with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states. 1},
author = {Parr, Ronald and Russell, Stuart},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Neural Information Processing Systems (NIPS)/Parr, Russell/Parr, Russell - 1998 - Reinforcement learning with hierarchies of machines.pdf:pdf},
isbn = {0-262-10076-2},
issn = {0031-8116},
journal = {Neural Information Processing Systems (NIPS)},
pages = {1043--1049},
title = {{Reinforcement learning with hierarchies of machines}},
url = {http://www.cs.berkeley.edu/{~}russell/classes/cs294/s11/readings/Parr+Russell:1998.pdf},
year = {1998}
}
@article{Panov2015g,
abstract = {Рассматриваются особенности представления пространственных и временных знаний для планирования согласованного перемещения коалиции интеллектуальных агентов. В качестве примера подобной задачи рассмотрен перелет группы беспилотных летательных аппаратов в заданную область на местности с препятствиями. Рассматривается знаковый подход к описанию знаний агента, основанный на психологической теории деятельности и биологически правдоподобной иерархической модели распознавания. Особое внимание уделено описанию действий и процессам обучения на основе поступающей первичной информации.},
author = {Панов, А. И.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Робототехника и техническая кибернетика/Панов/Панов - 2015 - Представление знаний автономных агентов, планирующих согласованные перемещения.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Робототехника и техническая кибернетика/Панов/Панов - 2015 - Представление знаний автономных агентов, планирующих согласованные перемещения.docx:docx},
journal = {Робототехника и техническая кибернетика},
keywords = {14-07-31194,elibrary,mypub,группа БПЛА,знак,иерархия автоматов,когнитивные процессы,представление знаний},
language = {russian},
mendeley-tags = {14-07-31194,elibrary,mypub},
number = {4(9)},
pages = {34--40},
title = {{Представление знаний автономных агентов, планирующих согласованные перемещения}},
year = {2015}
}
@article{Donadello2017,
abstract = {Semantic Image Interpretation (SII) is the task of extracting structured semantic descriptions from images. It is widely agreed that the combined use of visual data and background knowledge is of great importance for SII. Recently, Statistical Relational Learning (SRL) approaches have been developed for reasoning under uncertainty and learning in the presence of data and rich knowledge. Logic Tensor Networks (LTNs) are an SRL framework which integrates neural networks with first-order fuzzy logic to allow (i) efficient learning from noisy data in the presence of logical constraints, and (ii) reasoning with logical formulas describing general properties of the data. In this paper, we develop and apply LTNs to two of the main tasks of SII, namely, the classification of an image's bounding boxes and the detection of the relevant part-of relations between objects. To the best of our knowledge, this is the first successful application of SRL to such SII tasks. The proposed approach is evaluated on a standard image processing benchmark. Experiments show that the use of background knowledge in the form of logical constraints can improve the performance of purely data-driven approaches, including the state-of-the-art Fast Region-based Convolutional Neural Networks (Fast R-CNN). Moreover, we show that the use of logical background knowledge adds robustness to the learning system when errors are present in the labels of the training data.},
archivePrefix = {arXiv},
arxivId = {1705.08968},
author = {Donadello, Ivan and Serafini, Luciano and d'Avila Garcez, Artur},
eprint = {1705.08968},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI'17)/Donadello, Serafini, Garcez/Donadello, Serafini, Garcez - 2017 - Logic Tensor Networks for Semantic Image Interpret.pdf:pdf},
journal = {Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI'17)},
keywords = {Machine Learning: Knowledge-based Learning,Robotics and Vision: Vision and Perception,Uncertainty in AI: Uncertainty in AI},
pages = {1596--1602},
title = {{Logic Tensor Networks for Semantic Image Interpretation}},
url = {http://arxiv.org/abs/1705.08968},
year = {2017}
}
@article{Hiraoka2017,
author = {Hiraoka, Takuya and Neubig, Graham and Yoshino, Koichiro and Toda, Tomoki and Nakamura, Satoshi},
doi = {10.1007/978-981-10-2585-3_5},
isbn = {9789811025846},
issn = {18761119},
journal = {Lecture Notes in Electrical Engineering},
keywords = {Active learning,Dialog management,Example-based dialog},
pages = {67--78},
title = {{Active learning for example-based dialog systems}},
volume = {999 LNEE},
year = {2017}
}
@article{Branting2004,
abstract = {Two key objectives of conversational case-based reasoning (CCBR) systems are (1) eliciting case facts in a manner that minimizes the user's burden in terms of resources such as time, information cost, and cognitive load, and (2) integrating CBR with other problem solving modalities. This paper proposes an architecture that addresses both these goals by integrating CBR with a discourse-oriented dialogue engine. The dialogue engine determines when CBR or other problem-solving tech- niques are needed to achieve pending discourse goals. Conversely, the CBR component has the full resources of a dialogue engine to handle topic changes, interruptions, clarification questions by either the user or the system, and other speech acts that arise in problem-solving dialogues. 1},
author = {Branting, Karl and Lester, James and Mott, Bradford},
doi = {10.1007/978-3-540-28631-8_7},
isbn = {3-540-22882-9},
issn = {03029743},
journal = {Advances in Case-Based Reasoning},
title = {{Dialogue management for conversational case-based reasoning}},
url = {http://www.springerlink.com/index/e5369pe8c4yref2g.pdf},
year = {2004}
}
@article{Gorodetsky2012,
author = {Городецкий, В И},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Известия РАН. Теория и системы управления/Городецкий/Городецкий - 2012 - Самоорганизация и многоагентные системы. I. Модели многоагентной самоорганизации.pdf:pdf},
journal = {Известия РАН. Теория и системы управления},
language = {russian},
number = {2},
pages = {92--120},
title = {{Самоорганизация и многоагентные системы. I. Модели многоагентной самоорганизации}},
year = {2012}
}
@inproceedings{Kiselev2018b,
address = {Севастополь},
author = {Киселев, Г. А. and Панов, А. И.},
booktitle = {Интеллектуальные системы, управление и мехатроника – 2018: Ма-териалы Всероссийской научн.-техн. конф., Севастополь 29-31 мая 2018 г.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Интеллектуальные системы, управление и мехатроника – 2018 Ма-териалы Всероссийской научн.-техн. конф., Севастополь 29-31 мая 2018 г/Киселев, Панов/Киселев, Панов - 2018 - Семиотическое представление простран.docx:docx;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Интеллектуальные системы, управление и мехатроника – 2018 Ма-териалы Всероссийской научн.-техн. конф., Севастополь 29-31 мая 2018 г/Киселев, Панов/Киселев, Панов - 2018 - Семиотическое представление пространс.pdf:pdf},
keywords = {17-07-00281,17-29-07053,frccsc,myconf},
language = {russian},
mendeley-tags = {17-07-00281,17-29-07053,frccsc,myconf},
pages = {91--97},
publisher = {Изд-во СевГУ},
title = {{Семиотическое представление пространственных отношений для задачи интеллектуального перемещения}},
url = {https://elibrary.ru/item.asp?id=35263035},
year = {2018}
}
@article{Kaelbling2013,
abstract = {We describe an integrated strategy for planning, perception, state estimation and action in complex mobile manipulation domains based on planning in the belief space of probability distributions over states using hierarchical goal regression (pre-image back-chaining). We develop a vocabulary of logical expressions that describe sets of belief states, which are goals and subgoals in the planning process. We show that a relatively small set of symbolic operators can give rise to task-oriented perception in support of the manipulation goals. An implementation of this method is demonstrated in simulation and on a real PR2 robot, showing robust, flexible solution of mobile manipulation problems with multiple objects and substantial uncertainty.},
author = {Kaelbling, Leslie Pack and Lozano-P{\'{e}}rez, Tomas},
doi = {10.1177/0278364913484072},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/The International Journal of Robotics Research/Kaelbling, Lozano-P{\'{e}}rez/Kaelbling, Lozano-P{\'{e}}rez - 2013 - Integrated task and motion planning in belief space.pdf:pdf},
isbn = {0278-3649},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
number = {9-10},
pages = {1194--1227},
title = {{Integrated task and motion planning in belief space}},
url = {http://ijr.sagepub.com/content/32/9-10/1194.abstract?etoc},
volume = {32},
year = {2013}
}
@article{Epstein2015,
abstract = {Background: Optimal navigation for a simulated robot relies on a detailed map and explicit path planning. This is problematic for realworld robots, whose sensors and actuators are subject to noise and error, and whose environment may be dynamic. This paper reports on robots that rely on local spatial perception, learning, and commonsense rationales instead. Aims: Our thesis is that spatial abstractions learned from local sensing can support effective, autonomous robot navigation. Method: The simulated robot experiences real-world actuator error while it navigates autonomously. The robot's decision-making cognitive architecture relies on reactive and heuristic procedures based on simple rationales and spatial abstractions of where it has been. As the robot travels, it learns (and shares) affordances that facilitate movement, including perceived unobstructed areas and trail markers. Together they represent the environment but do not constitute a map. Robots navigate to five sets of targets in each of three environments. Results: This approach quickly produces efficient travel without planning or a map. Metrics include travel time, decision time, and distance. Experiments examine the impact of each kind of affordance. Comparison with a traditional A∗ planner shows that performance becomes only slightly suboptimal for one robot. Preliminary data with multiple robots suggest that, because our approach does need to replan when actuators err or robots threaten to collide, it will outperform the traditional approach. Conclusions: People have been shown not to navigate from detailed mental maps. Robots can also learn to navigate well without them, when they learn from local percepts.},
author = {Epstein, Susan L. and Aroor, Anoop and Evanusa, Matthew and Sklar, Elizabeth I. and Parsons, Simon},
doi = {10.1007/s10339-015-0713-x},
file = {::},
issn = {16124790},
journal = {Cognitive Processing},
keywords = {Qualitative reasoning,Robot navigation,Spatial abstractions,Spatial model},
pages = {215--219},
publisher = {Springer Berlin Heidelberg},
title = {{Spatial abstraction for autonomous robot navigation}},
volume = {16},
year = {2015}
}
@article{Silver2013,
abstract = {Lifelong Machine Learning Systems: Beyond Learning Algorithms},
author = {Silver, Daniel L and Yang, Qiang and Li, Lianghao},
file = {::},
isbn = {9781577356028},
journal = {AAAI Spring Symposium Series},
keywords = {AAAI Technical Report SS-13-05},
number = {Solomonoff 1989},
pages = {49--55},
title = {{Lifelong Machine Learning Systems : Beyond Learning Algorithms}},
year = {2013}
}
@article{Simsek2005,
abstract = {We present a new subgoal-based method for automatically creating useful skills in reinforcement learning. Our method identifies subgoals by partitioning local state transition graphs—those that are constructed using only the most recent experiences of the agent. The local scope of our subgoal discovery method allows it to successfully identify the type of subgoals we seek—states that lie between two densely-connected regions of the state space—while producing an algorithm with low computational cost.},
author = {Şimşek, {\"{O}}zg{\"{u}}r and Wolfe, Alicia P. and Barto, Andrew G.},
doi = {10.1145/1102351.1102454},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 22nd international conference on Machine learning - ICML '05/Şimşek, Wolfe, Barto/Şimşek, Wolfe, Barto - 2005 - Identifying useful subgoals in reinforcement learning by local graph partitio.pdf:pdf},
isbn = {1595931805},
journal = {Proceedings of the 22nd international conference on Machine learning  - ICML '05},
pages = {816--823},
title = {{Identifying useful subgoals in reinforcement learning by local graph partitioning}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102454},
year = {2005}
}
@article{Botvinick2009a,
abstract = {Research on human and animal behavior has long emphasized its hierarchical structure-the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior.},
author = {Botvinick, Matthew M. and Niv, Yael and Barto, Andrew C.},
doi = {10.1016/j.cognition.2008.08.011},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Cognition/Botvinick, Niv, Barto/Botvinick, Niv, Barto - 2009 - Hierarchically organized behavior and its neural foundations a reinforcement learning perspective.pdf:pdf},
issn = {1873-7838},
journal = {Cognition},
keywords = {Animals,Humans,Models,Nerve Net,Nerve Net: physiology,Prefrontal Cortex,Prefrontal Cortex: physiology,Problem Solving,Problem Solving: physiology,Psychological,Reinforcement (Psychology)},
number = {3},
pages = {262--80},
pmid = {18926527},
title = {{Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2783353{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {113},
year = {2009}
}
@inproceedings{Hayes2015,
abstract = {In this work, we present an algorithm for improv- ing collaborator performance on sequential manipulation tasks. Our agent-decoupled, optimization-based, task and motion planning approach merges considerations derived from both symbolic and geometric planning domains. This results in the generation of supportive behaviors enabling a teammate to reduce cognitive and kinematic burdens during task completion. We describe our algorithm alongside representative use cases, with an evaluation based on solving complex circuit building problems. We conclude with a discussion of applications and extensions to human-robot teaming scenarios.},
author = {Hayes, Bradley and Scassellati, Brian},
booktitle = {International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2015.7354288},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/International Conference on Intelligent Robots and Systems/Hayes, Scassellati/Hayes, Scassellati - 2015 - Effective Robot Teammate Behaviors for Supporting Sequential Manipulation Tasks.pdf:pdf},
isbn = {9781479999934},
issn = {21530866},
pages = {6374--6380},
title = {{Effective Robot Teammate Behaviors for Supporting Sequential Manipulation Tasks}},
url = {http://www.bradhayes.info/papers/iros15.pdf},
year = {2015}
}
@article{Riedmiller2018,
abstract = {We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.},
archivePrefix = {arXiv},
arxivId = {1802.10567},
author = {Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and {Van de Wiele}, Tom and Mnih, Volodymyr and Heess, Nicolas and Springenberg, Jost Tobias},
eprint = {1802.10567},
file = {::},
issn = {1938-7228},
number = {table 1},
title = {{Learning by Playing - Solving Sparse Reward Tasks from Scratch}},
url = {http://arxiv.org/abs/1802.10567},
volume = {48},
year = {2018}
}
@article{Fox2017,
abstract = {Augmenting an agent's control with useful higher-level behaviors called options can greatly reduce the sample complexity of reinforcement learning, but manually designing options is infeasible in high-dimensional and abstract state spaces. While recent work has proposed several techniques for automated option discovery, they do not scale to multi-level hierarchies and to expressive representations such as deep networks. We present Discovery of Deep Options (DDO), a policy-gradient algorithm that discovers parametrized options from a set of demonstration trajectories, and can be used recursively to discover additional levels of the hierarchy. The scalability of our approach to multi-level hierarchies stems from the decoupling of low-level option discovery from high-level meta-control policy learning, facilitated by under-parametrization of the high level. We demonstrate that using the discovered options to augment the action space of Deep Q-Network agents can accelerate learning by guiding exploration in tasks where random actions are unlikely to reach valuable states. We show that DDO is effective in adding options that accelerate learning in 4 out of 5 Atari RAM environments chosen in our experiments. We also show that DDO can discover structure in robot-assisted surgical videos and kinematics that match expert annotation with 72{\%} accuracy.},
archivePrefix = {arXiv},
arxivId = {1703.08294},
author = {Fox, Roy and Krishnan, Sanjay and Stoica, Ion and Goldberg, Ken},
eprint = {1703.08294},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Fox et al/Fox et al. - 2017 - Multi-Level Discovery of Deep Options.pdf:pdf},
isbn = {9781510827806},
title = {{Multi-Level Discovery of Deep Options}},
url = {http://arxiv.org/abs/1703.08294},
year = {2017}
}
@article{Roy2005,
author = {Roy, D.},
doi = {10.1016/j.artint.2005.04.007},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Artificial Intelligence/Roy/Roy - 2005 - Semiotic schemas A framework for grounding language in action and perception.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {1,action,and consequently our ability,and the physical world,cross-modal,embodied,grounding,language,language and meaning,meaning,multimodal,perception,representation,schemas,semiotic,semiotics,situated,the relationship between words,to},
mendeley-tags = {semiotics},
number = {1-2},
pages = {170--205},
title = {{Semiotic schemas: A framework for grounding language in action and perception}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0004370205001037},
volume = {167},
year = {2005}
}
@incollection{Castro2012,
abstract = {Abstract. Temporally extended actions are usually effective in speeding up reinforcement learning. In this paper we present a mechanism for automatically constructing such actions, expressed as options [Sutton et al., 1999], in a finite Markov Decision Process (MDP). To do this, we compute a bisimulation metric [Ferns et al., 2004] between the states in a small MDP and the states in a large MDP, which we want to solve. The shape of this metric is then used to completely define a set of options for the large MDP. We demonstrate empirically that our approach is able to improve the speed of reinforcement learning, and is generally not sensitive to parameter tuning.},
author = {Castro, Pablo Samuel and Precup, Doina},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-29946-9_16},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Castro, Precup/Castro, Precup - 2012 - Automatic Construction of Temporally.pdf:pdf},
isbn = {9783642299452},
issn = {03029743},
pages = {140--152},
title = {{Automatic Construction of Temporally Extended Actions for MDPs Using Bisimulation Metrics}},
url = {http://link.springer.com/10.1007/978-3-642-29946-9{\_}16},
volume = {7188 LNAI},
year = {2012}
}
@article{Florensa2017a,
abstract = {Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.},
archivePrefix = {arXiv},
arxivId = {1705.06366},
author = {Florensa, Carlos and Held, David and Geng, Xinyang and Abbeel, Pieter},
doi = {arXiv:1705.06366v3},
eprint = {1705.06366},
file = {::},
isbn = {076453601X},
issn = {1938-7228},
title = {{Automatic Goal Generation for Reinforcement Learning Agents}},
url = {http://arxiv.org/abs/1705.06366},
year = {2017}
}
@book{Osipov2009,
address = {М.},
author = {Осипов, Г. С.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Осипов/Осипов - 2009 - Лекции по искусственному интеллекту.pdf:pdf},
keywords = {osipov},
language = {russian},
mendeley-tags = {osipov},
pages = {266},
publisher = {УРСС},
title = {{Лекции по искусственному интеллекту}},
year = {2009}
}
@article{Osipov2015d,
abstract = {Рассматриваются процедуры формирования элемента картины мира субъекта деятельности (знака), введенные в первой части работы. Исследуется процесс формирование пары образ – значение знака с учетом современных представлений о строении и функционировании коры головного мозга человека. Строится алгоритм синтеза плана поведения и предлагается новая архитектура интеллектуальных агентов, обладающих, в частности, способностями к распределению ролей в коалициях.},
author = {Осипов, Г. С. and Панов, А. И. and Чудова, Н. В.},
doi = {10.7868/S0002338815050108},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Известия Российский академии наук. Теория и системы управления/Осипов, Панов, Чудова/Осипов, Панов, Чудова - 2015 - Управление поведением как функция сознания. II. Синтез плана поведения.pdf:pdf},
isbn = {0002338815},
journal = {Известия РАН. Теория и системы управления},
keywords = {14-11-00692,15-07-06214,elibrary,mypub,vak},
language = {russian},
mendeley-tags = {14-11-00692,15-07-06214,elibrary,mypub,vak},
number = {6},
pages = {47--61},
title = {{Управление поведением как функция сознания. II. Синтез плана поведения}},
year = {2015}
}
@article{Osipov2017a,
abstract = {В соответствии с современными взглядами на возникновение психических функций и на роль в этом нейрофизиологических процессов, формирование психических функций связывается с существованием или синтезом в процессе коммуникации специальных информационных структур, содержащих три различных по происхождению вида информации: информации, поступающей из внешней среды, информации, извлекаемой из памяти, и информации, приходящей из центров мотивации. Связывание таких компонент в единое целое обеспечивается их именованием; оно же обеспечивает устойчивость возникающих структур. Такие информационные структуры были названы нами знаками ввиду их сходства с аналогичными структурами, изучаемыми в семиотике. Множество знаков, формируемых субъектом в процессе деятельности и коммуникации, образует его знаковую картину мира, отражающую его представления о внешней среде, о себе и о других субъектах.Знаковая картина мира позволяет поставить и решить ряд задач, возникающих при моделировании поведения интеллектуальных агентов и их коалиций, таких как задачи целеполагания, синтеза целенаправленного поведения, распределения ролей и взаимодействия агентов в коалиции. В работе рассматривается специальный объект - каузальная матрица, с помощью которой описывается строение компонент знака. На этой основе определяются операции и отношения в знаковой картине мира, моделирующие психологические особенности поведения человека.},
author = {Осипов, Г. С. and Панов, А. И.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Искусственный интеллект и принятие решений/Осипов, Панов/Осипов, Панов - 2017 - Отношения и операции в знаковой картине мира субъекта поведения(2).docx:docx;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Искусственный интеллект и принятие решений/Осипов, Панов/Осипов, Панов - 2017 - Отношения и операции в знаковой картине мира субъекта поведения.pdf:pdf},
journal = {Искусственный интеллект и принятие решений},
keywords = {15-37-20893,16-37-60055,elibrary,frccsc,mypub,osipov,vak},
language = {russian},
mendeley-tags = {15-37-20893,16-37-60055,elibrary,frccsc,mypub,osipov,vak},
number = {4},
pages = {5--22},
title = {{Отношения и операции в знаковой картине мира субъекта поведения}},
url = {http://aidt.ru/index.php?option=com{\_}content{\&}view=article{\&}id=775:g-s-osipov-a-i-panov-otnosheniya-i-operatsii-v-znakovoj-kartine-mira-sub-ekta-povedeniya{\&}catid=321:kognitivnoe-modelirovanie{\&}Itemid=200{\&}lang=ru https://elibrary.ru/item.asp?id=30771439},
year = {2017}
}
@inproceedings{Mehta2008b,
abstract = {We present an algorithm, HI-MAT (Hierar- chy Induction via Models And Trajectories), that discovers MAXQ task hierarchies by ap- plying dynamic Bayesian network models to a successful trajectory from a source rein- forcement learning task. HI-MAT discovers subtasks by analyzing the causal and tem- poral relationships among the actions in the trajectory. Under appropriate assumptions, HI-MAT induces hierarchies that are consis- tent with the observed trajectory and have compact value-function tables employing safe state abstractions. We demonstrate empir- ically that HI-MAT constructs compact hi- erarchies that are comparable to manually- engineered hierarchies and facilitate signifi- cant speedup in learning when transferred to a target task.},
address = {New York, New York, USA},
author = {Mehta, Neville and Ray, Soumya and Tadepalli, Prasad and Dietterich, Thomas},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390238},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 25th international conference on Machine learning - ICML '08/Mehta et al/Mehta et al. - 2008 - Automatic discovery and transfer of MAXQ hierarchies.pdf:pdf},
isbn = {9781605582054},
pages = {648--655},
publisher = {ACM Press},
title = {{Automatic discovery and transfer of MAXQ hierarchies}},
url = {http://portal.acm.org/citation.cfm?id=1390238 http://portal.acm.org/citation.cfm?doid=1390156.1390238},
year = {2008}
}
@article{Levine2016,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
archivePrefix = {arXiv},
arxivId = {1504.00702},
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1504.00702},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Journal of Machine Learning Research/Levine et al/Levine et al. - 2016 - End-to-End Training of Deep Visuomotor Policies.pdf:pdf},
isbn = {9781479969227},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Neural Networks,Optimal Control,Reinforcement Learning,Vision},
pages = {1--40},
pmid = {15003161},
title = {{End-to-End Training of Deep Visuomotor Policies}},
volume = {17},
year = {2016}
}
@inproceedings{Brafman2015,
abstract = {To engage diverse agents in cooperative behavior, it is important, even necessary, to provide algo- rithms that do not reveal information that is private or proprietary. A number of recent planning algo- rithms enable agents to plan together for shared goals without disclosing information about their private state and actions. But these algorithms lack clear and formal privacy guarantees: the fact that they do not require agents to explicitly reveal pri- vate information, does not imply that such informa- tion cannot be deduced. The main contribution of this paper is an enhanced version of the distributed forward-search planning framework of Nissim and Brafman that reveals less information than the orig- inal algorithm, and the first, to our knowledge, dis- cussion and formal proof of privacy guarantees for distributed planning and search algorithms.},
author = {Brafman, Ronen I},
booktitle = {Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015)},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015)/Brafman/Brafman - 2015 - A Privacy Preserving Algorithm for Multi-Agent Planning and Search.pdf:pdf},
pages = {1530--1536},
title = {{A Privacy Preserving Algorithm for Multi-Agent Planning and Search}},
year = {2015}
}
@inproceedings{MacGlashan2010,
abstract = {We present skill bootstrapping, a proposed new research direction for agent learning and planning that allows an agent to start with low-level primitive actions, and develop skills that can be used for higher-level planning. Skills are developed over the course of solving many different problems in a domain, using reinforcement learning techniques to complement the benefits and disadvantages of heuristic-search planning. We describe the overall architecture of the proposed approach, discuss how it relates to other work, and give motivating examples for why this approach would be successful.},
author = {MacGlashan, James and DesJArdins, Marie},
booktitle = {24th AAAI Conference on Artificial Intelligence},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/24th AAAI Conference on Artificial Intelligence/MacGlashan, DesJArdins/MacGlashan, DesJArdins - 2010 - Hierarchical Skill Learning for High-Level Planning.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/24th AAAI Conference on Artificial Intelligence/MacGlashan, DesJArdins/MacGlashan, DesJArdins - 2010 - Hierarchical Skill Learning for High-Level Planning.pdf:pdf},
isbn = {9781577354666},
keywords = {abstraction,approximation,planning,reinforcement learning},
pages = {1988--1989},
title = {{Hierarchical Skill Learning for High-Level Planning}},
volume = {3},
year = {2010}
}
@article{Gordon,
author = {Gordon, Joshua},
title = {{Spoken Dialog System Architecture}}
}
@article{Osipov2014c,
abstract = {Рассматриваются функции, которые в психологии принято относить к функциям сознания. К числу таких функций относятся рефлексия, осознание мотива деятельности, целеполагание, синтез целенаправленного поведения и некоторые иные. Описание опирается на понятие знака, достаточно широко используемое в психологии, в частности, в культурно-исторической теории Л.С.Выготского, где знак понимается неформальным образом. В настоящей работе уточняется понятие знака, рассматриваются механизмы формирования знаков и некоторые процедуры самоорганизации на множестве знаков. Благодаря работе механизмов самоорганизации возникает новый способ представления картины мира субъекта деятельности. Вводится понятие семиотической сети, которое используется для исследования картин мира субъектов. Строятся модели некоторых из указанных выше функций. Вторая часть статьи посвящена функциям самосознания и применению построенных моделей для задачи синтеза плана и построения новых архитектур интеллектуальных агентов, обладающих, в частности, способностями к распределению ролей в коалициях.},
author = {Осипов, Г. С. and Панов, А. И. and Чудова, Н. В.},
doi = {10.7868/S000233881404012X},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Известия Российской академии наук. Теория и системы управления/Осипов, Панов, Чудова/Осипов, Панов, Чудова - 2014 - Управление поведением как функция сознания. I. Картина мира и целеполагание.doc:doc;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Известия Российской академии наук. Теория и системы управления/Осипов, Панов, Чудова/Осипов, Панов, Чудова - 2014 - Управление поведением как функция сознания. I. Картина мира и целеполагание.pdf:pdf},
issn = {0002-3388},
journal = {Известия Российской академии наук. Теория и системы управления},
keywords = {12-07-00611,elibrary,mypub,vak},
language = {russian},
mendeley-tags = {12-07-00611,elibrary,mypub,vak},
number = {4},
pages = {49--62},
title = {{Управление поведением как функция сознания. I. Картина мира и целеполагание}},
url = {http://elibrary.ru/item.asp?doi=10.7868/S000233881404012X},
year = {2014}
}
@article{Hayes2016b,
abstract = {Collaboration between humans and robots requires solutions to an array of challenging problems, including multi-agent planning, state estimation, and goal inference. There already exist feasible solutions for many of these challenges, but they depend upon having rich task models. In this work we detail a novel type of Hierarchical Task Network we call a Clique/Chain HTN (CC-HTN), alongside an algorithm for autonomously constructing them from topological properties derived from graphical task representations. As the presented method relies on the structure of the task itself, our work imposes no particular type of symbolic insight into motor primitives or environmental representation, making it applicable to a wide variety of use cases critical to human-robot interaction. We present evaluations within a multi-resolution goal inference task and a transfer learning application showing the utility of our approach.},
author = {Hayes, Bradley and Scassellati, Brian},
doi = {10.1109/ICRA.2016.7487760},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings - IEEE International Conference on Robotics and Automation/Hayes, Scassellati/Hayes, Scassellati - 2016 - Autonomously constructing hierarchical task networks for planning and human-robot collabor.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {5469--5476},
title = {{Autonomously constructing hierarchical task networks for planning and human-robot collaboration}},
volume = {2016-June},
year = {2016}
}
@misc{Presentation2016d,
author = {Панов, А. И.},
file = {::},
pages = {30},
title = {{Теория сознания Тонони}},
year = {2016}
}
@article{Cao2012,
abstract = {We describe an approach to incorporating Bayesian priors in the MAXQ framework for hierarchical reinforcement learning (HRL). We define priors on the primitive environment model and on task pseudo-rewards. Since models for composite tasks can be complex, we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy. We show empirically that (i) our approach results in improved convergence over non-Bayesian baselines, (ii) using both task hierarchies and Bayesian priors is better than either alone, (iii) taking advantage of the task hierarchy reduces the computational cost of Bayesian reinforcement learning and (iv) in this framework, task pseudo-rewards can be learned instead of being manually specified, leading to hierarchically optimal rather than recursively optimal policies.},
author = {Cao, F and Ray, Soumya},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Advances in Neural Information Processing Systems/Cao, Ray/Cao, Ray - 2012 - Bayesian hierarchical reinforcement learning.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {73--81},
title = {{Bayesian hierarchical reinforcement learning}},
url = {http://papers.nips.cc/paper/4752-bay},
year = {2012}
}
@article{Zubarev2013,
abstract = {В статье рассматриваются и анализируются различные подходы к построению архитектуры системы управления беспилотным летательным аппаратом типа «вертолет». На основании проведенного анализа делается вывод о целесообразности применения иерархической, трехуровневой схемы – стратегический уровень, тактический уровень, уровень управления. Описываются решаемые на каждом уровне задачи и дается краткая характеристика методов, подходов и алгоритмов, потенциально применимых для их решения, выделяются наиболее перспективные на взгляд авторов методы и подходы.},
author = {Зубарев, Д. В. and Макаров, Д. А. and Панов, А. И. and Яковлев, К. С.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Авиакосмическое приборостроение/Зубарев et al/Зубарев et al. - 2013 - Принципы построения многоуровневых архитектур систем управления беспилотными летательными аппаратами.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Авиакосмическое приборостроение/Зубарев et al/Зубарев et al. - 2013 - Принципы построения многоуровневых архитектур систем управления беспилотными летательными аппаратами.docx:docx},
journal = {Авиакосмическое приборостроение},
keywords = {12-07-00611,12-07-31057,12-07-31058,elibrary,mypub,алгоритмы управления,архитектура системы управления,беспилотные летательные аппараты,вертолеты,когнитивное компьютерное моделирование,методы планирования траекторий,регуляторы},
language = {russian},
mendeley-tags = {12-07-00611,12-07-31057,12-07-31058,elibrary,mypub},
number = {4},
pages = {10--28},
title = {{Принципы построения многоуровневых архитектур систем управления беспилотными летательными аппаратами}},
year = {2013}
}
@article{Daniel2010,
abstract = {jetyak; path planning; theta*},
archivePrefix = {arXiv},
arxivId = {arXiv:1401.3843},
author = {Daniel, Kenny and Nash, Alex and Koenig, Sven and Felner, Ariel},
doi = {10.1613/jair.2994},
eprint = {arXiv:1401.3843},
file = {::},
isbn = {1577353234},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {533--579},
title = {{Theta*: Any-angle path planning on grids}},
volume = {39},
year = {2010}
}
@inproceedings{Jakubuv2015,
abstract = {Multiagent planning is a coordination technique used for deliberative acting of a team of agents. One of vital planning techniques uses declarative description of agents' plans based on Finite State Machines and their later coordination by intersection of such machines with successive verification of the resulting joint plans. In this work, we firstly propose to use projections of agents' actions directly for multiagent planning based on iterative building of a coordinated multiagent plan. Secondly, we describe integration of the static analysis provided by process calculi type systems for approximate verification of exchanged local plans. Finally, we compare our approach with current state-of-the-art planner on an extensive benchmark set.},
author = {Jakubův, Jan and To{\v{z}}i{\v{c}}ka, Jan and Komenda, Anton{\'{i}}n},
booktitle = {Proceedings of the International Conference on Agents and Artificial Intelligence},
doi = {10.5220/0005222101730182},
file = {::},
isbn = {978-989-758-073-4},
pages = {173--182},
publisher = {SCITEPRESS - Science and and Technology Publications},
title = {{Multiagent Planning by Plan Set Intersection and Plan Verification}},
url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0005222101730182},
year = {2015}
}
@book{Osipov2018a,
abstract = {В монографии рассмотрено возникновение и формирование картин мира субъекта деятельности. Рассматривается понятие знака в качестве основного элемента картины мира. Приведены психологические и нейрофизиологические основания знаковой структуры картин мира. Строится модель знака, рассматриваются структура знака, семейства отношений и операций на множестве знаков. Показывается, что различные семейства отношений на множестве знаков позволяют моделировать различные типы картин мира. Продемонстрированы возможности знакового подхода в моделировании некоторых когнитивных функций, таких как целеполагание и динамическое распределение ролей в коалициях субъектов деятельности. Книга предназначена специалистам в области искусственного интеллекта, психологии, лингвистики и всем интересующимся проблемами моделирования человеческого сознания. Может быть использована аспирантами и студентами старших курсов университетов.},
address = {М.},
author = {Осипов, Г. С. and Панов, А. И. and Чудова, Н. В. and Кузнецова, Ю. М.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Осипов et al/Осипов et al. - 2018 - Знаковая картина мира субъекта поведения.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Осипов et al/Осипов et al. - 2018 - Знаковая картина мира субъекта поведения.pdf:pdf},
isbn = {978-5-9221-1781-4},
keywords = {15-07-06214,17-17-00138,frccsc,mybook,osipov},
language = {russian},
mendeley-tags = {15-07-06214,17-17-00138,frccsc,mybook,osipov},
pages = {264},
publisher = {Физматлит},
title = {{Знаковая картина мира субъекта поведения}},
url = {http://www.rfbr.ru/rffi/ru/books/o{\_}2052004},
year = {2018}
}
@article{Dimakopoulou2018a,
abstract = {We consider a team of reinforcement learning agents that concurrently operate in a common environment, and we develop an approach to efficient coordinated exploration that is suitable for problems of practical scale. Our approach builds on seed sampling (Dimakopoulou and Van Roy, 2018) and randomized value function learning (Osband et al., 2016). We demonstrate that, for simple tabular contexts, the approach is competitive with previously proposed tabular model learning methods (Dimakopoulou and Van Roy, 2018). With a higher-dimensional problem and a neural network value function representation, the approach learns quickly with far fewer agents than alternative exploration schemes.},
archivePrefix = {arXiv},
arxivId = {1805.08948},
author = {Dimakopoulou, Maria and Osband, Ian and {Van Roy}, Benjamin},
doi = {10.1.1.151.8250},
eprint = {1805.08948},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Dimakopoulou, Osband, Van Roy/Dimakopoulou, Osband, Van Roy - 2018 - Scalable Coordinated Exploration in Concurrent Reinforcement Learning.pdf:pdf},
isbn = {0262042088},
issn = {1049-5258},
title = {{Scalable Coordinated Exploration in Concurrent Reinforcement Learning}},
url = {http://arxiv.org/abs/1805.08948},
year = {2018}
}
@phdthesis{Rasmussen2014,
author = {Rasmussen, Daniel},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Rasmussen/Rasmussen - 2014 - Hierarchical reinforcement learning in a biologically plausible neural architecture.pdf:pdf},
pages = {175},
school = {Unversetu of Waterloo},
title = {{Hierarchical reinforcement learning in a biologically plausible neural architecture}},
year = {2014}
}
@article{Osipov2017a,
abstract = {В соответствии с современными взглядами на возникновение психических функций и на роль в этом нейрофизиологических процессов, формирование психических функций связывается с существованием или синтезом в процессе коммуникации специальных информационных структур, содержащих три различных по происхождению вида информации: информации, поступающей из внешней среды, информации, извлекаемой из памяти, и информации, приходящей из центров мотивации. Связывание таких компонент в единое целое обеспечивается их именованием; оно же обеспечивает устойчивость возникающих структур. Такие информационные структуры были названы нами знаками ввиду их сходства с аналогичными структурами, изучаемыми в семиотике. Множество знаков, формируемых субъектом в процессе деятельности и коммуникации, образует его знаковую картину мира, отражающую его представления о внешней среде, о себе и о других субъектах.Знаковая картина мира позволяет поставить и решить ряд задач, возникающих при моделировании поведения интеллектуальных агентов и их коалиций, таких как задачи целеполагания, синтеза целенаправленного поведения, распределения ролей и взаимодействия агентов в коалиции. В работе рассматривается специальный объект - каузальная матрица, с помощью которой описывается строение компонент знака. На этой основе определяются операции и отношения в знаковой картине мира, моделирующие психологические особенности поведения человека.},
author = {Осипов, Г. С. and Панов, А. И.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Искусственный интеллект и принятие решений/Осипов, Панов/Осипов, Панов - 2017 - Отношения и операции в знаковой картине мира субъекта поведения.pdf:pdf},
journal = {Искусственный интеллект и принятие решений},
keywords = {15-37-20893,16-37-60055,frccsc,mypub},
language = {russian},
mendeley-tags = {15-37-20893,16-37-60055,frccsc,mypub},
number = {4},
pages = {5--22},
title = {{Отношения и операции в знаковой картине мира субъекта поведения}},
url = {http://aidt.ru/index.php?option=com{\_}content{\&}view=article{\&}id=775:g-s-osipov-a-i-panov-otnosheniya-i-operatsii-v-znakovoj-kartine-mira-sub-ekta-povedeniya{\&}catid=321:kognitivnoe-modelirovanie{\&}Itemid=200{\&}lang=ru},
year = {2017}
}
@article{Ghavamzadeh2015,
author = {Ghavamzadeh, Mohammed and Mannor, Shie and Pineau, Joelle and Tamar, Aviv},
doi = {10.1561/2200000049},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Foundations and Trends in Machine Learning/Ghavamzadeh et al/Ghavamzadeh et al. - 2015 - Bayesian Reinforcement Learning A Survey.pdf:pdf},
isbn = {2200000049},
issn = {1935-8237},
journal = {Foundations and Trends in Machine Learning},
number = {5-6},
pages = {359--483},
title = {{Bayesian Reinforcement Learning: A Survey}},
url = {http://www.nowpublishers.com/article/Details/MAL-049},
volume = {8},
year = {2015}
}
@inproceedings{Davoodabadi2011,
abstract = {In this paper the problem of automatically discovering subtasks and hierarchies in reinforcement learning is considered. We present a novel method that allows an agent to autonomously discover subgoals and create a hierarchy from actions. Our method identifies subgoals by partitioning local state transition graphs. Options constructed for reaching these subgoals are added to action choices and used for accelerating the Q-Learning algorithm. Experimental results show significant performance improvements, especially in the initial learning phase.},
author = {Davoodabadi, M and Beigy, H},
booktitle = {Proceedings of the 5th Indian International Conference on Artificial Intelligence, IICAI 2011},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 5th Indian International Conference on Artificial Intelligence, IICAI 2011/Davoodabadi, Beigy/Davoodabadi, Beigy - 2011 - A new method for discovering subgoals and constructing options in r.pdf:pdf},
isbn = {9780972741286},
keywords = {Artificial intelligence,Autonomously discovering subgoals,Community detection,Hierarchical reinforcement learning,Learning algorithms,Learning phase,Local state,Option,Performance improvements,Q-learning algorithms,Reinforcement learning,Subgoals,Subtasks},
pages = {441--450},
title = {{A new method for discovering subgoals and constructing options in reinforcement learning}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84872195092{\&}partnerID=40{\&}md5=9bf17b3f8f09d77ec8cd05285124028d},
year = {2011}
}
@article{Milford2010,
abstract = {The challenge of persistent navigation and mapping is to develop an autonomous robot system that can simultaneously localize, map and navigate over the lifetime of the robot with little or no human intervention. Most solutions to the simultaneous localization and mapping (SLAM) problem aim to produce highly accurate maps of areas that are assumed to be static. In contrast, solutions for persistent navigation and mapping must produce reliable goal-directed navigation outcomes in an environment that is assumed to be in constant flux. We investigate the persistent navigation and mapping problem in the context of an autonomous robot that performs mock deliveries in a working office environment over a two-week period. The solution was based on the biologically inspired visual SLAM system, RatSLAM. RatSLAM performed SLAM continuously while interacting with global and local navigation systems, and a task selection module that selected between exploration, delivery, and recharging modes. The robot performed 1,143 delivery tasks to 11 different locations with only one delivery failure (from which it recovered), traveled a total distance of more than 40 km over 37 hours of active operation, and recharged autonomously a total of 23 times.},
author = {Milford, Michael and Wyeth, Gordon},
doi = {10.1177/0278364909340592},
file = {::},
isbn = {0278-3649},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {RatSLAM,SLAM,biologically inspired,persistent navigation and mapping},
number = {9},
pages = {1131--1153},
title = {{Persistent navigation and mapping using a biologically inspired slam system}},
volume = {29},
year = {2010}
}
@article{Steckel2013,
abstract = {We propose to combine a biomimetic navigation model which solves a simultaneous localization and mapping task with a biomimetic sonar mounted on a mobile robot to address two related questions. First, can robotic sonar sensing lead to intelligent interactions with complex environments? Second, can we model sonar based spatial orientation and the construction of spatial maps by bats? To address these questions we adapt the mapping module of RatSLAM, a previously published navigation system based on computational models of the rodent hippocampus. We analyze the performance of the proposed robotic implementation operating in the real world. We conclude that the biomimetic navigation model operating on the information from the biomimetic sonar allows an autonomous agent to map unmodified (office) environments efficiently and consistently. Furthermore, these results also show that successful navigation does not require the readings of the biomimetic sonar to be interpreted in terms of individual objects/landmarks in the environment. We argue that the system has applications in robotics as well as in the field of biology as a simple, first order, model for sonar based spatial orientation and map building},
author = {Steckel, Jan and Peremans, Herbert},
doi = {10.1371/journal.pone.0054076},
file = {::},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {1},
pmid = {23365647},
title = {{BatSLAM: Simultaneous Localization and Mapping Using Biomimetic Sonar}},
volume = {8},
year = {2013}
}
@article{Vig2006,
author = {Vig, Lovekesh and Adams, Julie A and Member, Senior},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IEEE Transactions on Robotics/Vig, Adams, Member/Vig, Adams, Member - 2006 - Multi-Robot Coalition Formation.pdf:pdf},
journal = {IEEE Transactions on Robotics},
number = {4},
pages = {637--649},
title = {{Multi-Robot Coalition Formation}},
volume = {22},
year = {2006}
}
@article{Besold2017,
abstract = {The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research.},
archivePrefix = {arXiv},
arxivId = {1711.03902},
author = {Besold, Tarek R. and d'Avila Garcez, Artur and Bader, Sebastian and Bowman, Howard and Domingos, Pedro and Hitzler, Pascal and Kuehnberger, Kai-Uwe and Lamb, Luis C. and Lowd, Daniel and Lima, Priscila Machado Vieira and de Penning, Leo and Pinkas, Gadi and Poon, Hoifung and Zaverucha, Gerson},
eprint = {1711.03902},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Besold et al/Besold et al. - 2017 - Neural-Symbolic Learning and Reasoning A Survey and Interpretation.pdf:pdf},
pages = {1--58},
title = {{Neural-Symbolic Learning and Reasoning: A Survey and Interpretation}},
url = {http://arxiv.org/abs/1711.03902},
year = {2017}
}
@article{Bai2017,
abstract = {In the context of hierarchical reinforcement learn-ing, the idea of hierarchies of abstract machines (HAMs) is to write a partial policy as a set of hierar-chical finite state machines with unspecified choice states, and use reinforcement learning to learn an optimal completion of this partial policy. Given a HAM with deep hierarchical structure, there of-ten exist many internal transitions where a machine calls another machine with the environment state unchanged. In this paper, we propose a new hier-archical reinforcement learning algorithm that au-tomatically discovers such internal transitions, and shortcircuits them recursively in the computation of Q values. The resulting HAMQ-INT algorithm outperforms the state of the art significantly on the benchmark Taxi domain and a much more complex RoboCup Keepaway domain.},
author = {Bai, Aijun and Russell, Stuart},
file = {::},
isbn = {9780999241103},
issn = {10450823},
journal = {Proc. of the 26th International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning: Reinforcement Learning,Planning and Scheduling: Markov Decisions Processe,Robotics and Vision: Multi-Robot Systems,Robotics and Vision: Robotics},
pages = {1418--1424},
title = {{Efficient Reinforcement Learning with Hierarchies of Machines by Leveraging Internal Transitions}},
year = {2017}
}
@article{Frans2017,
abstract = {We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives---policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.},
archivePrefix = {arXiv},
arxivId = {1710.09767},
author = {Frans, Kevin and Ho, Jonathan and Chen, Xi and Abbeel, Pieter and Schulman, John},
eprint = {1710.09767},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Frans et al/Frans et al. - 2017 - Meta Learning Shared Hierarchies.pdf:pdf},
pages = {1--11},
title = {{Meta Learning Shared Hierarchies}},
url = {http://arxiv.org/abs/1710.09767},
year = {2017}
}
@inproceedings{Goel2017,
author = {Goel, Karan and Mu, Tong and Brunskill, Emma},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Goel, Mu, Brunskill/Goel, Mu, Brunskill - 2017 - Optimal Hierarchical Policy Extraction From Noisy Imperfect Demonstrations.pdf:pdf},
title = {{Optimal Hierarchical Policy Extraction From Noisy Imperfect Demonstrations}},
url = {https://drive.google.com/file/d/101FsZkczKMfGeUBTP-089mhkTeepj8IW/view},
year = {2017}
}
@inproceedings{Singh2005,
author = {Singh, Satinder and Barto, Andrew G. and Chentanez, Nuttapong},
booktitle = {Proceeding of NIPS 2005},
doi = {10.1.1.123.395},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceeding of NIPS 2005/Singh, Barto, Chentanez/Singh, Barto, Chentanez - 2005 - Intrinsically motivated reinforcement learning.pdf:pdf},
pages = {1281--1288},
title = {{Intrinsically motivated reinforcement learning}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2005{\_}724.pdf},
year = {2005}
}
@article{Rasmussen2017a,
abstract = {We present the first model capable of performing hierarchical reinforcement learning in a general, neurally detailed imple-mentation. We show that this model is able to learn a spatial pickup and delivery task more quickly than one without hier-archical abilities. In addition, we show that this model is able to leverage its hierarchical structure to transfer learned knowl-edge between related tasks. These results point towards the advantages to be gained by using a hierarchical RL framework to understand the brain's powerful learning ability.},
author = {Rasmussen, Daniel and Voelker, Aaron and Eliasmith, Chris},
doi = {10.1371/journal.pone.0180234},
editor = {Cymbalyuk, Gennady},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/PLOS ONE/Rasmussen, Voelker, Eliasmith/Rasmussen, Voelker, Eliasmith - 2017 - A neural model of hierarchical reinforcement learning.pdf:pdf},
isbn = {1111111111},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {brain could achieve its,hierarchical,hrl into a theory,neural engineering framework,neural model,of neural function,providing a hypothe-,reinforcement learning,sis for how the,strengths in scaling},
month = {jul},
number = {7},
pages = {e0180234},
title = {{A neural model of hierarchical reinforcement learning}},
url = {http://dx.plos.org/10.1371/journal.pone.0180234},
volume = {12},
year = {2017}
}
@article{Tanaka1997,
author = {Tanaka, Fumihide and Yamamura, Masayuki},
file = {::},
journal = {6th European Workshop on Learning Robots},
pages = {93--99},
title = {{An approach to lifelong reinforcement learning through multiple environments}},
year = {1997}
}
@book{Hadad2013,
author = {Hadad, Meirav and Kraus, Sarit and {Ben-Arroyo Hartman}, Irith and Rosenfeld, Avi},
booktitle = {Annals of Mathematics and Artificial Intelligence},
doi = {10.1007/s10472-013-9363-9},
file = {::},
isbn = {1047201393639},
issn = {10122443},
keywords = {Artificial intelligence,Cooperation,Coordination,Multiagent system,Planning,Time constraints},
number = {3},
pages = {243--291},
title = {{Group planning with time constraints}},
volume = {69},
year = {2013}
}
@article{Panov2016b,
abstract = {Behavior planning is known to be one of the basic cognitive functions, which is essential for any cognitive architecture of any control system used in robotics. At the same time most of the widespread planning algorithms employed in those systems are developed using only approaches and models of Artificial Intelligence and don't take into account numerous results of cognitive experiments. As a result, there is a strong need for novel methods of behavior planning suitable for modern cognitive architectures aimed at robot control. One such method is presented in this work and is studied within a special class of navigation task called smart relocation task. The method is based on the hierarchical two-level model of abstraction and knowledge representation, e.g. symbolic and subsymbolic. On the symbolic level sign world model is used for knowledge representation and hierarchical planning algorithm, MAP, is utilized for planning. On the subsymbolic level the task of path planning is considered and solved as a graph search problem. Interaction between both planners is examined and inter-level interfaces and feedback loops are described. Preliminary experimental results are presented.},
author = {Panov, Aleksandr I. and Yakovlev, Konstantin S.},
doi = {10.1016/j.procs.2016.07.414},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Procedia Computer Science/Panov, Yakovlev/Panov, Yakovlev - 2016 - Psychologically Inspired Planning Method for Smart Relocation Task.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Procedia Computer Science/Panov, Yakovlev/Panov, Yakovlev - 2016 - Psychologically Inspired Planning Method for Smart Relocation Task(2).pdf:pdf},
issn = {1877-0509},
journal = {Procedia Computer Science},
keywords = {16-11-00048,behavior planning,computer cognitive modeling,map,myconf,path planning,planner,scopus,sign world model,theory of activity,wos{\_}core},
mendeley-tags = {16-11-00048,myconf,scopus,wos{\_}core},
pages = {115--124},
title = {{Psychologically Inspired Planning Method for Smart Relocation Task}},
url = {http://www.sciencedirect.com/science/article/pii/S1877050916316702},
volume = {88},
year = {2016}
}
@article{Dietterich2000,
abstract = {This paper presents a new approach to hierarchical reinforcement learning based on de-composing the target Markov decision process MDP into a hierarchy o f smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The decomposition, known as the MAXQ decom-position, has both a procedural semantics|as a subroutine hierarchy|and a declarative semantics|as a representation of the value function of a hierarchical policy. MAXQ uniies and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. It is based on the assumption that the programmer can identify useful subgoals and deene subtasks that achieve these subgoals. By deening such subgoals, the programmer constrains the set of policies that need to be considered during reinforcement learning. The MAXQ v alue function decomposition can represent the value function of any policy that is consistent with the given hierarchy. The decomposition also creates oppor-tunities to exploit state abstractions, so that individual MDPs within the hierarchy can ignore large parts of the state space. This is important for the practical application of the method. This paper deenes the MAXQ hierarchy, proves formal results on its representa-tional power, and establishes sve conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges with probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the eve kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q with state abstractions converges to a recursively optimal policy much faster than nat Q learning. The fact that MAXQ learns a representation of the value function has an important beneet: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the eeectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoos in hierarchical reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:cs/9905014v1},
author = {Dietterich, Thomas G},
doi = {10.1613/jair.639},
eprint = {9905014v1},
file = {::},
isbn = {978-3-540-67839-7},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {227--303},
primaryClass = {arXiv:cs},
title = {{Hierarchical reinforcement learning with the MAXQ value function decomp osition}},
volume = {13},
year = {2000}
}
@book{Berg2014,
abstract = {Current spoken dialogue systems are often criticised because they lack natural behaviour. In this thesis, a model to facilitate the development of user-friendly dialogues for information and control systems (e.g. travel-booking or smart room control systems) is created in order to address this problem. This also includes a study about the users' preferences, the classification of utterances according to intention and answer type and the development of a dialogue engine that can process dialogues based on this model.$\backslash$r$\backslash$nThe developed model describes the dialogue flow and the combination of questions, answers and the resulting actions. Features like mixed initiative, open-ended questions, subdialogues and adaptable phrasings of system utterances lead to more natural dialogues and increase the usability of dialogue systems while linguistic datatypes, abstract question descriptions in connection with answer types and language generation methods enormously facilitate the definition of such dialogues for the developer. The separation of dialogue model and dialogue engine makes possible the reuse of base functionalities und prevents the mixing of execution logic and dialogue knowledge. The engine contains dialogue acts to classify user utterances and to prevent ambiguities as well as language understanding modules to identify the user's goal. In addition, it infers the next dialogue step under consideration of the specified dialogue behaviour.$\backslash$r$\backslash$nBy implementing the Natural Dialogue System (NADIA), that runs the XML-based model, the functionality is proven.},
author = {Berg, Markus M.},
isbn = {9783898385084},
keywords = {Dialogmodellierung,Sprachdialogsystem,Sprachverarbeitung,dialogue modelling,natural language processing,spoken dialogue system},
title = {{Modelling of Natural Dialogues in the Context of Speech-based Information and Control Systems}},
url = {http://macau.uni-kiel.de/receive/dissertation{\_}diss{\_}00016739},
year = {2014}
}
@inproceedings{Apeldoorn2015,
author = {Apeldoorn, Daan},
booktitle = {Proceedings of the 5th Workshop on Dynamics of Knowledge and Belief (DKB-2015) and the 4th Workshop KI {\&} Kognition (KIK-2015) co-located with 38th German Conference on Artificial Intelligence (KI-2015)},
editor = {Beierle, Christoph and Kern-Isberner, Gabriele and Ragni, Marco and Frieder, Stolzenburg},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 5th Workshop on Dynamics of Knowledge and Belief (DKB-2015) and the 4th Workshop KI {\&} Kognition (KIK-2015) co-located wit./Apeldoorn/Apeldoorn - 2015 - Learning Rules for Cooperative Solvin.pdf:pdf},
keywords = {cooperative problem solving,knowledge extraction,learning,multi-agent simulation,rule},
pages = {5--15},
title = {{Learning Rules for Cooperative Solving of Spatio-Temporal Problems}},
year = {2015}
}
@article{Mousavi2014,
abstract = {Reinforcement learning (RL) for solving large and complex problems faces the curse of dimensions problem. To overcome this problem, frameworks based on the temporal abstraction have been presented; each having their advantages and disadvantages. This paper proposes a new method like the strategies introduced in the hierarchical abstract machines (HAMs) to create a high-level controller layer of reinforcement learning which uses options. The proposed framework considers a non-deterministic automata as a controller to make a more effective use of temporally extended actions and state space clustering. This method can be viewed as a bridge between option and HAM frameworks, which tries to suggest a new framework to decrease the disadvantage of both by creating connection structures between them and at the same time takes advantages of them. Experimental results on different test environments show significant efficiency of the proposed method.},
author = {Mousavi, Seyed Sajad and Ghazanfari, Behzad and Mozayani, Nasser and Jahed-Motlagh, Mohammad Reza},
doi = {10.1016/j.asoc.2014.08.071},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Applied Soft Computing Journal/Mousavi et al/Mousavi et al. - 2014 - Automatic abstraction controller in reinforcement learning agent via automata.pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Cluster,Hierarchical reinforcement learning,Multi-agent learning,Reinforcement learning},
pages = {118--128},
publisher = {Elsevier B.V.},
title = {{Automatic abstraction controller in reinforcement learning agent via automata}},
url = {http://dx.doi.org/10.1016/j.asoc.2014.08.071},
volume = {25},
year = {2014}
}
@article{2017,
abstract = {Системы комического наблюдения являются важным источником информации, необ- ходимой для оперативного решения широкого круга задач гражданского и военного назначе- ния. Эффективность функционирования таких систем зависит от многих факторов. Наряду с множеством технических, информационных, технологических, инфраструктурных, эконо- мических и других факторов, их эффективность в значительной степени зависит от качест- ва планирования миссий космических аппаратов и управления в реальном времени процессом исполнения миссии. В работе сформулирована проблема построения самоорганизующейся системы группового управления поведением кластера малых спутников, реализующего авто- номное исполнение заявок на сервис по добыванию информации о наземных объектах средст- вами космического наблюдения. В ней предлагается новая концепция группового управления в системе космического наблюдения. В основу этой концепции положен принцип самоорганиза- ции группового поведения кластера спутников. Такая система управления оказывается в со- стоянии реализовать автономное планирование и оперативное управления космической груп- пировкой, в которой все базовые функции процесса управления реализуются ее бортовыми средствами. Теоретический фундамент этой концепции строится на моделях коллективной робототехники, которые в настоящее время активно развиваются в области многоагент- ных систем. В работе приведен краткий обзор современного состояния исследований в об- ласти систем управления кластерами малых спутников, дана достаточно общая постановка задачи, в которой кластер малых спутников рассматривается как полностью автономная система, предназначенная для выполнения заказов на сбор и доставку космической информа- ции о наземных объектах. При этом полагается, что система управления самостоятельно в реальном времени распределяет задачи наблюдения на множестве спутников группировки, планирует и составляет расписание выполнения наблюдений в соответствии с пространст- венно-временными требованиями заказчика, выполняет оперативное управление распреде- ленным исполнением построенного расписания и выполняет коррекцию распределения задач и расписания их выполнения при возникновении нештатных ситуаций. В работе дано деталь- ное описание разработанной концепции самоорганизующейся системы группового управления кластером малых спутников, и архитектуры ее программной реализации.},
author = {Городецкий, В. И. and Карасев, О. В.},
doi = {10.18522/2311-3103-2017-1-234247},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Известия ЮФУ. Технические науки/Городецкий, Карасев/Городецкий, Карасев - 2017 - Самоорганизация группового поведения кластера малых спутников распределенной системы наблюдения.pdf:pdf},
issn = {23113103},
journal = {Известия ЮФУ. Технические науки},
keywords = {Малый спутник,автономная миссия,блюдения,динамическая коммуникационная сеть.,задача наблюдения,коллективное поведение,парное взаимо- действие спутников,распределенная система на-,ресурсы спутника,самоорганизация},
language = {russian},
month = {jan},
number = {2},
pages = {234--247},
title = {{Самоорганизация группового поведения кластера малых спутников распределенной системы наблюдения}},
url = {http://izv-tn.tti.sfedu.ru/wp-content/uploads/2017/1/19.pdf},
volume = {187},
year = {2017}
}
@incollection{Hoek2007,
author = {van der Hoek, Wiebe and Wooldridge, Michael},
booktitle = {Handbook of Knowledge Representation},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Handbook of Knowledge Representation/Hoek, Wooldridge/Hoek, Wooldridge - 2007 - Multi-Agent Systems.pdf:pdf},
pages = {1--44},
title = {{Multi-Agent Systems}},
year = {2007}
}
@article{Panov2015g,
abstract = {Рассматриваются особенности представления пространственных и временных знаний для планирования согласованного перемещения коалиции интеллектуальных агентов. В качестве примера подобной задачи рассмотрен перелет группы беспилотных летательных аппаратов в заданную область на местности с препятствиями. Рассматривается знаковый подход к описанию знаний агента, основанный на психологической теории деятельности и биологически правдоподобной иерархической модели распознавания. Особое внимание уделено описанию действий и процессам обучения на основе поступающей первичной информации.},
author = {Панов, А. И.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Робототехника и техническая кибернетика/Панов/Панов - 2015 - Представление знаний автономных агентов, планирующих согласованные перемещения.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Робототехника и техническая кибернетика/Панов/Панов - 2015 - Представление знаний автономных агентов, планирующих согласованные перемещения.docx:docx},
journal = {Робототехника и техническая кибернетика},
keywords = {14-07-31194,elibrary,mypub,группа БПЛА,знак,иерархия автоматов,когнитивные процессы,представление знаний},
language = {russian},
mendeley-tags = {14-07-31194,elibrary,mypub},
number = {4(9)},
pages = {34--40},
title = {{Представление знаний автономных агентов, планирующих согласованные перемещения}},
url = {https://elibrary.ru/item.asp?id=25085187},
year = {2015}
}
@article{Lester2004,
abstract = {Conversational agents integrate computational linguistics techniques with the communi- cation channel of theWeb to interpret and respond to statements made by users in ordinary natural language. Web-based conversational agents deliver high-volumes of interactive text-based dialogs. Recent years have seen significant activity in enterprise-class conversational agents. This chap- ter describes the principal applications of conversational agents in the enterprise and the technical challenges posed by their design and large-scale deployments. These technical challenges fall into two categories: accurate and efficient natural-language processing; and the scalability, performance, reliability, integration, and maintenance requirements posed by enterprise deployments},
author = {Lester, James and Mott, Bradford},
isbn = {978-1584883814},
journal = {The Practical Handbook of Internet Computing},
pages = {220--240},
title = {{Conversational Agents}},
year = {2004}
}
@article{Mueller2013a,
abstract = {The hippocampus has long been thought to be critical in learning and representing the cognitive map, and thus support functions such as search, pathfinding and route planning. This work aims to demonstrate the utility of hippocampus-based neural networks in modeling human search task behavior. Human solutions to pathfinding problems are generally fast but approximate, in contrast to traditional AI approaches. In this paper, we report data on a human search task, and then examine a set of models, based upon the structure of the hippocampus, which use a goal scent mechanism similar to the optimal pathfinding algorithms used in artificial intelligence systems. We compare five distinct search models, and conclude that a goal scent model driven by multiple goals spread throughout the search space provides the best and most accurate account of the human data. This research suggests a convergence in traditional AI and biologically- inspired approaches to pathfinding that may be mutually beneficial. {\textcopyright} 2013 Elsevier B.V.},
author = {Mueller, Shane T. and Perelman, Brandon S. and Simpkins, Benjamin G.},
doi = {10.1016/j.bica.2013.05.002},
file = {::},
issn = {2212683X},
journal = {Biologically Inspired Cognitive Architectures},
keywords = {Hippocampus,Pathfinding},
pages = {94--111},
publisher = {Elsevier B.V.},
title = {{Pathfinding in the cognitive map: Network models of mechanisms for search and planning}},
url = {http://dx.doi.org/10.1016/j.bica.2013.05.002},
volume = {5},
year = {2013}
}
@article{Mcgovern2002,
abstract = {The ability to create and to use abstractions in complex environemnts, that is, to systematically ignore irrelevant details, is a key reason that humans are effective problem solvers. Although the utility of abstraction is commonly accepted, there has been relatively little research on autonomously discovering or creating useful abstractions. A system that can create new abstractions autonomously can learn and plan in situations that its original designer was not able to anticipate. This dissertation introduces two related methods that allow an agent to autonomously discover and create temporal abstractions from its accumulated experience with its environment. A temporal abstraction is an encapsulation of a complex set of actions into a single higher-level action that allows an agent to learn and plan while ignoring details that appear at finer levels of temporal resolution. The main idea of both methods is to search for patterns that occur frequently within an agent's accumulated successful experience and that do not occur in unsuccessful experiences. These patterns are used to create the new temporal abstractions. The two types of temporal abstractions that our methods create are 1) suboals and closed-loop policies for achieving them, and 2) open-loop policies, or action sequences, that are useful "macros." We demonstrate the utility of both types of temporal abstractions in several simulated tasks, including two simulated mobile robot tasks. We use these tasks to demonstrate that the autonomously created temporal abstractions can both facilitate the learning of an agent within a task and can enable effective knowledge transfer related tasks. As a larger task, we focus on the difficult problem of scheduling the assembly instructions for computers with multiple pipelines in such a manner that the reordered instructions will execute as quickly as possible. We demonstrate that the autonomously discovered action sequences can significantly improve performance of the scheduler and can enable effective knowledge transfer across similar processors. Both methods can extract the temporal abstractions from collections of behavioral trajectories generated by different processes. In particular, we demonstrate that the methods can be effective when applied to collections generated by reinforcement learning agents, heuristic searchers, and human tele-operators.},
author = {Mcgovern, Elizabeth Amy},
file = {::},
isbn = {0-493-71665-3},
issn = {16113349},
journal = {Power},
number = {May},
title = {{Autonomous discovery of temporal abstractions from interaction with an environment}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.136.3079{\&}rep=rep1{\&}type=pdf},
year = {2002}
}
@article{Makarov2015a,
abstract = {В работе рассмотрен ряд вопросов, возникающих в области автоматизации управления малыми беспилотными летательными аппаратами (БПЛА) мультироторного типа. Предложена совокупность методов планирования и управления, рассмотрена задача организации взаимодействия различных методов и алгоритмов (авторских и известных) в единую интеллектуальную систему управления БПЛА. Предлагается использование трех уровней управления – стратегического, тактического и реактивного, описывается соответствующая архитектура – STRL (от англ. strategic, tactical, reactive, layered). Использование этой архитектуры позволит автоматизировать управление коалициями БПЛА при решении широкого круга задач в различных средах.},
author = {Макаров, Д. А. and Панов, А. И. and Яковлев, К. С.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Искусственный интеллект и принятие решений/Макаров, Панов, Яковлев/Макаров, Панов, Яковлев - 2015 - Архитектура многоуровневой интеллектуальной системы управления беспилотными летательными аппаратами.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Искусственный интеллект и принятие решений/Макаров, Панов, Яковлев/Макаров, Панов, Яковлев - 2015 - Архитектура многоуровневой интеллектуальной системы управления беспилотными летательными аппаратами.doc:doc},
journal = {Искусственный интеллект и принятие решений},
keywords = {14-11-00692,elibrary,mypub,vak,wos{\_}rsci,беспилотные летательные аппараты,знаковая картина мира,интеллектуальная система управления,многоуровневая архитектура,нелинейное управление,планирование траектории,уравнение Риккати},
language = {russian},
mendeley-tags = {14-11-00692,elibrary,mypub,vak,wos{\_}rsci},
number = {3},
pages = {18--33},
title = {{Архитектура многоуровневой интеллектуальной системы управления беспилотными летательными аппаратами}},
year = {2015}
}
@incollection{Menache2002,
abstract = {We present the Q-Cut algorithm, a graph theoretic approach for automatic detection of sub-goals in a dynamic environment, which is used for acceleration of the Q-Learning algorithm. The learning agent creates an on-line map of the process history, and uses an efficient Max-Flow/Min-Cut algorithm for identifying bottlenecks. The policies for reaching bottlenecks are separately learned and added to the model in a form of options (macro-actions). We then extend the basic Q-Cut algorithm to the Segmented Q-Cut algorithm, which uses previously identified bottlenecks for state space partitioning, necessary for finding additional bottlenecks in complex environments. Experiments show significant performance improvements, particulary in the initial learning phase.},
author = {Menache, Ishai and Mannor, Shie and Shimkin, Nahum},
booktitle = {ECML 2002: Machine Learning: ECML 2002},
doi = {10.1007/3-540-36755-1_25},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/ECML 2002 Machine Learning ECML 2002/Menache, Mannor, Shimkin/Menache, Mannor, Shimkin - 2002 - Q-Cut—Dynamic Discovery of Sub-goals in Reinforcement Learning.pdf:pdf},
isbn = {978-3-540-44036-9, 978-3-540-36755-0},
issn = {16113349},
pages = {295--306},
title = {{Q-Cut—Dynamic Discovery of Sub-goals in Reinforcement Learning}},
url = {http://link.springer.com/10.1007/3-540-36755-1{\_}25},
year = {2002}
}
@article{Zanette2018,
abstract = {In order to make good decision under uncertainty an agent must learn from observations. To do so, two of the most common frameworks are Contextual Bandits and Markov Decision Processes (MDPs). In this paper, we study whether there exist algorithms for the more general framework (MDP) which automatically provide the best performance bounds for the specific problem at hand without user intervention and without modifying the algorithm. In particular, it is found that a very minor variant of a recently proposed reinforcement learning algorithm for MDPs already matches the best possible regret bound˜O bound˜ bound˜O(√ SAT) in the dominant term if deployed on a tabular Contextual Bandit problem despite the agent being agnostic to such setting.},
author = {Zanette, Andrea},
file = {::},
issn = {1938-7228},
journal = {Icml},
title = {{Problem Dependent Reinforcement Learning Bounds Which Can Identify Bandit Structure in MDPs}},
url = {http://proceedings.mlr.press/v80/zanette18a/zanette18a.pdf},
year = {2018}
}
@article{Smolensky2014,
abstract = {Mental representations have continuous as well as discrete, combinatorial aspects. For example, while predominantly discrete, phonological representations also vary continuously, as evidenced by instrumental studies of both grammatically-­‐‑induced sound alternations and speech errors. Can an integrated theoretical framework address both aspects of structure? The framework we introduce here, Gradient Symbol Processing, characterizes the emergence of grammatical macrostructure from the Parallel Distributed Processing microstructure (McClelland {\&} Rumelhart, 1986) of language processing. The mental representations that emerge, Distributed Symbol Systems, have both combinatorial and gradient structure. They are processed through Subsymbolic Optimization-­‐‑Quantization, in which an optimization process favoring representations that satisfy well-­‐‑formedness constraints operates in parallel with a distributed quantization process favoring discrete symbolic structures. We apply a particular instantiation of this framework, $\lambda$-­‐‑ Diffusion Theory, to phonological production. Simulations of the resulting model suggest that Gradient Symbol Processing offers a way to unify accounts of discrete grammatical competence with both discrete and continuous patterns in language performance.},
author = {Smolensky, Paul and Goldrick, Matthew and Mathis, Donald},
doi = {10.1111/cogs.12047},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Cognitive Science/Smolensky, Goldrick, Mathis/Smolensky, Goldrick, Mathis - 2014 - Optimization and quantization in gradient symbol systems A framework for integrating the continuous.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Combinatorial structure,Distributed representation,Harmonic grammar,Optimization,Selection,Speech errors},
number = {6},
pages = {1102--1138},
pmid = {23802807},
title = {{Optimization and quantization in gradient symbol systems: A framework for integrating the continuous and the discrete in cognition}},
volume = {38},
year = {2014}
}
@article{Schrodt2017,
author = {Schrodt, Fabian and Kneissler, Jan and Ehrenfeld, Stephan and Butz, Martin V.},
doi = {10.1111/tops.12252},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Topics in Cognitive Science/Schrodt et al/Schrodt et al. - 2017 - Mario Becomes Cognitive.pdf:pdf},
issn = {17568757},
journal = {Topics in Cognitive Science},
keywords = {artificial intelligence,cognitive architecture,cognitive science and games,self-},
pages = {1--31},
title = {{Mario Becomes Cognitive}},
url = {http://doi.wiley.com/10.1111/tops.12252},
year = {2017}
}
@article{Kiselev2018a,
abstract = {В настоящей работе рассмотрена задача распределения ролей при составлении общего плана действий в коалиции когнитивных агентов. Когнитивные агенты реализуют основные функции интеллектуального агента с использованием моделей когнитивных функций человека. В качестве психологических оснований построения моделей когнитивных функций использованы теория деятельности и формальная модель знаковой картины мира. В работе представлен оригинальный метод распределения ролей – алгоритм MultiMAP, основанный на знаковом способе планирования поведения агента. Представлены основные особенности описываемого подхода, включающие способы представления знаний агента о себе и о других агентах, способы знаковой коммуникации и сохранения опыта кооперации с другими агентами. Описаны модельные эксперименты, демонстрирующие основные преимущества представленного подхода и некоторые недостатки, на устранение которых направлена будущая работа в данном направлении.},
author = {Киселев, Г. А. and Панов, А. И.},
doi = {10.15622/sp.57.7},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Труды СПИИРАН/Киселев, Панов/Киселев, Панов - 2018 - Знаковый подход к задаче распределения ролей в коалиции когнитивных агентов.docx:docx;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Труды СПИИРАН/Киселев, Панов/Киселев, Панов - 2018 - Знаковый подход к задаче распределения ролей в коалиции когнитивных агентов.pdf:pdf},
issn = {2078-9181},
journal = {Труды СПИИРАН},
keywords = {16-37-60055,frccsc,hse,mypub,vak},
language = {russian},
mendeley-tags = {16-37-60055,frccsc,hse,mypub,vak},
number = {2},
pages = {161--187},
title = {{Знаковый подход к задаче распределения ролей в коалиции когнитивных агентов}},
url = {http://proceedings.spiiras.nw.ru/ojs/index.php/sp/article/view/3665},
year = {2018}
}
@article{Oizumi2014,
abstract = {This paper presents Integrated Information Theory (IIT) of consciousness 3.0, which incorporates several advances over previous formulations. IIT starts from phenomenological axioms: information says that each experience is specific – it is what it is by how it differs from alternative experiences; integration says that it is unified – irreducible to non- interdependent components; exclusion says that it has unique borders and a particular spatio-temporal grain. These axioms are formalized into postulates that prescribe how physical mechanisms, such as neurons or logic gates, must be configured to generate experience (phenomenology). The postulates are used to define intrinsic information as ‘‘differences that make a difference'' within a system, and integrated information as information specified by a whole that cannot be reduced to that specified by its parts. By applying the postulates both at the level of individual mechanisms and at the level of systems of mechanisms, IIT arrives at an identity: an experience is a maximally irreducible conceptual structure (MICS, a constellation of concepts in qualia space), and the set of elements that generates it constitutes a complex. According to IIT, a MICS specifies the quality of an experience and integrated information WMax its quantity. From the theory follow several results, including: a system of mechanisms may condense into a major complex and non-overlapping minor complexes; the concepts that specify the quality of an experience are always about the complex itself and relate only indirectly to the external environment; anatomical connectivity influences complexes and associated MICS; a complex can generate a MICS even if its elements are inactive; simple systems can be minimally conscious; complicated systems can be unconscious; there can be true ‘‘zombies'' – unconscious feed-forward systems that are functionally equivalent to conscious complexes. Citation:},
author = {Oizumi, Masafumi and Albantakis, Larissa and Tononi, Giulio},
doi = {10.1371/journal.pcbi.1003588},
file = {::;::;::;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/PLoS Computational Biology/Oizumi, Albantakis, Tononi/Oizumi, Albantakis, Tononi - 2014 - From the Phenomenology to the Mechanisms of Consciousness Integrated Information Theory 3.0.pdf:pdf},
issn = {1553-7358},
journal = {PLoS Computational Biology},
keywords = {consciousness},
mendeley-tags = {consciousness},
number = {5},
pages = {e1003588},
title = {{From the Phenomenology to the Mechanisms of Consciousness: Integrated Information Theory 3.0}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1003588},
volume = {10},
year = {2014}
}
@inproceedings{Mannor2004,
abstract = {We consider a graph theoretic approach for automatic construction of options in a dynamic environment. A map of the environment is generated on-line by the learning agent, representing the topological structure of the state transitions. A clustering algorithm is then used to partition the state space to different regions. Policies for reaching the different parts of the space are separately learned and added to the model in a form of options (macro-actions). The options are used for accelerating the Q-Learning algorithm. We extend the basic algorithm and consider building a map that includes preliminary indication of the location of "interesting" regions of the state space, where the value gradient is significant and additional exploration might be beneficial. Experiments indicate significant speedups, especially in the initial learning phase.},
address = {New York, New York, USA},
author = {Mannor, Shie and Menache, Ishai and Hoze, Amit and Klein, Uri},
booktitle = {Twenty-first international conference on Machine learning - ICML '04},
doi = {10.1145/1015330.1015355},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Twenty-first international conference on Machine learning - ICML '04/Mannor et al/Mannor et al. - 2004 - Dynamic abstraction in reinforcement learning via clustering.pdf:pdf},
isbn = {1581138285},
keywords = {clustering,hierarchical reinforcement learning,options,q-learning,reinforcement learning},
pages = {71},
publisher = {ACM Press},
title = {{Dynamic abstraction in reinforcement learning via clustering}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015355},
year = {2004}
}
@article{Abel2018a,
abstract = {In lifelong reinforcement learning, agents must effectively transfer knowledge across tasks while simultaneously addressing exploration, credit assignment, and generalization. State abstraction can help overcome these hurdles by compressing the representation used by an agent, thereby reducing the computational and statistical burdens of learning. To this end, we here develop theory to compute and use state abstractions in lifelong reinforcement learning. We introduce two new classes of abstractions: (1) transitive state abstractions, whose optimal form can be computed efficiently, and (2) PAC state abstractions, which are guaranteed to hold with respect to a distribution of tasks. We show that the joint family of transitive PAC abstractions can be acquired efficiently, preserve near optimal-behavior, and experimentally reduce sample complexity in simple domains, thereby yielding a family of desirable abstractions for use in lifelong reinforcement learning. Along with these positive results, we show that there are pathological cases where state abstractions can negatively impact performance.},
author = {Abel, David and Arumugam, Dilip and Lehnert, Lucas and Littman, Michael},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 35th International Conference on Machine Learning/Abel et al/Abel et al. - 2018 - State Abstractions for Lifelong Reinforcement Learning.pdf:pdf},
journal = {Proceedings of the 35th International Conference on Machine Learning},
pages = {10--19},
title = {{State Abstractions for Lifelong Reinforcement Learning}},
url = {http://proceedings.mlr.press/v80/abel18a.html},
volume = {80},
year = {2018}
}
@article{Dietterich2000a,
abstract = {Reinforcement learning addresses the problem of learning optimal policies for sequential decision-making problems involving stochastic operators and numerical reward functions rather than the more traditional deterministic operators and logical goal predicates. In many ways, reinforcement learning research is recapitulating the development of classical research in planning and problem solving. After studying the problem of solving flat problem spaces, researchers have recently turned their attention to hierarchical methods that incorporate subroutines and state abstractions. This paper gives an overview of the MAXQ value function decomposition and its support for state abstraction and action abstraction.},
archivePrefix = {arXiv},
arxivId = {cs/9905015},
author = {Dietterich, Thomas G},
doi = {10.1007/3-540-44914-0_2},
eprint = {9905015},
file = {::},
isbn = {3540678395},
issn = {16113349},
journal = {Abstraction Reformulation and Approximation},
pages = {26--44},
pmid = {25246403},
primaryClass = {cs},
title = {{An Overview of MAXQ Hierarchical Reinforcement Learning}},
url = {http://www.springerlink.com/index/W11KBY3DH4VEEHDN.pdf},
volume = {1864/2000},
year = {2000}
}
@article{Mcgovern2001,
abstract = {An ability to adjust to changing environments and unforeseen circumstances is likely to be an important component of a successful autonomous space robot. This paper shows how to augment reinforcement learning algorithms with a method for automatically discovering certain types of subgoals online. By creating useful new subgoals while learning, the agent is able to accelerate learning on a current task and to transfer its expertise to related tasks through the reuse of its ability to attain subgoals. Subgoals are created based on commonalities across multiple paths to a solution. We cast the task of finding these commonalities as a multiple-instance learning problem and use the concept of diverse density to find solutions. We introduced this approach in [10] and here we present additional results for a simulated mobile robot task.},
author = {Mcgovern, Amy and Barto, Andrew},
file = {::},
journal = {6th International Symposium on Artificial Intelligence, Robotics, and Automation in Space},
keywords = {automatic discovering,reinforcement learning,subgoal discovering},
title = {{Accelerating reinforcement learning through the discovery of useful subgoals}},
url = {/home/obada/Desktop/MAS 1 - ISW/Robot Learning/Accelerating reinforcement learning through the discovery of useful subgoals.pdf},
year = {2001}
}
@article{Pospelov1996,
author = {Поспелов, Д. А.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Программные продукты и системы/Поспелов/Поспелов - 1996 - Прикладная семиотика и искусственный интеллект.pdf:pdf},
journal = {Программные продукты и системы},
language = {russian},
number = {3},
pages = {10--13},
title = {{Прикладная семиотика и искусственный интеллект}},
year = {1996}
}
@article{Uv,
author = {Uv, R T S},
file = {::},
title = {{¢¤{\pounds}¦¥¨{\S} ¸ ¹ {\textcopyright} »3¸ {\textcopyright} {\S}}}
}
@article{Glass1999,
author = {Glass, James R},
journal = {Proceedings of the 1999 IEEE ASRU Workshop},
title = {{Challenges for spoken dialogue systems}},
year = {1999}
}
@article{Siagian2007,
abstract = {We present a robot localization system using biologically-inspired$\backslash$nvision. Our system models two extensively studied human visual capabilities:$\backslash$n(1) extracting the {\^{a}}gist{\^{a}} of a scene to produce a coarse localization$\backslash$nhypothesis, and (2) refining it by locating salient landmark regions$\backslash$nin the scene. Gist is computed here as a holistic statistical signature$\backslash$nof the image, yielding abstract scene classification and layout.$\backslash$nSaliency is computed as a measure of interest at every image location,$\backslash$nefficiently directing the time-consuming landmark identification$\backslash$nprocess towards the most likely candidate locations in the image.$\backslash$nThe gist and salient landmark features are then further processed$\backslash$nusing a Monte-Carlo localization algorithm to allow the robot to$\backslash$ngenerate its position. We test the system in three different outdoor$\backslash$nenvironments - building complex (126x180ft. area, 3794 testing images),$\backslash$nvegetation-filled park (270x360ft. area, 7196 testing images), and$\backslash$nopen-field park (450x585ft. area, 8287 testing images) - each with$\backslash$nits own challenges. The system is able to localize, on average, within$\backslash$n6.0, 10.73, and 32.24 ft., respectively, even with multiple kidnapped-robot$\backslash$ninstances.},
author = {Siagian, Christian and Itti, Laurent},
doi = {10.1109/IROS.2007.4399349},
file = {::},
isbn = {1424409128},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {1723--1730},
title = {{Biologically-inspired robotics vision Monte-Carlo localization in the outdoor environment}},
year = {2007}
}
@article{Dietterich2018,
abstract = {Exogenous state variables and rewards can slow down reinforcement learning by injecting uncontrolled variation into the reward signal. We formalize exogenous state variables and rewards and identify conditions under which an MDP with exogenous state can be decomposed into an exogenous Markov Reward Process involving only the exogenous state+reward and an endogenous Markov Decision Process defined with respect to only the endogenous rewards. We also derive a variance-covariance condition under which Monte Carlo policy evaluation on the endogenous MDP is accelerated compared to using the full MDP. Similar speedups are likely to carry over to all RL algorithms. We develop two algorithms for discovering the exogenous variables and test them on several MDPs. Results show that the algorithms are practical and can significantly speed up reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1806.01584},
author = {Dietterich, Thomas G. and Trimponias, George and Chen, Zhitang},
eprint = {1806.01584},
file = {::},
issn = {1938-7228},
title = {{Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning}},
url = {http://arxiv.org/abs/1806.01584},
year = {2018}
}
@article{Fernandes2013,
author = {Fernandes, J F Ribas},
file = {::},
number = {November},
title = {{Hierarchical reinforcement learning in behavior and the brain}},
year = {2013}
}
@article{Alexandersson2004,
author = {Alexandersson, Jan and Becker, Tilman and Engel, Ralf and L{\"{o}}ckelt, Markus and Pecourt, Elsa and Poller, Peter and Pfleger, Norbert and Reithinger, Norbert},
journal = {HLT-NAACL 2004 Workshop: 2nd Workshop on Scalable Natural Language Understanding},
pages = {25--32},
title = {{Ends-based Dialogue Processing}},
year = {2004}
}
@article{Panov2018a,
abstract = {В работе рассматривается знаковый подход к проблеме моделирования процесса целеполагания и его интеграции с методами синтеза плана поведения. На основе психологически правдоподобной модели знаковой картины мира когнитивного агента предложен алгоритм GoalMAP, который на основе формальной сетевой модели реализует итеративный процесс иерархического планирования с выделенным этапом постановки или выбора новой цели. Рассмотрена оценка сложности алгоритма планирования поведения, проведены модельные эксперименты с программной реализацией построенных алгоритмов, демонстрирующей ключевые особенности используемого подхода.},
author = {Панов, А. И.},
doi = {10.14357/20718594180202},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Искусственный интеллект и принятие решений/Панов/Панов - 2018 - Целеполагание и синтез плана поведения когнитивным агентом.docx:docx;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Искусственный интеллект и принятие решений/Панов/Панов - 2018 - Целеполагание и синтез плана поведения когнитивным агентом.pdf:pdf},
journal = {Искусственный интеллект и принятие решений},
keywords = {16-37-60055,17-07-00281,GoalMAP,elibrary,frccsc,mypub,vak,знак,знаковая картина мира,когнитивный агент,культурно-исторический подход,планирование,планирование поведения,теория деятельности,целеполагание},
language = {russian},
mendeley-tags = {16-37-60055,17-07-00281,elibrary,frccsc,mypub,vak},
number = {2},
pages = {21--35},
title = {{Целеполагание и синтез плана поведения когнитивным агентом}},
url = {http://aidt.ru/index.php?option=com{\_}content{\&}view=article{\&}id=796:a-i-panov-tselepolaganie-i-sintez-plana-povedeniya-kognitivnym-agentom{\&}catid=334:kognitivnoe-modelirovanie{\&}Itemid=204{\&}lang=ru https://elibrary.ru/item.asp?id=35125454},
year = {2018}
}
@article{Tirinzoni2018,
abstract = {We consider the transfer of experience samples (i.e., tuples {\textless} s, a, s', r {\textgreater}) in reinforcement learning (RL), collected from a set of source tasks to improve the learning process in a given target task. Most of the related approaches focus on selecting the most relevant source samples for solving the target task, but then all the transferred samples are used without considering anymore the discrepancies between the task models. In this paper, we propose a model-based technique that automatically estimates the relevance (importance weight) of each source sample for solving the target task. In the proposed approach, all the samples are transferred and used by a batch RL algorithm to solve the target task, but their contribution to the learning process is proportional to their importance weight. By extending the results for importance weighting provided in supervised learning literature, we develop a finite-sample analysis of the proposed batch RL algorithm. Furthermore, we empirically compare the proposed algorithm to state-of-the-art approaches, showing that it achieves better learning performance and is very robust to negative transfer, even when some source tasks are significantly different from the target task.},
archivePrefix = {arXiv},
arxivId = {1805.10886},
author = {Tirinzoni, Andrea and Sessa, Andrea and Pirotta, Matteo and Restelli, Marcello},
eprint = {1805.10886},
file = {::},
title = {{Importance Weighted Transfer of Samples in Reinforcement Learning}},
url = {http://arxiv.org/abs/1805.10886},
year = {2018}
}
@article{Guo2016,
author = {Guo, Meng and Dimarogonas, Dimos V},
doi = {10.1109/TASE.2016.2628389},
file = {::},
issn = {15455955},
pages = {1--12},
title = {{Task and Motion Coordination for Heterogeneous Multiagent Systems With Loosely Coupled Local Tasks}},
year = {2016}
}
@misc{Presentation2016b,
author = {Панов, А. И.},
file = {::},
pages = {19},
title = {{Биологически и психологически правдоподобные методы моделирования в искусственном интеллекте}},
year = {2016}
}
@article{Konidaris2016,
abstract = {We describe a framework for building abstraction hierarchies whereby an agent alternates skill- and representation-acquisition phases to construct a sequence of increasingly abstract Markov decision processes. Our formulation builds on recent results showing that the appropriate abstract representation of a problem is specified by the agent's skills. We describe how such a hierarchy can be used for fast planning, and illustrate the construction of an appropriate hierarchy for the Taxi domain.},
archivePrefix = {arXiv},
arxivId = {1509.07582},
author = {Konidaris, George},
doi = {10.1016/j.surg.2015.03.052.Rate},
eprint = {1509.07582},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IJCAI International Joint Conference on Artificial Intelligence/Konidaris/Konidaris - 2016 - Constructing abstraction hierarchies using a skill-symbol loop.pdf:pdf},
isbn = {6176366127},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning},
pages = {1648--1654},
pmid = {28579718},
title = {{Constructing abstraction hierarchies using a skill-symbol loop}},
volume = {2016-Janua},
year = {2016}
}
