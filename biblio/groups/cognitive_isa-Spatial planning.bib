Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Milford2014,
abstract = {Mobile robots and animals alike must effectively navigate their environments in order to achieve their goals. For animals goal-directed navigation facilitates finding food, seeking shelter or migration; similarly robots perform goal-directed navigation to find a charging station, get out of the rain or guide a person to a destination. This similarity in tasks extends to the environment as well; increasingly, mobile robots are operating in the same underwater, ground and aerial environments that animals do. Yet despite these similarities, goal-directed navigation research in robotics and biology has proceeded largely in parallel, linked only by a small amount of interdisciplinary research spanning both areas. Most state-of-the-art robotic navigation systems employ a range of sensors, world representations and navigation algorithms that seem far removed from what we know of how animals navigate; their navigation systems are shaped by key principles of navigation in 'real-world' environments including dealing with uncertainty in sensing, landmark observation and world modelling. By contrast, biomimetic animal navigation models produce plausible animal navigation behaviour in a range of laboratory experimental navigation paradigms, typically without addressing many of these robotic navigation principles. In this paper, we attempt to link robotics and biology by reviewing the current state of the art in conventional and biomimetic goal-directed navigation models, focusing on the key principles of goal-oriented robotic navigation and the extent to which these principles have been adapted by biomimetic navigation models and why.},
author = {Milford, M. and Schulz, R.},
doi = {10.1098/rstb.2013.0484},
file = {::},
isbn = {1471-2970},
issn = {0962-8436},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
number = {1655},
pages = {20130484--20130484},
pmid = {25267826},
title = {{Principles of goal-directed spatial robot navigation in biomimetic models}},
url = {http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2013.0484},
volume = {369},
year = {2014}
}
@article{Madl2018,
abstract = {Computational cognitive models of spatial memory often neglect difficulties posed by the real world, such as sensory noise, uncertainty, and high spatial complexity. On the other hand, robotics is unconcerned with understanding biological cognition. Here, we describe a computational framework for robotic architectures aiming to function in realistic environments, as well as to be cognitively plausible. We motivate and describe several mechanisms towards achieving this despite the sensory noise and spatial complexity inherent in the physical world. We tackle error accumulation during path integration by means of Bayesian localization, and loop closing with sequential gradient descent. Finally, we outline a method for structuring spatial representations using metric learning and clustering. Crucially, unlike the algorithms of traditional robotics, we show that these mechanisms can be implemented in neuronal or cognitive models. We briefly outline a concrete implementation of the proposed framework as part of the LIDA cognitive architecture, and argue that this kind of probabilistic framework is well-suited for use in cognitive robotic architectures aiming to combine spatial functionality and psychological plausibility.},
author = {Madl, Tamas and Franklin, Stan and Chen, Ke and Trappl, Robert},
doi = {10.1016/j.cogsys.2017.08.002},
file = {::},
issn = {13890417},
journal = {Cognitive Systems Research},
keywords = {Bayesian brain,Cognitive architecture,Computational cognitive modeling,LIDA,Spatial memory},
pages = {147--172},
publisher = {Elsevier B.V.},
title = {{A computational cognitive framework of spatial memory in brains and robots}},
url = {https://doi.org/10.1016/j.cogsys.2017.08.002},
volume = {47},
year = {2018}
}
@article{Erdem2014,
abstract = {We propose an extended version of our previous goal directed navigation model based on forward planning of trajectories in a network of head direction cells, persistent spiking cells, grid cells, and place cells. In our original work the animat incrementally creates a place cell map by random exploration of a novel environment. After the exploration phase, the animat decides on its next movement direction towards a goal by probing linear look-ahead trajectories in several candidate directions while stationary and picking the one activating place cells representing the goal location. In this work we present several improvements over our previous model. We improve the range of linear look-ahead probes significantly by imposing a hierarchical structure on the place cell map consistent with the experimental findings of differences in the firing field size and spacing of grid cells recorded at different positions along the dorsal to ventral axis of entorhinal cortex. The new model represents the environment at different scales by populations of simulated hippocampal place cells with different firing field sizes. Among other advantages this model allows simultaneous constant duration linear look-ahead probes at different scales while significantly extending each probe range. The extension of the linear look-ahead probe range while keeping its duration constant also limits the degrading effects of noise accumulation in the network. We show the extended model's performance using an animat in a large open field environment. {\textcopyright} 2013 Elsevier Ltd.},
author = {Erdem, UÇ§ur M. and Hasselmo, Michael E.},
doi = {10.1016/j.jphysparis.2013.07.002},
file = {::},
isbn = {1769-7115 (Electronic)$\backslash$r0928-4257 (Linking)},
issn = {09284257},
journal = {Journal of Physiology Paris},
keywords = {Entorhinal cortex,Grid cell,Hippocampus,Navigation,Place cell},
number = {1},
pages = {28--37},
pmid = {23891644},
title = {{A biologically inspired hierarchical goal directed navigation model}},
volume = {108},
year = {2014}
}
@article{Epstein2013,
abstract = {This paper describes how a cognitive architecture builds a spatial model and navigates from it without a map. Each con- structed model is a collage of spatial affordances that de- scribes how the environment has been sensed and traversed. The system exploits the evolving model while it directs an agent to explore the environment. Effective models are learned quickly during travel. Moreover, when combined with simple heuristics, the learned spatial model supports effective navigation. In three simple environments, these learned mod- els describe space in ways familiar to people, and often pro- duce near-optimal travel times.},
author = {Epstein, Susan L. and Aroor, Anoop and Sklar, Elizabeth I. and Parsons, Simon},
file = {::},
keywords = {affordances,cognitive architecture,exploration,learning,spatial,spatial cognition},
pages = {1--6},
title = {{Navigation with Learned Spatial Affordances}},
url = {http://www.compsci.hunter.cuny.edu/{~}epstein/papers/CogSciFinal.Epstein.pdf},
year = {2013}
}
@article{Li2013a,
abstract = {Simultaneous localization and consistent mapping in dynamic environments is a fundamental and unsolved problem in the mobile robotics community. Most of the algorithms for this problem heavily rely on discriminating dynamic objects from static objects. Because these recursive filters based discrimination algorithms always have lag before the model selection parameters converge to the steady states, they have a period of time that the filter could identify a dynamic target as static or vice versa. Mis-classifications decrease precision and consistence, and induce filter divergence.A brain interacts with dynamic environments. The biological basis of this adaptability is provided by the connectivity and the dynamic properties of neurons. Biologically inspired by the adaptability, the paper proposes a shunting STM (Short Term Memory) based method to solve the simultaneous localization and consistent mapping problem, especially in dynamic environments. The proposed method utilizes a shunting STM neural network to represent environments and to probabilistically reflect the probability of existence of an object; it adapts a scan matching scheme to localize robot based on the map representation. Dynamic properties of the neural network are used to reflect environmental changes, therefore, the proposed method does not require explicit discrimination of objects. As a result, the proposed method does not have the lag of convergence, and it has high utilization ratio of observation information. Theoretical analyses in the paper show the proposed method has Lyapunov stability and its computational complexity does not depend on the size of the environment. The paper compares the proposed method with the classification based Extend Kalman Filter on a classical outdoor dataset, in simulated environments and in real indoor environments. The results show the proposed method outperforms the classification based EKF on precision and consistence in both static environments and dynamic environments. {\textcopyright} 2012 Elsevier B.V..},
author = {Li, Yangming and Li, Shuai and Ge, Yunjian},
doi = {10.1016/j.neucom.2012.10.011},
file = {::},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Biological inspiration,Dynamic environment,Scan matching,Shunting short term memory model,Simultaneous localization and consistent mapping},
pages = {170--179},
publisher = {Elsevier},
title = {{A biologically inspired solution to simultaneous localization and consistent mapping in dynamic environments}},
url = {http://dx.doi.org/10.1016/j.neucom.2012.10.011},
volume = {104},
year = {2013}
}
@article{Huang2017a,
abstract = {Our ultimate goal is to efficiently enable end-users to correctly anticipate a robot's behavior in novel situations. This behavior is often a direct result of the robot's underlying objective function. Our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, and then selecting those behaviors that are maximally informative. The problem of computing a posterior over objectives from observed behavior is known as Inverse Reinforcement Learning (IRL), and has been applied to robots learning human objectives. We consider the problem where the roles of human and robot are swapped. Our main contribution is to recognize that unlike robots, humans will not be $\backslash$emph{\{}exact{\}} in their IRL inference. We thus introduce two factors to define candidate approximate-inference models for human learning in this setting, and analyze them in a user study in the autonomous driving domain. We show that certain approximate-inference models lead to the robot generating example behaviors that better enable users to anticipate what the robot will do in test situations. Our results also suggest, however, that additional research is needed in modeling how humans extrapolate from examples of robot behavior.},
archivePrefix = {arXiv},
arxivId = {1702.03465},
author = {Huang, Sandy H. and Held, David and Abbeel, Pieter and Dragan, Anca D.},
doi = {10.15607/RSS.2017.XIII.059},
eprint = {1702.03465},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Huang et al/Huang et al. - 2017 - Enabling Robots to Communicate their Objectives.pdf:pdf},
isbn = {1234567245},
issn = {15737527},
title = {{Enabling Robots to Communicate their Objectives}},
url = {http://arxiv.org/abs/1702.03465},
year = {2017}
}
@article{Mueller2013,
abstract = {The hippocampus has long been thought to be critical in learning and representing the cognitive map, and thus support functions such as search, pathfinding and route planning. This work aims to demonstrate the utility of hippocampus-based neural networks in modeling human search task behavior. Human solutions to pathfinding problems are generally fast but approximate, in contrast to traditional AI approaches. In this paper, we report data on a human search task, and then examine a set of models, based upon the structure of the hippocampus, which use a goal scent mechanism similar to the optimal pathfinding algorithms used in artificial intelligence systems. We compare five distinct search models, and conclude that a goal scent model driven by multiple goals spread throughout the search space provides the best and most accurate account of the human data. This research suggests a convergence in traditional AI and biologically- inspired approaches to pathfinding that may be mutually beneficial. {\textcopyright} 2013 Elsevier B.V.},
author = {Mueller, Shane T. and Perelman, Brandon S. and Simpkins, Benjamin G.},
doi = {10.1016/j.bica.2013.05.002},
file = {::},
issn = {2212683X},
journal = {Biologically Inspired Cognitive Architectures},
keywords = {Hippocampus,Pathfinding},
pages = {94--111},
publisher = {Elsevier B.V.},
title = {{Pathfinding in the cognitive map: Network models of mechanisms for search and planning}},
url = {http://dx.doi.org/10.1016/j.bica.2013.05.002},
volume = {5},
year = {2013}
}
@article{Epstein2015,
abstract = {Background: Optimal navigation for a simulated robot relies on a detailed map and explicit path planning. This is problematic for realworld robots, whose sensors and actuators are subject to noise and error, and whose environment may be dynamic. This paper reports on robots that rely on local spatial perception, learning, and commonsense rationales instead. Aims: Our thesis is that spatial abstractions learned from local sensing can support effective, autonomous robot navigation. Method: The simulated robot experiences real-world actuator error while it navigates autonomously. The robot's decision-making cognitive architecture relies on reactive and heuristic procedures based on simple rationales and spatial abstractions of where it has been. As the robot travels, it learns (and shares) affordances that facilitate movement, including perceived unobstructed areas and trail markers. Together they represent the environment but do not constitute a map. Robots navigate to five sets of targets in each of three environments. Results: This approach quickly produces efficient travel without planning or a map. Metrics include travel time, decision time, and distance. Experiments examine the impact of each kind of affordance. Comparison with a traditional Aâ planner shows that performance becomes only slightly suboptimal for one robot. Preliminary data with multiple robots suggest that, because our approach does need to replan when actuators err or robots threaten to collide, it will outperform the traditional approach. Conclusions: People have been shown not to navigate from detailed mental maps. Robots can also learn to navigate well without them, when they learn from local percepts.},
author = {Epstein, Susan L. and Aroor, Anoop and Evanusa, Matthew and Sklar, Elizabeth I. and Parsons, Simon},
doi = {10.1007/s10339-015-0713-x},
file = {::},
issn = {16124790},
journal = {Cognitive Processing},
keywords = {Qualitative reasoning,Robot navigation,Spatial abstractions,Spatial model},
pages = {215--219},
publisher = {Springer Berlin Heidelberg},
title = {{Spatial abstraction for autonomous robot navigation}},
volume = {16},
year = {2015}
}
@article{Daniel2010,
abstract = {jetyak; path planning; theta*},
archivePrefix = {arXiv},
arxivId = {arXiv:1401.3843},
author = {Daniel, Kenny and Nash, Alex and Koenig, Sven and Felner, Ariel},
doi = {10.1613/jair.2994},
eprint = {arXiv:1401.3843},
file = {::},
isbn = {1577353234},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {533--579},
title = {{Theta*: Any-angle path planning on grids}},
volume = {39},
year = {2010}
}
@article{Milford2010,
abstract = {The challenge of persistent navigation and mapping is to develop an autonomous robot system that can simultaneously localize, map and navigate over the lifetime of the robot with little or no human intervention. Most solutions to the simultaneous localization and mapping (SLAM) problem aim to produce highly accurate maps of areas that are assumed to be static. In contrast, solutions for persistent navigation and mapping must produce reliable goal-directed navigation outcomes in an environment that is assumed to be in constant flux. We investigate the persistent navigation and mapping problem in the context of an autonomous robot that performs mock deliveries in a working office environment over a two-week period. The solution was based on the biologically inspired visual SLAM system, RatSLAM. RatSLAM performed SLAM continuously while interacting with global and local navigation systems, and a task selection module that selected between exploration, delivery, and recharging modes. The robot performed 1,143 delivery tasks to 11 different locations with only one delivery failure (from which it recovered), traveled a total distance of more than 40 km over 37 hours of active operation, and recharged autonomously a total of 23 times.},
author = {Milford, Michael and Wyeth, Gordon},
doi = {10.1177/0278364909340592},
file = {::},
isbn = {0278-3649},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {RatSLAM,SLAM,biologically inspired,persistent navigation and mapping},
number = {9},
pages = {1131--1153},
title = {{Persistent navigation and mapping using a biologically inspired slam system}},
volume = {29},
year = {2010}
}
@article{Steckel2013,
abstract = {We propose to combine a biomimetic navigation model which solves a simultaneous localization and mapping task with a biomimetic sonar mounted on a mobile robot to address two related questions. First, can robotic sonar sensing lead to intelligent interactions with complex environments? Second, can we model sonar based spatial orientation and the construction of spatial maps by bats? To address these questions we adapt the mapping module of RatSLAM, a previously published navigation system based on computational models of the rodent hippocampus. We analyze the performance of the proposed robotic implementation operating in the real world. We conclude that the biomimetic navigation model operating on the information from the biomimetic sonar allows an autonomous agent to map unmodified (office) environments efficiently and consistently. Furthermore, these results also show that successful navigation does not require the readings of the biomimetic sonar to be interpreted in terms of individual objects/landmarks in the environment. We argue that the system has applications in robotics as well as in the field of biology as a simple, first order, model for sonar based spatial orientation and map building},
author = {Steckel, Jan and Peremans, Herbert},
doi = {10.1371/journal.pone.0054076},
file = {::},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {1},
pmid = {23365647},
title = {{BatSLAM: Simultaneous Localization and Mapping Using Biomimetic Sonar}},
volume = {8},
year = {2013}
}
@article{Mueller2013a,
abstract = {The hippocampus has long been thought to be critical in learning and representing the cognitive map, and thus support functions such as search, pathfinding and route planning. This work aims to demonstrate the utility of hippocampus-based neural networks in modeling human search task behavior. Human solutions to pathfinding problems are generally fast but approximate, in contrast to traditional AI approaches. In this paper, we report data on a human search task, and then examine a set of models, based upon the structure of the hippocampus, which use a goal scent mechanism similar to the optimal pathfinding algorithms used in artificial intelligence systems. We compare five distinct search models, and conclude that a goal scent model driven by multiple goals spread throughout the search space provides the best and most accurate account of the human data. This research suggests a convergence in traditional AI and biologically- inspired approaches to pathfinding that may be mutually beneficial. {\textcopyright} 2013 Elsevier B.V.},
author = {Mueller, Shane T. and Perelman, Brandon S. and Simpkins, Benjamin G.},
doi = {10.1016/j.bica.2013.05.002},
file = {::},
issn = {2212683X},
journal = {Biologically Inspired Cognitive Architectures},
keywords = {Hippocampus,Pathfinding},
pages = {94--111},
publisher = {Elsevier B.V.},
title = {{Pathfinding in the cognitive map: Network models of mechanisms for search and planning}},
url = {http://dx.doi.org/10.1016/j.bica.2013.05.002},
volume = {5},
year = {2013}
}
@article{Siagian2007,
abstract = {We present a robot localization system using biologically-inspired$\backslash$nvision. Our system models two extensively studied human visual capabilities:$\backslash$n(1) extracting the {\^{a}}ÂÂgist{\^{a}}ÂÂ of a scene to produce a coarse localization$\backslash$nhypothesis, and (2) refining it by locating salient landmark regions$\backslash$nin the scene. Gist is computed here as a holistic statistical signature$\backslash$nof the image, yielding abstract scene classification and layout.$\backslash$nSaliency is computed as a measure of interest at every image location,$\backslash$nefficiently directing the time-consuming landmark identification$\backslash$nprocess towards the most likely candidate locations in the image.$\backslash$nThe gist and salient landmark features are then further processed$\backslash$nusing a Monte-Carlo localization algorithm to allow the robot to$\backslash$ngenerate its position. We test the system in three different outdoor$\backslash$nenvironments - building complex (126x180ft. area, 3794 testing images),$\backslash$nvegetation-filled park (270x360ft. area, 7196 testing images), and$\backslash$nopen-field park (450x585ft. area, 8287 testing images) - each with$\backslash$nits own challenges. The system is able to localize, on average, within$\backslash$n6.0, 10.73, and 32.24 ft., respectively, even with multiple kidnapped-robot$\backslash$ninstances.},
author = {Siagian, Christian and Itti, Laurent},
doi = {10.1109/IROS.2007.4399349},
file = {::},
isbn = {1424409128},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {1723--1730},
title = {{Biologically-inspired robotics vision Monte-Carlo localization in the outdoor environment}},
year = {2007}
}
