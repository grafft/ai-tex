Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Haarnoja2018,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy - that is, succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
archivePrefix = {arXiv},
arxivId = {1801.01290},
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
eprint = {1801.01290},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Haarnoja et al/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf:pdf},
title = {{Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}},
url = {http://arxiv.org/abs/1801.01290},
year = {2018}
}
@article{Srinivas2018,
abstract = {A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differentiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image-based goals. We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities.},
annote = {Код - https://github.com/aravind0706/upn},
archivePrefix = {arXiv},
arxivId = {1804.00645},
author = {Srinivas, Aravind and Jabri, Allan and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea},
eprint = {1804.00645},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Srinivas et al/Srinivas et al. - 2018 - Universal Planning Networks.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Srinivas et al/Srinivas et al. - 2018 - Universal Planning Networks.jpg:jpg},
title = {{Universal Planning Networks}},
url = {http://arxiv.org/abs/1804.00645},
year = {2018}
}
@article{Kaplanis2018,
abstract = {Unlike humans, who are capable of continual learning over their lifetimes, artificial neural networks have long been known to suffer from a phenomenon known as catastrophic forgetting, whereby new learning can lead to abrupt erasure of previously acquired knowledge. Whereas in a neural network the parameters are typically modelled as scalar values, an individual synapse in the brain comprises a complex network of interacting biochemical components that evolve at different timescales. In this paper, we show that by equipping tabular and deep reinforcement learning agents with a synaptic model that incorporates this biological complexity (Benna {\&} Fusi, 2016), catastrophic forgetting can be mitigated at multiple timescales. In particular, we find that as well as enabling continual learning across sequential training of two simple tasks, it can also be used to overcome within-task forgetting by reducing the need for an experience replay database.},
archivePrefix = {arXiv},
arxivId = {1802.07239},
author = {Kaplanis, Christos and Shanahan, Murray and Clopath, Claudia},
eprint = {1802.07239},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Kaplanis, Shanahan, Clopath/Kaplanis, Shanahan, Clopath - 2018 - Continual Reinforcement Learning with Complex Synapses.pdf:pdf},
issn = {1938-7228},
title = {{Continual Reinforcement Learning with Complex Synapses}},
url = {http://arxiv.org/abs/1802.07239},
year = {2018}
}
@article{Schaul2015,
abstract = {Value functions are a core component of rein- forcement learning systems. The main idea is to to construct a single function approximator V (s; $\theta$) that estimates the long-term reward from any state s, using parameters $\theta$. In this paper we introduce universal value function approx- imators (UVFAs) V (s, g; $\theta$) that generalise not just over states s but also over goals g. We de- velop an efficient technique for supervised learn- ing of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a re- inforcement learning algorithm that updates the UVFAsolely from observed rewards. Finally, we demonstrate that a UVFAcan successfully gener- alise to previously unseen goals. 1.},
author = {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of The 32nd International Conference on Machine Learning/Schaul et al/Schaul et al. - 2015 - Universal Value Function Approximators.pdf:pdf},
isbn = {9781510810587},
issn = {1938-7228},
journal = {Proceedings of The 32nd International Conference on Machine Learning},
pages = {1312--1320},
title = {{Universal Value Function Approximators}},
url = {http://jmlr.org/proceedings/papers/v37/schaul15.html},
year = {2015}
}
@article{Nachum2017,
abstract = {We establish a new connection between value and policy based reinforcement learning (RL) based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization. Specifically, we show that softmax consistent action values correspond to optimal entropy regularized policy probabilities along any action sequence, regardless of provenance. From this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes a notion of soft consistency error along multi-step action sequences extracted from both on- and off-policy traces. We examine the behavior of PCL in different scenarios and show that PCL can be interpreted as generalizing both actor-critic and Q-learning algorithms. We subsequently deepen the relationship by showing how a single model can be used to represent both a policy and the corresponding softmax state values, eliminating the need for a separate critic. The experimental evaluation demonstrates that PCL significantly outperforms strong actor-critic and Q-learning baselines across several benchmarks.},
archivePrefix = {arXiv},
arxivId = {1702.08892},
author = {Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
eprint = {1702.08892},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Nachum et al/Nachum et al. - 2017 - Bridging the Gap Between Value and Policy Based Reinforcement Learning.pdf:pdf},
number = {Nips},
pages = {1--21},
title = {{Bridging the Gap Between Value and Policy Based Reinforcement Learning}},
url = {http://arxiv.org/abs/1702.08892},
year = {2017}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Schulman et al/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:pdf},
pages = {1--12},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@inproceedings{Turner2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1706.00387v1},
author = {Turner, Richard E and Lillicrap, Timothy and Sch{\"{o}}lkopf, Bernhard},
booktitle = {ArXiv},
eprint = {arXiv:1706.00387v1},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/ArXiv/Turner, Lillicrap, Sch{\"{o}}lkopf/Turner, Lillicrap, Sch{\"{o}}lkopf - 2016 - Interpolated Policy Gradient Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinfo.pdf:pdf},
title = {{Interpolated Policy Gradient : Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning}},
year = {2016}
}
@article{Lee2018,
abstract = {Value Iteration Networks (VINs) are effective differentiable path planning modules that can be used by agents to perform navigation while still maintaining end-to-end differentiability of the entire architecture. Despite their effectiveness, they suffer from several disadvantages including training instability, random seed sensitivity, and other optimization problems. In this work, we reframe VINs as recurrent-convolutional networks which demonstrates that VINs couple recurrent convolutions with an unconventional max-pooling activation. From this perspective, we argue that standard gated recurrent update equations could potentially alleviate the optimization issues plaguing VIN. The resulting architecture, which we call the Gated Path Planning Network, is shown to empirically outperform VIN on a variety of metrics such as learning speed, hyperparameter sensitivity, iteration count, and even generalization. Furthermore, we show that this performance gap is consistent across different maze transition types, maze sizes and even show success on a challenging 3D environment, where the planner is only provided with first-person RGB images.},
archivePrefix = {arXiv},
arxivId = {1806.06408},
author = {Lee, Lisa and Parisotto, Emilio and Chaplot, Devendra Singh and Xing, Eric and Salakhutdinov, Ruslan},
eprint = {1806.06408},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Lee et al/Lee et al. - 2018 - Gated Path Planning Networks.pdf:pdf},
issn = {1938-7228},
title = {{Gated Path Planning Networks}},
url = {http://arxiv.org/abs/1806.06408},
year = {2018}
}
@article{Efroni2018,
abstract = {The famous Policy Iteration algorithm alternates between policy improvement and policy evaluation. Implementations of this algorithm with several variants of the latter evaluation stage, e.g, {\$}n{\$}-step and trace-based returns, have been analyzed in previous works. However, the case of multiple-step lookahead policy improvement, despite the recent increase in empirical evidence of its strength, has to our knowledge not been carefully analyzed yet. In this work, we introduce the first such analysis. Namely, we formulate variants of multiple-step policy improvement, derive new algorithms using these definitions and prove their convergence. Moreover, we show that recent prominent Reinforcement Learning algorithms are, in fact, instances of our framework. We thus shed light on their empirical success and give a recipe for deriving new algorithms for future study.},
archivePrefix = {arXiv},
arxivId = {1802.03654},
author = {Efroni, Yonathan and Dalal, Gal and Scherrer, Bruno and Mannor, Shie},
eprint = {1802.03654},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Efroni et al/Efroni et al. - 2018 - Beyond the One Step Greedy Approach in Reinforcement Learning.pdf:pdf},
title = {{Beyond the One Step Greedy Approach in Reinforcement Learning}},
url = {http://arxiv.org/abs/1802.03654},
year = {2018}
}
@article{Mania2018,
abstract = {A common belief in model-free reinforcement learning is that methods based on random search in the parameter space of policies exhibit significantly worse sample complexity than those that explore the space of actions. We dispel such beliefs by introducing a random search method for training static, linear policies for continuous control problems, matching state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a nearly optimal controller for a challenging instance of the Linear Quadratic Regulator, a classical problem in control theory, when the dynamics are not known. Computationally, our random search algorithm is at least 15 times more efficient than the fastest competing model-free methods on these benchmarks. We take advantage of this computational efficiency to evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. Our simulations highlight a high variability in performance in these benchmark tasks, suggesting that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms.},
archivePrefix = {arXiv},
arxivId = {1803.07055},
author = {Mania, Horia and Guy, Aurelia and Recht, Benjamin},
eprint = {1803.07055},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Mania, Guy, Recht/Mania, Guy, Recht - 2018 - Simple random search provides a competitive approach to reinforcement learning.pdf:pdf},
pages = {1--22},
title = {{Simple random search provides a competitive approach to reinforcement learning}},
url = {http://arxiv.org/abs/1803.07055},
year = {2018}
}
@article{Dabney2018,
archivePrefix = {arXiv},
arxivId = {1806.06923},
author = {Dabney, Will},
eprint = {1806.06923},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Icml/Dabney/Dabney - 2018 - Implicit Quantile Networks for Distributional Reinforcement Learning.pdf:pdf},
issn = {1938-7228},
journal = {Icml},
title = {{Implicit Quantile Networks for Distributional Reinforcement Learning}},
year = {2018}
}
@article{Silver2014,
abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic pol- icy gradient has a particularly appealing form: it is the expected gradient of the action-value func- tion. This simple form means that the deter- ministic policy gradient can be estimated much more efficiently than the usual stochastic pol- icy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counter- parts in high-dimensional action spaces.},
author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 31st International Conference on Machine Learning (ICML-14)/Silver et al/Silver et al. - 2014 - Deterministic Policy Gradient Algorithms.pdf:pdf},
isbn = {9781634393973},
issn = {1938-7228},
journal = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
pages = {387--395},
title = {{Deterministic Policy Gradient Algorithms}},
year = {2014}
}
@article{Waltz1965,
author = {Waltz, M and Fu, K},
doi = {10.1109/TAC.1965.1098193},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Automatic Control, IEEE Transactions on/Waltz, Fu/Waltz, Fu - 1965 - A heuristic approach to reinforcement learning control systems.pdf:pdf},
issn = {0018-9286},
journal = {Automatic Control, IEEE Transactions on},
number = {4},
pages = {390--398},
title = {{A heuristic approach to reinforcement learning control systems}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1098193},
volume = {AC-10},
year = {1965}
}
@article{Dosovitskiy2016,
abstract = {We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.},
archivePrefix = {arXiv},
arxivId = {1611.01779},
author = {Dosovitskiy, Alexey and Koltun, Vladlen},
eprint = {1611.01779},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Dosovitskiy, Koltun/Dosovitskiy, Koltun - 2016 - Learning to Act by Predicting the Future.pdf:pdf},
pages = {1--14},
title = {{Learning to Act by Predicting the Future}},
url = {http://arxiv.org/abs/1611.01779},
year = {2016}
}
@article{Author2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1807.06763v1},
author = {Author, Anonymous and Address, Affiliation},
eprint = {arXiv:1807.06763v1},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Author, Address/Author, Address - 2018 - General Value Function Networks.pdf:pdf},
number = {Nips},
title = {{General Value Function Networks}},
year = {2018}
}
@article{Wang2016,
abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
archivePrefix = {arXiv},
arxivId = {1611.05763},
author = {Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
eprint = {1611.05763},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Wang et al/Wang et al. - 2016 - Learning to reinforcement learn.pdf:pdf},
month = {nov},
pages = {1--17},
title = {{Learning to reinforcement learn}},
url = {http://arxiv.org/abs/1611.05763},
year = {2016}
}
@article{Zheng2018,
abstract = {In many sequential decision making tasks, it is challenging to design reward functions that help an RL agent efficiently learn behavior that is considered good by the agent designer. A number of different formulations of the reward-design problem, or close variants thereof, have been proposed in the literature. In this paper we build on the Optimal Rewards Framework of Singh et.al. that defines the optimal intrinsic reward function as one that when used by an RL agent achieves behavior that optimizes the task-specifying or extrinsic reward function. Previous work in this framework has shown how good intrinsic reward functions can be learned for lookahead search based planning agents. Whether it is possible to learn intrinsic reward functions for learning agents remains an open problem. In this paper we derive a novel algorithm for learning intrinsic rewards for policy-gradient based learning agents. We compare the performance of an augmented agent that uses our algorithm to provide additive intrinsic rewards to an A2C-based policy learner (for Atari games) and a PPO-based policy learner (for Mujoco domains) with a baseline agent that uses the same policy learners but with only extrinsic rewards. Our results show improved performance on most but not all of the domains.},
archivePrefix = {arXiv},
arxivId = {1804.06459},
author = {Zheng, Zeyu and Oh, Junhyuk and Singh, Satinder},
eprint = {1804.06459},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Zheng, Oh, Singh/Zheng, Oh, Singh - 2018 - On Learning Intrinsic Rewards for Policy Gradient Methods.pdf:pdf},
title = {{On Learning Intrinsic Rewards for Policy Gradient Methods}},
url = {http://arxiv.org/abs/1804.06459},
year = {2018}
}
@article{Andrychowicz2018,
abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
archivePrefix = {arXiv},
arxivId = {1707.01495},
author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
eprint = {1707.01495},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Andrychowicz et al/Andrychowicz et al. - 2018 - Hindsight Experience Replay.pdf:pdf},
number = {Nips},
title = {{Hindsight Experience Replay}},
url = {http://arxiv.org/abs/1707.01495},
year = {2018}
}
@article{Schulman2015,
abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1502.05477},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
doi = {10.1063/1.4927398},
eprint = {1502.05477},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Schulman et al/Schulman et al. - 2015 - Trust Region Policy Optimization.pdf:pdf},
isbn = {0375-9687},
issn = {2158-3226},
title = {{Trust Region Policy Optimization}},
url = {http://arxiv.org/abs/1502.05477},
year = {2015}
}
@article{Nachum2018,
abstract = {State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value. We show that such smoothed Q-values still satisfy a Bellman equation, making them learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships, we develop new algorithms for training a Gaussian policy directly from a learned smoothed Q-value approximator. The approach is additionally amenable to proximal optimization by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training leads to significantly improved results on standard continuous control benchmarks.},
archivePrefix = {arXiv},
arxivId = {1803.02348},
author = {Nachum, Ofir and Norouzi, Mohammad and Tucker, George and Schuurmans, Dale},
eprint = {1803.02348},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Nachum et al/Nachum et al. - 2018 - Smoothed Action Value Functions for Learning Gaussian Policies.pdf:pdf},
title = {{Smoothed Action Value Functions for Learning Gaussian Policies}},
url = {http://arxiv.org/abs/1803.02348},
year = {2018}
}
@article{BouAmmar,
abstract = {Online multi-task learning is an important capa-bility for lifelong learning agents, enabling them to acquire models for diverse tasks over time and rapidly learn new tasks by building upon prior ex-perience. However, recent progress toward lifelong reinforcement learning (RL) has been limited to learning from within a single task domain. For truly versatile lifelong learning, the agent must be able to autonomously transfer knowledge between differ-ent task domains. A few methods for cross-domain transfer have been developed, but these methods are computationally inefficient for scenarios where the agent must learn tasks consecutively. In this paper, we develop the first cross-domain life-long RL framework. Our approach efficiently op-timizes a shared repository of transferable knowl-edge and learns projection matrices that specialize that knowledge to different task domains. We pro-vide rigorous theoretical guarantees on the stability of this approach, and empirically evaluate its per-formance on diverse dynamical systems. Our re-sults show that the proposed method can learn ef-fectively from interleaved task domains and rapidly acquire high performance in new domains.},
author = {{Bou Ammar}, Haitham and Eaton, Eric and Luna, Jos{\'{e}} Marcio and Ruvolo, Paul},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Bou Ammar et al/Bou Ammar et al. - Unknown - Autonomous Cross-Domain Knowledge Transfer in Lifelong Policy Gradient Reinforcement Learning.pdf:pdf},
title = {{Autonomous Cross-Domain Knowledge Transfer in Lifelong Policy Gradient Reinforcement Learning}},
url = {https://scholar.princeton.edu/sites/default/files/bouammar/files/bouammar2015autonomous.pdf}
}
