Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Yang2016,
abstract = {We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the word and sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
archivePrefix = {arXiv},
arxivId = {1606.02393},
author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
doi = {10.18653/v1/N16-1174},
eprint = {1606.02393},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologie./Yang et al/Yang et al. - 2016 - Hierarchical Attention Networks fo.pdf:pdf},
isbn = {9781941643914},
journal = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
pages = {1480--1489},
title = {{Hierarchical Attention Networks for Document Classification}},
url = {http://aclweb.org/anthology/N16-1174},
year = {2016}
}
@article{Xu2015a,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
archivePrefix = {arXiv},
arxivId = {1502.03044},
author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
eprint = {1502.03044},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/ArXIV/Xu et al/Xu et al. - 2015 - Show, Attend and Tell Neural Image Caption Generation with Visual Attention.pdf:pdf},
journal = {ArXIV},
month = {feb},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
url = {http://arxiv.org/abs/1502.03044},
year = {2015}
}
