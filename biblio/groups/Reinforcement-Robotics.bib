Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Ghadirzadeh2017,
abstract = {Skilled robot task learning is best implemented by predictive action policies due to the inherent latency of sensorimotor processes. However, training such predictive policies is challenging as it involves finding a trajectory of motor activations for the full duration of the action. We propose a data-efficient deep predictive policy training (DPPT) framework with a deep neural network policy architecture which maps an image observation to a sequence of motor activations. The architecture consists of three sub-networks referred to as the perception, policy and behavior super-layers. The perception and behavior super-layers force an abstraction of visual and motor data trained with synthetic and simulated training samples, respectively. The policy super-layer is a small sub-network with fewer parameters that maps data in-between the abstracted manifolds. It is trained for each task using methods for policy search reinforcement learning. We demonstrate the suitability of the proposed architecture and learning framework by training predictive policies for skilled object grasping and ball throwing on a PR2 robot. The effectiveness of the method is illustrated by the fact that these tasks are trained using only about 180 real robot attempts with qualitative terminal rewards.},
archivePrefix = {arXiv},
arxivId = {1703.00727},
author = {Ghadirzadeh, Ali and Maki, Atsuto and Kragic, Danica and Bjorkman, Marten},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2017.8206046},
eprint = {1703.00727},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IEEE International Conference on Intelligent Robots and Systems/Ghadirzadeh et al/Ghadirzadeh et al. - 2017 - Deep predictive policy training using reinforcement learning.pdf:pdf},
isbn = {9781538626825},
issn = {21530866},
pages = {2351--2358},
title = {{Deep predictive policy training using reinforcement learning}},
year = {2017}
}
@article{Kober2013,
abstract = {Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
doi = {10.1177/0278364913495721},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/The International Journal of Robotics Research/Kober, Bagnell, Peters/Kober, Bagnell, Peters - 2013 - Reinforcement learning in robotics A survey.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {learning control,reinforcement learning,robot,survey},
month = {sep},
number = {11},
pages = {1238--1274},
title = {{Reinforcement learning in robotics: A survey}},
url = {http://link.springer.com/10.1007/978-3-319-03194-1{\_}2 http://journals.sagepub.com/doi/10.1177/0278364913495721},
volume = {32},
year = {2013}
}
@article{Deisenroth2011,
abstract = {Policy search is a subfield in reinforcement learning which focuses on finding good parameters for a given policy parametrization. It is well suited for robotics as it can cope with high-dimensional state and action spaces, one of the main challenges in robot learning. We review recent successes of both model-free and model-based policy search in robot learning. Model-free policy search is a general approach to learn policies based on sampled trajectories. We classify model-free methods based on their policy evaluation strategy, policy update strategy, and exploration strategy and present a unified view on existing algorithms. Learning a policy is often easier than learning an accurate forward model, and, hence, model-free methods are more frequently used in practice. However, for each sampled trajectory, it is necessary to interact with the robot, which can be time consuming and challenging in practice. Model-based policy search addresses this problem by first learning a simulator of the robot's dynamics from data. Subsequently, the simulator generates trajectories that are used for policy learning. For both model-free and model-based policy search methods, we review their respective properties and their applicability to robotic systems.},
author = {Deisenroth, Marc Peter and Neumann, Gerhard and Peters, Jan},
doi = {10.1561/2300000021},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Foundations and Trends in Robotics/Deisenroth, Neumann, Peters/Deisenroth, Neumann, Peters - 2011 - A Survey on Policy Search for Robotics.pdf:pdf},
isbn = {9781601987037},
issn = {1935-8253},
journal = {Foundations and Trends in Robotics},
keywords = {Artificial Intelligence in Robotics,Markov Decision Processes,Planning and Control,Policy Search},
number = {1-2},
pages = {1--142},
title = {{A Survey on Policy Search for Robotics}},
url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-robotics/ROB-021},
volume = {2},
year = {2011}
}
@inproceedings{Yahya2017,
abstract = {In principle, reinforcement learning and policy search methods can enable robots to learn highly complex and general skills that may allow them to function amid the complexity and diversity of the real world. However, training a policy that generalizes well across a wide range of real-world conditions requires far greater quantity and diversity of experience than is practical to collect with a single robot. Fortunately, it is possible for multiple robots to share their experience with one another, and thereby, learn a policy collectively. In this work, we explore distributed and asynchronous policy learning as a means to achieve generalization and improved training times on challenging, real-world manipulation tasks. We propose a distributed and asynchronous version of Guided Policy Search and use it to demonstrate collective policy learning on a vision-based door opening task using four robots. We show that it achieves better generalization, utilization, and training times than the single robot alternative.},
archivePrefix = {arXiv},
arxivId = {1610.00673},
author = {Yahya, Ali and Li, Adrian and Kalakrishnan, Mrinal and Chebotar, Yevgen and Levine, Sergey},
booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
eprint = {1610.00673},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/2017 IEEERSJ International Conference on Intelligent Robots and Systems (IROS)/Yahya et al/Yahya et al. - 2017 - Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search.pdf:pdf},
isbn = {9781538626818},
keywords = {Deep Learning in Rob,Learning and Adaptive Systems},
pages = {79--86},
publisher = {IEEE},
title = {{Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search}},
url = {http://arxiv.org/abs/1610.00673},
year = {2017}
}
