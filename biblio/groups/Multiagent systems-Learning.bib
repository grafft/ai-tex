Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@incollection{Gupta2017a,
author = {Gupta, Jayesh K. and Egorov, Maxim and Kochenderfer, Mykel},
booktitle = {AAMAS 2017 Best Papers},
doi = {10.1007/978-3-319-71682-4_5},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Gupta, Egorov, Kochenderfer/2017/Cooperative Multi-agent Control Using Deep Reinforcement Learning.pdf:pdf},
keywords = {ars,gamification,lsp,teaching mas},
pages = {66--83},
title = {{Cooperative Multi-agent Control Using Deep Reinforcement Learning}},
url = {http://link.springer.com/10.1007/978-3-319-71682-4{\_}5},
volume = {2},
year = {2017}
}
@inproceedings{Dickens2010,
author = {Dickens, Luke and Broda, Krysia and Russo, Alessandra},
booktitle = {ECAI 2010: 19th European Conference on Artificial Intelligence, 16-20 August 2010, Lisbon, Portugal : Including Prestigious Applications of Artificial Intelligence (PAIS-2010) : Proceedings},
doi = {10.3233/978-1-60750-606-5-367},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Dickens, Broda, Russo/2010/The Dynamics of Multi-Agent Reinforcement Learning.pdf:pdf},
isbn = {9781607506065},
pages = {367--372},
title = {{The Dynamics of Multi-Agent Reinforcement Learning}},
year = {2010}
}
@article{Nikolaidis2014,
abstract = {We present a framework for learning human user models from joint-action demonstrations that enables the robot to compute a robust policy for a collaborative task with a human. The learning takes place completely automatically, without any human intervention. First, we describe the clustering of demonstrated action sequences into different human types using an unsupervised learning algorithm. These demonstrated sequences are also used by the robot to learn a reward function that is representative for each type, through the employment of an inverse reinforcement learning algorithm. The learned model is then used as part of a Mixed Observability Markov Decision Process formulation, wherein the human type is a partially observable variable. With this framework, we can infer, either offline or online, the human type of a new user that was not included in the training set, and can compute a policy for the robot that will be aligned to the preference of this new user and will be robust to deviations of the human actions from prior demonstrations. Finally we validate the approach using data collected in human subject experiments, and conduct proof-of-concept demonstrations in which a person performs a collaborative task with a small industrial robot.},
archivePrefix = {arXiv},
arxivId = {1405.6341},
author = {Nikolaidis, Stefanos and Gu, Keren and Ramakrishnan, Ramya and Shah, Julie},
doi = {10.1145/2696454.2696455},
eprint = {1405.6341},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Nikolaidis et al/2014/Efficient Model Learning for Human-Robot Collaborative Tasks.pdf:pdf},
isbn = {9781450328838},
issn = {21672148},
journal = {arXiv},
pages = {1--9},
title = {{Efficient Model Learning for Human-Robot Collaborative Tasks}},
url = {http://arxiv.org/abs/1405.6341},
year = {2014}
}
@article{Bansal2017,
abstract = {Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX},
archivePrefix = {arXiv},
arxivId = {1710.03748},
author = {Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
eprint = {1710.03748},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Bansal et al/2017/Emergent Complexity via Multi-Agent Competition.pdf:pdf},
journal = {arXiv},
title = {{Emergent Complexity via Multi-Agent Competition}},
url = {http://arxiv.org/abs/1710.03748},
year = {2017}
}
