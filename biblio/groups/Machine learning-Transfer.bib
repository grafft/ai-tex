Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Ruder2017,
abstract = {Domain adaptation is crucial in many real-world applications where the distribution of the training data differs from the distribution of the test data. Previous Deep Learning-based approaches to domain adaptation need to be trained jointly on source and target domain data and are therefore unappealing in scenarios where models need to be adapted to a large number of domains or where a domain is evolving, e.g. spam detection where attackers continuously change their tactics. To fill this gap, we propose Knowledge Adaptation, an extension of Knowledge Distillation (Bucilua et al., 2006; Hinton et al., 2015) to the domain adaptation scenario. We show how a student model achieves state-of-the-art results on unsupervised domain adaptation from multiple sources on a standard sentiment analysis benchmark by taking into account the domain-specific expertise of multiple teachers and the similarities between their domains. When learning from a single teacher, using domain similarity to gauge trustworthiness is inadequate. To this end, we propose a simple metric that correlates well with the teacher's accuracy in the target domain. We demonstrate that incorporating high-confidence examples selected by this metric enables the student model to achieve state-of-the-art performance in the single-source scenario.},
archivePrefix = {arXiv},
arxivId = {1702.02052},
author = {Ruder, Sebastian and Ghaffari, Parsa and Breslin, John G.},
eprint = {1702.02052},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/arXiv/Ruder, Ghaffari, Breslin/Ruder, Ghaffari, Breslin - 2017 - Knowledge Adaptation Teaching to Adapt.pdf:pdf},
journal = {arXiv},
month = {feb},
pages = {11},
title = {{Knowledge Adaptation: Teaching to Adapt}},
url = {http://arxiv.org/abs/1702.02052},
year = {2017}
}
@article{Mehta2008,
abstract = {Transfer learning seeks to leverage previously learned tasks to achieve faster learning in a new task. In this paper, we consider transfer learning in the context of related but distinct Reinforcement Learning (RL) problems. In particular, our RL problems are derived from Semi-Markov Decision Processes (SMDPs) that share the same transition dynamics but have different reward functions that are linear in a set of reward features. We formally define the transfer learning problem in the context of RL as learning an efficient algorithm to solve any SMDP drawn from a fixed distribution after experiencing a finite number of them. Furthermore, we introduce an online algorithm to solve this problem, Variable-Reward Reinforcement Learning (VRRL), that compactly stores the optimal value functions for several SMDPs, and uses them to optimally initialize the value function for a new SMDP. We generalize our method to a hierarchical RL setting where the different SMDPs share the same task hierarchy. Our experimental results in a simplified real-time strategy domain show that significant transfer learning occurs in both flat and hierarchical settings. Transfer is especially effective in the hierarchical setting where the overall value functions are decomposed into subtask value functions which are more widely amenable to transfer across different SMDPs.},
author = {Mehta, Neville and Natarajan, Sriraam and Tadepalli, Prasad and Fern, Alan},
doi = {10.1007/s10994-008-5061-y},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Machine Learning/Mehta et al/Mehta et al. - 2008 - Transfer in variable-reward hierarchical reinforcement learning.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Average-reward learning,Hierarchical reinforcement learning,Multi-criteria learning,Transfer learning},
number = {3},
pages = {289--312},
title = {{Transfer in variable-reward hierarchical reinforcement learning}},
volume = {73},
year = {2008}
}
@article{Wilson2012,
abstract = {Transfer learning is one way to close the gap between the apparent speed of human learning and the relatively slow pace of learning by machines. Transfer is doubly beneficial in reinforcement learning where the agent not only needs to generalize from sparse experience, but also needs to efficiently explore. In this paper, we show that the hierarchical Bayesian framework can be readily adapted to sequential decision problems and provides a natural formalization of transfer learning. Using our framework, we produce empirical results in a simple colored maze domain and a complex real-time strategy game. The results show that our Hierarchical Bayesian Transfer framework significantly improves learning speed when tasks are hierarchically related.},
author = {Wilson, Aaron and Fern, Alan and Tadepalli, Prasad},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Journal of Machine Learning {\ldots}/Wilson, Fern, Tadepalli/Wilson, Fern, Tadepalli - 2012 - Transfer Learning in Sequential Decision Problems A Hierarchical Bayesian Approach.pdf:pdf},
journal = {Journal of Machine Learning {\ldots}},
keywords = {erarchical bayesian framework,hi-,markov decision processes,reinforcement learning,transfer learning},
pages = {217--227},
title = {{Transfer Learning in Sequential Decision Problems: A Hierarchical Bayesian Approach.}},
url = {http://proceedings.mlr.press/v27/wilson12a/wilson12a.pdf{\%}0Ahttp://jmlr.csail.mit.edu/proceedings/papers/v27/wilson12a/wilson12a.pdf},
volume = {27},
year = {2012}
}
