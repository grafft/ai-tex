Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Greff2017,
abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs ({\$}\backslashapprox 15{\$} years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
archivePrefix = {arXiv},
arxivId = {1503.04069},
author = {Greff, Klaus and Srivastava, Rupesh K. and Koutnik, Jan and Steunebrink, Bas R. and Schmidhuber, Jurgen},
doi = {10.1109/TNNLS.2016.2582924},
eprint = {1503.04069},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Greff et al/2017/LSTM A Search Space Odyssey.pdf:pdf},
isbn = {9788578110796},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Functional ANalysis Of VAriance (fANOVA),long short-term memory (LSTM),random search,recurrent neural networks,sequence learning},
number = {10},
pages = {2222--2232},
pmid = {25246403},
title = {{LSTM: A Search Space Odyssey}},
volume = {28},
year = {2017}
}
@techreport{Jaeger2014a,
archivePrefix = {arXiv},
arxivId = {arXiv:1403.3369v1},
author = {Jaeger, Herbert},
eprint = {arXiv:1403.3369v1},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Jaeger/2014/Controlling Recurrent Neural Networks by Conceptors.pdf:pdf},
institution = {Jacobs University Bremen},
title = {{Controlling Recurrent Neural Networks by Conceptors}},
year = {2014}
}
@article{Graves2006,
abstract = {Many real-world sequence learning tasks re- quire the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their out- puts into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un- segmented sequences directly, thereby solv- ing both problems. An experiment on the TIMIT speech corpus demonstrates its ad- vantages over both a baseline HMM and a hybrid HMM-RNN.},
author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
doi = {10.1145/1143844.1143891},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Graves et al/2006/Connectionist Temporal Classification Labelling Unsegmented Sequence Data with Recurrent Neural Networks.pdf:pdf},
isbn = {1595933832},
issn = {10987576},
journal = {Proceedings of the 23rd international conference on Machine Learning},
pages = {369--376},
title = {{Connectionist Temporal Classification : Labelling Unsegmented Sequence Data with Recurrent Neural Networks}},
year = {2006}
}
@article{Koutnik2014a,
abstract = {Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when the long-term memory is required. This paper introduces a simple, yet powerful modification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of RNN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving two tasks: audio signal generation and TIMIT spoken word classification, where it outperforms both RNN and LSTM networks.},
archivePrefix = {arXiv},
arxivId = {1402.3511},
author = {Koutn{\'{i}}k, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, J{\"{u}}rgen},
eprint = {1402.3511},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Koutn{\'{i}}k et al/2014/A Clockwork RNN.pdf:pdf},
isbn = {9781634393973},
journal = {ArXIV},
month = {feb},
title = {{A Clockwork RNN}},
url = {http://arxiv.org/abs/1402.3511},
year = {2014}
}
@article{MacNeil2011,
abstract = {A central criticism of standard theoretical approaches to constructing stable, recurrent model networks is that the synaptic connection weights need to be finely-tuned. This criticism is severe because proposed rules for learning these weights have been shown to have various limitations to their biological plausibility. Hence it is unlikely that such rules are used to continuously fine-tune the network in vivo. We describe a learning rule that is able to tune synaptic weights in a biologically plausible manner. We demonstrate and test this rule in the context of the oculomotor integrator, showing that only known neural signals are needed to tune the weights. We demonstrate that the rule appropriately accounts for a wide variety of experimental results, and is robust under several kinds of perturbation. Furthermore, we show that the rule is able to achieve stability as good as or better than that provided by the linearly optimal weights often used in recurrent models of the integrator. Finally, we discuss how this rule can be generalized to tune a wide variety of recurrent attractor networks, such as those found in head direction and path integration systems, suggesting that it may be used to tune a wide variety of stable neural systems.},
author = {MacNeil, David and Eliasmith, Chris},
doi = {10.1371/journal.pone.0022885},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/MacNeil, Eliasmith/2011/Fine-tuning and the stability of recurrent neural networks.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {9},
pmid = {21980334},
title = {{Fine-tuning and the stability of recurrent neural networks}},
volume = {6},
year = {2011}
}
@article{Young2014,
abstract = {Deep machine learning offers a comprehensive framework for extracting meaningful features from complex observations in an unsupervised manner. The majority of deep learning architectures described in the literature primarily focus on extracting spatial features. However, in real-world settings, capturing temporal dependencies in observations is critical for accurate inference. This paper introduces an enhancement to DeSTIN - a compositional deep learning architecture in which each layer consists of multiple instantiations of a common node - that learns to represent spatiotemporal patterns in data based on a novel recurrent clustering algorithm. Contrary to mainstream deep architectures, such as deep belief networks where layer-by-layer training is assumed, each of the nodes in the proposed architecture is trained independently and in parallel. Moreover, top-down and bottom-up information flows facilitate rich feature formation. A semi-supervised setting is demonstrated achieving state-of-the-art results on the MNIST classification benchmarks. A GPU implementation is discussed further accentuating the scalability properties of the proposed framework. ?? 2013 Elsevier Ltd. All rights reserved.},
author = {Young, S. R. and Davis, A. and Mishtal, A. and Arel, I.},
doi = {10.1016/j.patrec.2013.07.013},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Young et al/2014/Hierarchical spatiotemporal feature extraction using recurrent online clustering.pdf:pdf},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Deep machine learning,Online clustering,Pattern recognition,Recurrent clustering,Spatiotemporal signals,Unsupervised feature extraction},
number = {1},
pages = {115--123},
publisher = {Elsevier B.V.},
title = {{Hierarchical spatiotemporal feature extraction using recurrent online clustering}},
url = {http://dx.doi.org/10.1016/j.patrec.2013.07.013},
volume = {37},
year = {2014}
}
