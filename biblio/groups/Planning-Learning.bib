Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Hertle2012,
author = {Hertle, Andreas and Dornhege, Christian and Keller, Thomas and Nebel, Bernhard},
booktitle = {ECAI 2012: 20h European Conference on Artificial Intelligence: Proceedings},
doi = {10.3233/978-1-61499-098-7-402},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Hertle et al/2012/Planning with Semantic Attachments An Object-Oriented View.pdf:pdf},
isbn = {9781614990987},
keywords = {plan},
mendeley-tags = {plan},
pages = {402--407},
title = {{Planning with Semantic Attachments : An Object-Oriented View}},
year = {2012}
}
@article{Fox2006,
abstract = {In this paper we present pddl+, a planning domain description language for modelling mixed discrete-continuous planning domains. We describe the syntax and modelling style of pddl+, showing that the language makes convenient the modelling of complex time-dependent effects. We provide a formal semantics for pddl+ by mapping planning instances into constructs of hybrid automata. Using the syntax of HAs as our semantic model we construct a semantic mapping to labelled transition systems to complete the formal interpretation of pddl+ planning instances. An advantage of building a mapping from pddl+ to HA theory is that it forms a bridge between the Planning and Real Time Systems research communities. One consequence is that we can expect to make use of some of the theoretical properties of HAs. For example, for a restricted class of HAs the Reachability problem (which is equivalent to Plan Existence) is decidable. pddl+ provides an alternative to the continuous durative action model of pddl2.1, adding a more flexible and robust model of time-dependent behaviour.},
author = {Fox, Maria and Long, Derek},
doi = {10.1613/jair.2044},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Fox, Long/2006/Modelling mixed discrete-continuous domains for planning.pdf:pdf},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
keywords = {plan},
mendeley-tags = {plan},
pages = {235--297},
title = {{Modelling mixed discrete-continuous domains for planning}},
volume = {27},
year = {2006}
}
@book{Zacharias2012,
address = {Berlin},
author = {Zacharias, Franziska},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Zacharias/2012/Knowledge Representations for Planning Manipulation Tasks.pdf:pdf},
isbn = {9783642251818},
pages = {144},
publisher = {Springer-Verlag},
title = {{Knowledge Representations for Planning Manipulation Tasks}},
volume = {16},
year = {2012}
}
@article{Santos2012,
abstract = {In a role-playing game, finding optimal trajectories is one of the most important tasks. In fact, the strategy decision system becomes a key component of a game engine. Determining the way in which decisions are taken (e.g. online, batch or simulated) and the consumed resources in decision making (e.g. execution time, memory) will influence, to a major degree, the game performance. When classical search algorithms such as A * can be used, they are the very first option. Nevertheless, such methods rely on precise and complete models of the search space so there are many interesting scenarios where its application is not possible, and hence, model free methods for sequential decision making under uncertainty are the best choice. In this paper, we propose a heuristic planning strategy to incorporate, into a Dyna agent, the ability of heuristic-search in path-finding. The proposed Dyna-H algorithm selects branches more likely to produce outcomes than other branches, just as A * does. However, unlike A *, it has the advantages of a model-free online reinforcement learning algorithm. We evaluate our proposed algorithm against the one-step Q-learning and Dyna-Q algorithms and found that the Dyna-H, with its advantages, produced clearly superior results. ?? 2011 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1101.4003v3},
author = {Santos, Matilde and {Martin H.}, Jose Antonio and Lopez, Victoria and Botella, Guillermo},
doi = {10.1016/j.knosys.2011.09.008},
eprint = {arXiv:1101.4003v3},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Santos et al/2012/Dyna-H A heuristic planning reinforcement learning algorithm applied to role-playing game strategy decision systems.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {A-star,Decision-making,Heuristic-search,Path-finding,Reinforcement-learning},
pages = {28--36},
publisher = {Elsevier B.V.},
title = {{Dyna-H: A heuristic planning reinforcement learning algorithm applied to role-playing game strategy decision systems}},
url = {http://dx.doi.org/10.1016/j.knosys.2011.09.008},
volume = {32},
year = {2012}
}
@inproceedings{Nasbe2014,
abstract = {Special issue with a variety of articles exploring the topic Deeper Learning},
author = {van Seijen, Harm and Sutton, Richard S.},
booktitle = {Proceedings of The 32nd International Conference on Machine Learning},
editor = {Bach, Francis and Blei, David},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/van Seijen, Sutton/2015/A Deeper Look at Planning as Learning from Replay.pdf:pdf},
keywords = {learning},
pages = {2314--2322},
series = {JMLR Workshop and Conference Proceedings},
title = {{A Deeper Look at Planning as Learning from Replay}},
url = {http://www.nasbe.org/wp-content/uploads/Standard{\_}Mar2014{\_}full{\_}online.pdf},
year = {2015}
}
@article{Milde2017,
author = {Milde, Moritz B. and Blum, Hermann and Dietm{\"{u}}ller, Alexander and Sumislawska, Dora and Conradt, J{\"{o}}rg and Indiveri, Giacomo and Sandamirskaya, Yulia},
doi = {10.3389/fnbot.2017.00028},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Milde et al/2017/Obstacle Avoidance and Target Acquisition for Robot Navigation Using a Mixed Signal AnalogDigital Neuromorphic Processing System.pdf:pdf},
issn = {1662-5218},
journal = {Frontiers in Neurorobotics},
keywords = {Dynamic neural fields,Dynamic vision sensor,Neuromorphic controller,Neurorobotics,Obstacle avoidance,Target acquisition},
month = {jul},
pages = {1--17},
title = {{Obstacle Avoidance and Target Acquisition for Robot Navigation Using a Mixed Signal Analog/Digital Neuromorphic Processing System}},
url = {http://journal.frontiersin.org/article/10.3389/fnbot.2017.00028/full},
volume = {11},
year = {2017}
}
@article{Yoon2008,
abstract = {A number of today's state-of-the-art planners are based on forward state-space search. The im- pressive performance can be attributed to progress in computing domain independent heuristics that perform well across many domains. However, it is easy to find domains where such heuristics provide poor guidance, leading to planning failure. Motivated by such failures, the focus of this pa- per is to investigate mechanisms for learning domain-specific knowledge to better control forward search in a given domain. While there has been a large body of work on inductive learning of con- trol knowledge for AI planning, there is a void of work aimed at forward-state-space search. One reason for this may be that it is challenging to specify a knowledge representation for compactly representing important concepts across a wide range of domains. One of the main contributions of this work is to introduce a novel feature space for representing such control knowledge. The key idea is to define features in terms of information computed via relaxed plan extraction, which has been a major source of success for non-learning planners. This gives a new way of leverag- ing relaxed planning techniques in the context of learning. Using this feature space, we describe three forms of control knowledge—reactive policies (decision list rules and measures of progress) and linear heuristics—and show how to learn them and incorporate them into forward state-space search. Our empirical results show that our approaches are able to surpass state-of-the-art non- learning planners across a wide range of planning competition domains.},
author = {Yoon, Sungwook and Fern, A and Givan, R},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Yoon, Fern, Givan/2008/Learning control knowledge for forward search planning.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {The Journal of Machine Learning Research},
keywords = {knowledge representation,machine learning,planning,search},
pages = {683--718},
title = {{Learning control knowledge for forward search planning}},
url = {http://dl.acm.org/citation.cfm?id=1390705},
volume = {9},
year = {2008}
}
@article{Kovacs2012a,
abstract = {Despite a recent increase of research activity in the field of multi-agent planning there is still no de-facto standard for the description of multi-agent planning problems similarly to the Planning Domain Definition Language (PDDL) in case of deterministic single-agent planning. For this reason, in this paper a multi-agent extension of the currently latest official version of PDDL (3.1) is proposed together with a corresponding multi-agent planning track for the International Planning Competition (IPC). Our aim is to allow for a more direct comparison of planning systems and approaches, a greater reuse of research, and a more coordinated development in the field. Multi-agent planning is fundamentally different from the single-agent case with a broad range of applications (e.g. multi-robot domains). Not only is it inherently harder because of an exponential increase of the number of actions in general, but among others also constructive/destructive synergies of concurrent actions, and agents' different abilities and goals may need to be considered. The proposed multi-agent extension copes with these issues and allows planning both for and by agents even in temporal, numeric domains. It implies minimal changes to the syntax of PDDL3.1 and the related parsers.},
author = {Kovacs, Daniel L.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kovacs/2012/A Multi-Agent Extension of PDDL3.pdf:pdf},
journal = {ICAPS Workshop International Planning Competition: Past, Present and Future (WS-IPC 2012)},
keywords = {plan},
mendeley-tags = {plan},
pages = {19--37},
title = {{A Multi-Agent Extension of PDDL3}},
url = {http://e-archivo.uc3m.es:8080/bitstream/10016/14914/1/proceedings-WS-IPC2012.pdf{\#}page=23},
year = {2012}
}
@inproceedings{Niveau2010,
author = {Niveau, Alexandre and Fargier, Helen and Pralet, Cedric and Verfaillie, Gerard},
booktitle = {ECAI 2010: 19th European Conference on Artificial Intelligence, 16-20 August 2010, Lisbon, Portugal : Including Prestigious Applications of Artificial Intelligence (PAIS-2010) : Proceedings},
doi = {10.3233/978-1-60750-606-5-459},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Niveau et al/2010/Knowledge Compilation Using Interval Automata and Applications to Planning.pdf:pdf},
isbn = {9781607506065},
pages = {459--464},
title = {{Knowledge Compilation Using Interval Automata and Applications to Planning}},
year = {2010}
}
@article{Billing2015,
abstract = {A technique for simultaneous planning and action based on dynamic field theory is presented. The model builds on previous work on representation of sequential behavior as attractors in dynamic neural fields. Here, we demonstrate how chains of competing attractors can be used to represent dynamic plans towards a goal state. The present work can be seen as an addition to a growing body of work that demonstrates the role of dynamic field theory as a bridge between low-level reactive approaches and high-level symbol processing mechanisms. The architecture is evaluated on a set of planning problems using a simulated e-puck robot, including analysis of the system's behavior in response to noise and temporary blockages of the planned route. The system makes no explicit distinction between planning and execution phases, allowing continuous adaptation of the planned path. The proposed architecture exploits the dynamic field theory property of stability in relation to noise and changes in the environment. The neural dynamics are also exploited such that stay-or-switch action selection emerges where blockage of a planned path occurs; stay until the transient blockage is removed versus switch to an alternative route to the goal.},
author = {Billing, E. and Lowe, R. and Sandamirskaya, Y.},
doi = {10.1177/1059712315601188},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Billing, Lowe, Sandamirskaya/2015/Simultaneous planning and action neural-dynamic sequencing of elementary behaviors in robot navigation.pdf:pdf},
issn = {1059-7123},
journal = {Adaptive Behavior},
keywords = {dynamic field theory,goal directed behavior,plan,simultaneous planning and action},
mendeley-tags = {plan},
number = {5},
pages = {243--264},
title = {{Simultaneous planning and action: neural-dynamic sequencing of elementary behaviors in robot navigation}},
url = {http://adb.sagepub.com/cgi/doi/10.1177/1059712315601188},
volume = {23},
year = {2015}
}
@article{Mizraji2015,
abstract = {We organize our behavior and store structured information with many procedures that require the coding of spatial and temporal order in specific neural modules. In the simplest cases, spatial and temporal relations are condensed in prepositions like “below” and “above”, “behind” and “in front of”, or “before” and “after”, etc. Neural operators lie beneath these words, sharing some similarities with logical gates that compute spatial and temporal asymmetric relations. We show how these operators can be modeled by means of neural matrix memories acting on Kronecker tensor products of vectors. The complexity of these memories is further enhanced by their ability to store episodes unfolding in space and time. How does the brain scale up from the raw plasticity of contingent episodic memories to the apparent stable connectivity of large neural networks? We clarify this transition by analyzing a model that flexibly codes episodic spatial and temporal structures into contextual markers capable of linking different memory modules.},
author = {Mizraji, Eduardo and Lin, Juan},
doi = {10.1007/s11571-015-9343-3},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Mizraji, Lin/2015/Modeling spatial–temporal operations with context-dependent associative memories.pdf:pdf},
isbn = {1157101593},
issn = {1871-4080},
journal = {Cognitive Neurodynamics},
keywords = {Cognitive order relations,Context-dependent associative memories,Hierarchical models,Neural computation},
number = {5},
pages = {523--534},
publisher = {Springer Netherlands},
title = {{Modeling spatial–temporal operations with context-dependent associative memories}},
url = {http://link.springer.com/10.1007/s11571-015-9343-3},
volume = {9},
year = {2015}
}
@article{Kaelbling2016,
author = {Kaelbling, Leslie Pack},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kaelbling/2016/Implicit Belief-Space Pre-images for Hierarchical Planning and Execution.pdf:pdf},
isbn = {9781467380256},
pages = {5455--5462},
title = {{Implicit Belief-Space Pre-images for Hierarchical Planning and Execution}},
year = {2016}
}
@article{Konidaris2004,
abstract = {We introduce a framework that enables an agent to autonomously learn its own symbolic representation of a low-level, continuous environment. Propositional symbols are formalized as names for probability distributions, providing a nat-ural means of dealing with uncertain rep-resentations and probabilistic plans. We determine the symbols that are sufficient for computing the probability with which a plan will succeed, and demonstrate the acquisition of a symbolic representation in a computer game domain.},
author = {Konidaris, George and Kaelbling, Leslie Pack and Lozano-Perez, Tomas},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Konidaris, Kaelbling, Lozano-Perez/2004/Symbol Acquisition for Probabilistic High-Level Planning.pdf:pdf},
keywords = {Special Track on Machine Learning},
title = {{Symbol Acquisition for Probabilistic High-Level Planning}},
year = {2004}
}
@article{Rueckert2016,
abstract = {A recurrent spiking neural network is proposed that implements planning as probabilistic inference for finite and infinite horizon tasks. The architecture splits this problem into two parts: The stochastic transient firing of the network embodies the dynamics of the planning task. With appropriate injected input this dynamics is shaped to generate high-reward state trajectories. A general class of reward- modulated plasticity rules for these afferent synapses is presented. The updates optimize the likelihood of getting a reward through a variant of an Expectation Maximization algorithm and learning is guaranteed to convergence to a local maximum. We find that the network dynamics are qualitatively similar to transient firing patterns during planning and foraging in the hippocampus of awake behaving rats. The model extends classical attractor models and provides a testable prediction on identifying modulating contextual information. In a real robot arm reaching and obstacle avoidance task the ability to represent multiple task solutions is investigated. The neural planning method with its local update rules provides the basis for future neuromorphic hardware implementations with promising potentials like large data processing abilities and early initiation of strategies to avoid dangerous situations in robot co-worker scenarios.},
author = {Rueckert, Elmar and Kappel, David and Tanneberg, Daniel and Pecevski, Dejan and Peters, Jan},
doi = {10.1038/srep21142},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Rueckert et al/2016/Recurrent Spiking Networks Solve Planning Tasks.pdf:pdf},
issn = {2045-2322},
journal = {Scientific Reports},
pages = {21142},
publisher = {Nature Publishing Group},
title = {{Recurrent Spiking Networks Solve Planning Tasks}},
url = {http://www.nature.com/articles/srep21142},
volume = {6},
year = {2016}
}
@inproceedings{Pratama2014,
abstract = {Uncertainty is one of the most difficult factors to handle when we wish to develop an algorithm for robot motion planning in real circumstances. This paper presents a solution for a robot to deal with “lack of observation” in the scope of object manipulation. Considering a robotic bartender that picks up a glass filled with an unknown amount of water and tilts it to pour the water into empty glasses, the question is how to find the angle at which the giver glass is tilted to pour the water to the same level in each of empty receiver glasses. To achieve the objective, the amount of water poured is represented with mathematical models of non-linear functions, and numerical simulations are performed using the point-based value iteration algorithm for POMDP to get corresponding tilting angles of the giver glass. We found that the experimental result accuracy reaches 99.025{\%} of similarity with the assumed mathematical model, given an initial tilting angle and the water level in the initial glass. We further verified the validity of the proposed algorithm through dynamic simulations.},
author = {Pratama, Ferdian and Jeong, Sungmoon and Chong, Nak Young},
booktitle = {The 11th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI 2014)},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Pratama, Jeong, Chong/2014/Learning Manipulative Skills Using a POMDP Framework.pdf:pdf},
isbn = {9781479953332},
keywords = {adaptive systems,dextrous manipulations,learning behavior,plan},
mendeley-tags = {plan},
pages = {169--175},
title = {{Learning Manipulative Skills Using a POMDP Framework}},
year = {2014}
}
@article{Hsu2014,
author = {Hsu, Yuan Pao and Jiang, Wei Cheng},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Hsu, Jiang/2014/A fast learning agent based on the dyna architecture.pdf:pdf},
issn = {10162364},
journal = {Journal of Information Science and Engineering},
keywords = {CMAC,Dyna agent ?? 2014, institute of information scien,Prioritized sweeping,Q-learning,Reinforcement learning},
number = {6},
pages = {1807--1823},
title = {{A fast learning agent based on the dyna architecture}},
volume = {30},
year = {2014}
}
@article{Barto1995,
abstract = {Learning methods based on dynamic programming (DP) are receiving increasing attention in artificial intelligence. Researchers have argued that DP provides the appropriate basis for compiling planning results into reactive strategies for real-time control, as well as for learning such strategies when the system being controlled is incompletely known. We introduce an algorithm based on DP, which we call Real-Time DP (RTDP), by which an embedded system can improve its performance with experience. RTDP generalizes Korf's Learning-Real-Time-A* algorithm to problems involving uncertainty. We invoke results from the theory of asynchronous DP to prove that RTDP achieves optimal behavior in several different classes of problems. We also use the theory of asynchronous DP to illuminate aspects of other DP-based reinforcement learning methods such as Watkins' Q-Learning algorithm. A secondary aim of this article is to provide a bridge between AI research on real-time planning and learning and relevant concepts and algorithms from control theory. ?? 1995 Elsevier Science B.V. All rights reserved.},
author = {Barto, Andrew G. and Bradtke, Steven J. and Singh, Satinder P.},
doi = {10.1016/0004-3702(94)00011-O},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Barto, Bradtke, Singh/1995/Learning to act using real-time dynamic programming.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
number = {1-2},
pages = {81--138},
title = {{Learning to act using real-time dynamic programming}},
volume = {72},
year = {1995}
}
@article{Paxton2017,
abstract = {We consider task and motion planning in complex dynamic environments for problems expressed in terms of a set of Linear Temporal Logic (LTL) constraints, and a reward function. We propose a methodology based on reinforcement learning that employs deep neural networks to learn low-level control policies as well as task-level option policies. A major challenge in this setting, both for neural network approaches and classical planning, is the need to explore future worlds of a complex and interactive environment. To this end, we integrate Monte Carlo Tree Search with hierarchical neural net control policies trained on expressive LTL specifications. This paper investigates the ability of neural networks to learn both LTL constraints and control policies in order to generate task plans in complex environments. We demonstrate our approach in a simulated autonomous driving setting, where a vehicle must drive down a road in traffic, avoid collisions, and navigate an intersection, all while obeying given rules of the road.},
archivePrefix = {arXiv},
arxivId = {1703.07887},
author = {Paxton, Chris and Raman, Vasumathi and Hager, Gregory D. and Kobilarov, Marin},
eprint = {1703.07887},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Paxton et al/2017/Combining Neural Networks and Tree Search for Task and Motion Planning in Challenging Environments.pdf:pdf},
journal = {ArXiv: 1703.07887},
title = {{Combining Neural Networks and Tree Search for Task and Motion Planning in Challenging Environments}},
url = {http://arxiv.org/abs/1703.07887},
year = {2017}
}
@article{Fox2003a,
abstract = {In recent years research in the planning community has moved increasingly toward s application of planners to realistic problems involving both time and many typ es of resources. For example, interest in planning demonstrated by the space res earch community has inspired work in observation scheduling, planetary rover ex ploration and spacecraft control domains. Other temporal and resource-intensive domains including logistics planning, plant control and manufacturing have also helped to focus the community on the modelling and reasoning issues that must be confronted to make planning technology meet the challenges of application. The International Planning Competitions have acted as an important motivating fo rce behind the progress that has been made in planning since 1998. The third com petition (held in 2002) set the planning community the challenge of handling tim e and numeric resources. This necessitated the development of a modelling langua ge capable of expressing temporal and numeric properties of planning domains. In this paper we describe the language, PDDL2.1, that was used in the competition. We describe the syntax of the language, its formal semantics and the validation of concurrent plans. We observe that PDDL2.1 has considerable modelling power --- exceeding the capabilities of current planning technology --- and presents a number of important challenges to the research community.},
archivePrefix = {arXiv},
arxivId = {1106.4561},
author = {Fox, Maria and Long, Derek},
doi = {10.1613/jair.1129},
eprint = {1106.4561},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Fox, Long/2003/PDDL2.1 An extension to PDDL for expressing temporal planning domains.pdf:pdf},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
keywords = {plan},
mendeley-tags = {plan},
pages = {61--124},
title = {{PDDL2.1: An extension to PDDL for expressing temporal planning domains}},
volume = {20},
year = {2003}
}
@article{Vezhnevets2016,
abstract = {We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub- sequences by learning for how long the plan can be committed to - i.e. followed without re-planing. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro- actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.},
archivePrefix = {arXiv},
arxivId = {1606.04695},
author = {Alexander and Vezhnevets and Mnih, Volodymyr and Agapiou, John and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Kavukcuoglu, Koray},
eprint = {1606.04695},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Alexander et al/2016/Strategic Attentive Writer for Learning Macro-Actions.pdf:pdf},
journal = {arXiv},
month = {jun},
title = {{Strategic Attentive Writer for Learning Macro-Actions}},
url = {http://arxiv.org/abs/1606.04695},
year = {2016}
}
@inproceedings{Silver2017,
author = {Silver, David and van Hasselt, Hado and Hessel, Matteo and Schaul, Tom and Guez, Arthur and Harley, Tim and Dulac-Arnold, Gabriel and Reichert, David and Rabinowitz, Neil and Barreto, Andre and Degris, Thomas},
booktitle = {ICML 2017},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Silver et al/2017/The Predictron End-To-End Learning and Planning.pdf:pdf},
keywords = {development plans,implementation,legislations,regulate,sustainable development},
title = {{The Predictron: End-To-End Learning and Planning}},
year = {2017}
}
@article{Gupta2017,
abstract = {We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the planner, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. Our experiments demonstrate that CMP outperforms both reactive strategies and standard memory-based architectures and performs well in novel environments. Furthermore, we show that CMP can also achieve semantically specified goals, such as "go to a chair".},
archivePrefix = {arXiv},
arxivId = {1702.03920},
author = {Gupta, Saurabh and Davidson, James and Levine, Sergey and Sukthankar, Rahul and Malik, Jitendra},
eprint = {1702.03920},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Gupta et al/2017/Cognitive Mapping and Planning for Visual Navigation.pdf:pdf},
journal = {ArXiv: 1702.03920},
month = {feb},
title = {{Cognitive Mapping and Planning for Visual Navigation}},
url = {http://arxiv.org/abs/1702.03920},
year = {2017}
}
