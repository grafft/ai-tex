Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Lanctot2017,
abstract = {To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and fictitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.},
archivePrefix = {arXiv},
arxivId = {1711.00832},
author = {Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and Perolat, Julien and Silver, David and Graepel, Thore},
eprint = {1711.00832},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Lanctot et al/Lanctot et al. - 2017 - A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning.pdf:pdf},
number = {Nips},
title = {{A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning}},
url = {http://arxiv.org/abs/1711.00832},
year = {2017}
}
@article{Dimakopoulou2018,
abstract = {We consider a team of reinforcement learning agents that concurrently operate in a common environment, and we develop an approach to efficient coordinated exploration that is suitable for problems of practical scale. Our approach builds on seed sampling (Dimakopoulou and Van Roy, 2018) and randomized value function learning (Osband et al., 2016). We demonstrate that, for simple tabular contexts, the approach is competitive with previously proposed tabular model learning methods (Dimakopoulou and Van Roy, 2018). With a higher-dimensional problem and a neural network value function representation, the approach learns quickly with far fewer agents than alternative exploration schemes.},
archivePrefix = {arXiv},
arxivId = {1805.08948},
author = {Dimakopoulou, Maria and Osband, Ian and {Van Roy}, Benjamin},
doi = {10.1.1.151.8250},
eprint = {1805.08948},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Dimakopoulou, Osband, Van Roy/Dimakopoulou, Osband, Van Roy - 2018 - Scalable Coordinated Exploration in Concurrent Reinforcement Learning.pdf:pdf},
isbn = {0262042088},
issn = {1049-5258},
title = {{Scalable Coordinated Exploration in Concurrent Reinforcement Learning}},
url = {http://arxiv.org/abs/1805.08948},
year = {2018}
}
@article{Rabinowitz2018,
abstract = {Theory of mind (ToM; Premack {\&} Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test (Wimmer {\&} Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.},
archivePrefix = {arXiv},
arxivId = {1802.07740},
author = {Rabinowitz, Neil C. and Perbet, Frank and Song, H. Francis and Zhang, Chiyuan and Eslami, S. M. Ali and Botvinick, Matthew},
eprint = {1802.07740},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Rabinowitz et al/Rabinowitz et al. - 2018 - Machine Theory of Mind.pdf:pdf},
issn = {1938-7228},
title = {{Machine Theory of Mind}},
url = {http://arxiv.org/abs/1802.07740},
year = {2018}
}
@article{Bargiacchi2018,
abstract = {Learning to coordinate between multiple agents is an important problem in many reinforcement learning problems. Key to learning to coordi-nate is exploiting loose couplings, i.e., condi-tional independences between agents. In this paper we study learning in repeated fully coop-erative games, multi-agent multi-armed bandits (MAMABs), in which the expected rewards can be expressed as a coordination graph. We pro-pose multi-agent upper confidence exploration (MAUCE), a new algorithm for MAMABs that ex-ploits loose couplings, which enables us to prove a regret bound that is logarithmic in the number of arm pulls and only linear in the number of agents. We empirically compare MAUCE to sparse coop-erative Q-learning, and a state-of-the-art combina-torial bandit approach, and show that it performs much better on a variety of settings, including learning control policies for wind farms.},
author = {Bargiacchi, Eugenio and Verstraeten, Timothy and Roijers, Diederik M and Now{\'{e}}, Ann and {Van Hasselt}, Hado},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Bargiacchi et al/Bargiacchi et al. - 2018 - Learning to Coordinate with Coordination Graphs in Repeated Single-Stage Multi-Agent Decision Problems.pdf:pdf},
title = {{Learning to Coordinate with Coordination Graphs in Repeated Single-Stage Multi-Agent Decision Problems}},
url = {http://proceedings.mlr.press/v80/bargiacchi18a/bargiacchi18a.pdf},
year = {2018}
}
@inproceedings{Yahya2017,
abstract = {In principle, reinforcement learning and policy search methods can enable robots to learn highly complex and general skills that may allow them to function amid the complexity and diversity of the real world. However, training a policy that generalizes well across a wide range of real-world conditions requires far greater quantity and diversity of experience than is practical to collect with a single robot. Fortunately, it is possible for multiple robots to share their experience with one another, and thereby, learn a policy collectively. In this work, we explore distributed and asynchronous policy learning as a means to achieve generalization and improved training times on challenging, real-world manipulation tasks. We propose a distributed and asynchronous version of Guided Policy Search and use it to demonstrate collective policy learning on a vision-based door opening task using four robots. We show that it achieves better generalization, utilization, and training times than the single robot alternative.},
archivePrefix = {arXiv},
arxivId = {1610.00673},
author = {Yahya, Ali and Li, Adrian and Kalakrishnan, Mrinal and Chebotar, Yevgen and Levine, Sergey},
booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
eprint = {1610.00673},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/2017 IEEERSJ International Conference on Intelligent Robots and Systems (IROS)/Yahya et al/Yahya et al. - 2017 - Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search.pdf:pdf},
isbn = {9781538626818},
keywords = {Deep Learning in Rob,Learning and Adaptive Systems},
pages = {79--86},
publisher = {IEEE},
title = {{Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search}},
url = {http://arxiv.org/abs/1610.00673},
year = {2017}
}
@article{Raileanu2018,
abstract = {We consider the multi-agent reinforcement learning setting with imperfect information in which each agent is trying to maximize its own utility. The reward function depends on the hidden state (or goal) of both agents, so the agents must infer the other players' hidden goals from their observed behavior in order to solve the tasks. We propose a new approach for learning in these domains: Self Other-Modeling (SOM), in which an agent uses its own policy to predict the other agent's actions and update its belief of their hidden state in an online manner. We evaluate this approach on three different tasks and show that the agents are able to learn better policies using their estimate of the other players' hidden states, in both cooperative and adversarial settings.},
archivePrefix = {arXiv},
arxivId = {1802.09640},
author = {Raileanu, Roberta and Denton, Emily and Szlam, Arthur and Fergus, Rob},
eprint = {1802.09640},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Raileanu et al/Raileanu et al. - 2018 - Modeling Others using Oneself in Multi-Agent Reinforcement Learning.pdf:pdf},
title = {{Modeling Others using Oneself in Multi-Agent Reinforcement Learning}},
url = {http://arxiv.org/abs/1802.09640},
year = {2018}
}
