Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Yahya2017,
abstract = {In principle, reinforcement learning and policy search methods can enable robots to learn highly complex and general skills that may allow them to function amid the complexity and diversity of the real world. However, training a policy that generalizes well across a wide range of real-world conditions requires far greater quantity and diversity of experience than is practical to collect with a single robot. Fortunately, it is possible for multiple robots to share their experience with one another, and thereby, learn a policy collectively. In this work, we explore distributed and asynchronous policy learning as a means to achieve generalization and improved training times on challenging, real-world manipulation tasks. We propose a distributed and asynchronous version of Guided Policy Search and use it to demonstrate collective policy learning on a vision-based door opening task using four robots. We show that it achieves better generalization, utilization, and training times than the single robot alternative.},
archivePrefix = {arXiv},
arxivId = {1610.00673},
author = {Yahya, Ali and Li, Adrian and Kalakrishnan, Mrinal and Chebotar, Yevgen and Levine, Sergey},
booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
eprint = {1610.00673},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Yahya et al/2017/Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search.pdf:pdf},
isbn = {9781538626818},
keywords = {Deep Learning in Rob,Learning and Adaptive Systems},
pages = {79--86},
publisher = {IEEE},
title = {{Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search}},
url = {http://arxiv.org/abs/1610.00673},
year = {2017}
}
