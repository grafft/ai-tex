Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Al-Emran2015,
author = {Al-Emran, Mostafa},
doi = {10.12785/ijcds/040207},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/International Journal of Computing and Digital Systems/Al-Emran/Al-Emran - 2015 - Hierarchical Reinforcement Learning - A Survey.pdf:pdf},
issn = {2210-142X},
journal = {International Journal of Computing and Digital Systems},
keywords = {hierarchical reinforcement learning,q-learning,reinforcement learning},
month = {apr},
number = {2},
pages = {137--143},
title = {{Hierarchical Reinforcement Learning - A Survey}},
url = {https://journal.journals.uob.edu.bh//Article/ArticleFile/2144},
volume = {4},
year = {2015}
}
@article{Co-Reyes2018,
abstract = {In this work, we take a representation learning perspective on hierarchical reinforcement learning, where the problem of learning lower layers in a hierarchy is transformed into the problem of learning trajectory-level generative models. We show that we can learn continuous latent representations of trajectories, which are effective in solving temporally extended and multi-stage problems. Our proposed model, SeCTAR, draws inspiration from variational autoencoders, and learns latent representations of trajectories. A key component of this method is to learn both a latent-conditioned policy and a latent-conditioned model which are consistent with each other. Given the same latent, the policy generates a trajectory which should match the trajectory predicted by the model. This model provides a built-in prediction mechanism, by predicting the outcome of closed loop policy behavior. We propose a novel algorithm for performing hierarchical RL with this model, combining model-based planning in the learned latent space with an unsupervised exploration objective. We show that our model is effective at reasoning over long horizons with sparse rewards for several simulated tasks, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration.},
archivePrefix = {arXiv},
arxivId = {1806.02813},
author = {Co-Reyes, John D. and Liu, YuXuan and Gupta, Abhishek and Eysenbach, Benjamin and Abbeel, Pieter and Levine, Sergey},
eprint = {1806.02813},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Co-Reyes et al/Co-Reyes et al. - 2018 - Self-Consistent Trajectory Autoencoder Hierarchical Reinforcement Learning with Trajectory Embeddings.jpg:jpg;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Co-Reyes et al/Co-Reyes et al. - 2018 - Self-Consistent Trajectory Autoencoder Hierarchical Reinforcement Learning with Trajectory Embeddings.pdf:pdf},
issn = {1938-7228},
title = {{Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings}},
url = {http://arxiv.org/abs/1806.02813},
year = {2018}
}
@article{Rasmussen2017a,
abstract = {We present the first model capable of performing hierarchical reinforcement learning in a general, neurally detailed imple-mentation. We show that this model is able to learn a spatial pickup and delivery task more quickly than one without hier-archical abilities. In addition, we show that this model is able to leverage its hierarchical structure to transfer learned knowl-edge between related tasks. These results point towards the advantages to be gained by using a hierarchical RL framework to understand the brain's powerful learning ability.},
author = {Rasmussen, Daniel and Voelker, Aaron and Eliasmith, Chris},
doi = {10.1371/journal.pone.0180234},
editor = {Cymbalyuk, Gennady},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/PLOS ONE/Rasmussen, Voelker, Eliasmith/Rasmussen, Voelker, Eliasmith - 2017 - A neural model of hierarchical reinforcement learning(2).pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/PLOS ONE/Rasmussen, Voelker, Eliasmith/Rasmussen, Voelker, Eliasmith - 2017 - A neural model of hierarchical reinforcement learning.pdf:pdf},
isbn = {1111111111},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {brain could achieve its,hierarchical,hrl into a theory,neural engineering framework,neural model,of neural function,providing a hypothe-,reinforcement learning,sis for how the,strengths in scaling},
month = {jul},
number = {7},
pages = {e0180234},
title = {{A neural model of hierarchical reinforcement learning}},
url = {http://dx.plos.org/10.1371/journal.pone.0180234},
volume = {12},
year = {2017}
}
@inproceedings{Harutyunyan2017,
abstract = {A temporally abstract action, or an option, is specified by a policy and a termination condition: the policy guides option behavior, and the termination condition roughly determines its length. Generally, learning with longer options (like learning with multi-step returns) is known to be more efficient. However, if the option set for the task is not ideal, and cannot express the primitive optimal policy exactly, shorter options offer more flexibility and can yield a better solution. Thus, the termination condition puts learning efficiency at odds with solution quality. We propose to resolve this dilemma by decoupling the behavior and target terminations, just like it is done with policies in off-policy learning. To this end, we give a new algorithm, Q($\backslash$beta), that learns the solution with respect to any termination condition, regardless of how the options actually terminate. We derive Q($\backslash$beta) by casting learning with options into a common framework with well-studied multi-step off-policy learning. We validate our algorithm empirically, and show that it holds up to its motivating claims.},
archivePrefix = {arXiv},
arxivId = {1711.03817},
author = {Harutyunyan, Anna and Vrancx, Peter and Bacon, Pierre-Luc and Precup, Doina and Nowe, Ann},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
eprint = {1711.03817},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Harutyunyan et al/Harutyunyan et al. - 2017 - Learning with Options that Terminate Off-Policy.pdf:pdf},
title = {{Learning with Options that Terminate Off-Policy}},
url = {http://arxiv.org/abs/1711.03817},
year = {2017}
}
@inproceedings{Stulp2011,
abstract = {Temporal abstraction and task decomposition drastically reduce the search space for planning and control, and are fundamental to making complex tasks amenable to learning. In the context of reinforcement learning, temporal abstractions are studied within the paradigm of hierarchical reinforcement learning.},
author = {Stulp, Freek and Schaal, Stefan},
booktitle = {2011 11th IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/Humanoids.2011.6100841},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/2011 11th IEEE-RAS International Conference on Humanoid Robots/Stulp, Schaal/Stulp, Schaal - 2011 - Hierarchical reinforcement learning with movement primitives.pdf:pdf},
isbn = {978-1-61284-868-6},
issn = {21640572},
month = {oct},
pages = {231--238},
publisher = {IEEE},
title = {{Hierarchical reinforcement learning with movement primitives}},
url = {http://ieeexplore.ieee.org/document/6100841/},
year = {2011}
}
@article{Teh2017,
abstract = {Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (Distill {\&} transfer learning). Instead of sharing parameters between the different workers, we propose to share a "distilled" policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust and more stable---attributes that are critical in deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1707.04175},
author = {Teh, Yee Whye and Bapst, Victor and Czarnecki, Wojciech Marian and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan},
eprint = {1707.04175},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Teh et al/Teh et al. - 2017 - Distral Robust Multitask Reinforcement Learning.pdf:pdf},
issn = {10495258},
title = {{Distral: Robust Multitask Reinforcement Learning}},
url = {http://arxiv.org/abs/1707.04175},
year = {2017}
}
@article{For2018,
author = {For, Ethod},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/For/For - 2018 - an Inference-Based Policy Gradient Method for Learning Options.pdf:pdf},
pages = {1--10},
title = {{an Inference-Based Policy Gradient Method for Learning Options}},
year = {2018}
}
@article{Oh2017,
abstract = {As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions.},
archivePrefix = {arXiv},
arxivId = {1706.05064},
author = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak and Kohli, Pushmeet},
eprint = {1706.05064},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Oh et al/Oh et al. - 2017 - Multi-task Deep Reinforcement Learning for Zero-shot Generalization with Hierarchical Task Dependencies.pdf:pdf},
title = {{Multi-task Deep Reinforcement Learning for Zero-shot Generalization with Hierarchical Task Dependencies}},
url = {http://arxiv.org/abs/1706.05064},
year = {2017}
}
