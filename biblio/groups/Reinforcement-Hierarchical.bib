Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Al-Emran2015,
author = {Al-Emran, Mostafa},
doi = {10.12785/ijcds/040207},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Al-Emran/2015/Hierarchical Reinforcement Learning - A Survey.pdf:pdf},
issn = {2210-142X},
journal = {International Journal of Computing and Digital Systems},
keywords = {hierarchical reinforcement learning,q-learning,reinforcement learning},
month = {apr},
number = {2},
pages = {137--143},
title = {{Hierarchical Reinforcement Learning - A Survey}},
url = {https://journal.journals.uob.edu.bh//Article/ArticleFile/2144},
volume = {4},
year = {2015}
}
@inproceedings{Harutyunyan2017,
abstract = {A temporally abstract action, or an option, is specified by a policy and a termination condition: the policy guides option behavior, and the termination condition roughly determines its length. Generally, learning with longer options (like learning with multi-step returns) is known to be more efficient. However, if the option set for the task is not ideal, and cannot express the primitive optimal policy exactly, shorter options offer more flexibility and can yield a better solution. Thus, the termination condition puts learning efficiency at odds with solution quality. We propose to resolve this dilemma by decoupling the behavior and target terminations, just like it is done with policies in off-policy learning. To this end, we give a new algorithm, Q($\backslash$beta), that learns the solution with respect to any termination condition, regardless of how the options actually terminate. We derive Q($\backslash$beta) by casting learning with options into a common framework with well-studied multi-step off-policy learning. We validate our algorithm empirically, and show that it holds up to its motivating claims.},
archivePrefix = {arXiv},
arxivId = {1711.03817},
author = {Harutyunyan, Anna and Vrancx, Peter and Bacon, Pierre-Luc and Precup, Doina and Nowe, Ann},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
eprint = {1711.03817},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Harutyunyan et al/2017/Learning with Options that Terminate Off-Policy.pdf:pdf},
title = {{Learning with Options that Terminate Off-Policy}},
url = {http://arxiv.org/abs/1711.03817},
year = {2017}
}
@inproceedings{Stulp2011,
abstract = {Temporal abstraction and task decomposition drastically reduce the search space for planning and control, and are fundamental to making complex tasks amenable to learning. In the context of reinforcement learning, temporal abstractions are studied within the paradigm of hierarchical reinforcement learning.},
author = {Stulp, Freek and Schaal, Stefan},
booktitle = {2011 11th IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/Humanoids.2011.6100841},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Stulp, Schaal/2011/Hierarchical reinforcement learning with movement primitives.pdf:pdf},
isbn = {978-1-61284-868-6},
issn = {21640572},
month = {oct},
pages = {231--238},
publisher = {IEEE},
title = {{Hierarchical reinforcement learning with movement primitives}},
url = {http://ieeexplore.ieee.org/document/6100841/},
year = {2011}
}
@article{Rasmussen2017a,
abstract = {We present the first model capable of performing hierarchical reinforcement learning in a general, neurally detailed imple-mentation. We show that this model is able to learn a spatial pickup and delivery task more quickly than one without hier-archical abilities. In addition, we show that this model is able to leverage its hierarchical structure to transfer learned knowl-edge between related tasks. These results point towards the advantages to be gained by using a hierarchical RL framework to understand the brain's powerful learning ability.},
author = {Rasmussen, Daniel and Voelker, Aaron and Eliasmith, Chris},
doi = {10.1371/journal.pone.0180234},
editor = {Cymbalyuk, Gennady},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Rasmussen, Voelker, Eliasmith/2017/A neural model of hierarchical reinforcement learning.pdf:pdf;:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Rasmussen, Voelker, Eliasmith/2017/A neural model of hierarchical reinforcement learning(2).pdf:pdf},
isbn = {1111111111},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {brain could achieve its,hierarchical,hrl into a theory,neural engineering framework,neural model,of neural function,providing a hypothe-,reinforcement learning,sis for how the,strengths in scaling},
month = {jul},
number = {7},
pages = {e0180234},
title = {{A neural model of hierarchical reinforcement learning}},
url = {http://dx.plos.org/10.1371/journal.pone.0180234},
volume = {12},
year = {2017}
}
