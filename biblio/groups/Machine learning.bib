Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Ruder2017,
abstract = {Domain adaptation is crucial in many real-world applications where the distribution of the training data differs from the distribution of the test data. Previous Deep Learning-based approaches to domain adaptation need to be trained jointly on source and target domain data and are therefore unappealing in scenarios where models need to be adapted to a large number of domains or where a domain is evolving, e.g. spam detection where attackers continuously change their tactics. To fill this gap, we propose Knowledge Adaptation, an extension of Knowledge Distillation (Bucilua et al., 2006; Hinton et al., 2015) to the domain adaptation scenario. We show how a student model achieves state-of-the-art results on unsupervised domain adaptation from multiple sources on a standard sentiment analysis benchmark by taking into account the domain-specific expertise of multiple teachers and the similarities between their domains. When learning from a single teacher, using domain similarity to gauge trustworthiness is inadequate. To this end, we propose a simple metric that correlates well with the teacher's accuracy in the target domain. We demonstrate that incorporating high-confidence examples selected by this metric enables the student model to achieve state-of-the-art performance in the single-source scenario.},
archivePrefix = {arXiv},
arxivId = {1702.02052},
author = {Ruder, Sebastian and Ghaffari, Parsa and Breslin, John G.},
eprint = {1702.02052},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Ruder, Ghaffari, Breslin/2017/Knowledge Adaptation Teaching to Adapt.pdf:pdf},
journal = {arXiv},
month = {feb},
pages = {11},
title = {{Knowledge Adaptation: Teaching to Adapt}},
url = {http://arxiv.org/abs/1702.02052},
year = {2017}
}
@techreport{Michalski2004,
author = {Michalski, Ryszard S.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Michalski/2004/Generating Alternative Hypotheses in AQ Le.pdf:pdf},
institution = {Gerge Mason university},
pages = {14},
title = {{Generating Alternative Hypotheses in AQ Le}},
year = {2004}
}
@article{Wiering1997,
author = {Wiering, M. and Schmidhuber, J.},
doi = {10.1177/105971239700600202},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Wiering, Schmidhuber/1997/HQ-Learning.pdf:pdf},
isbn = {1059712397006},
issn = {1059-7123},
journal = {Adaptive Behavior},
keywords = {hierarchical q-learning,non-markov,pomdps,reinforcement learning,subgoal learning},
number = {2},
pages = {219--246},
title = {{HQ-Learning}},
url = {http://adb.sagepub.com/cgi/doi/10.1177/105971239700600202},
volume = {6},
year = {1997}
}
@book{Westland2015,
address = {Cham},
author = {Westland, J. Christopher},
doi = {10.1007/978-3-319-16507-3},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Westland/2015/Structural Equation Models.pdf:pdf},
isbn = {978-3-319-16506-6},
issn = {1099-095X},
month = {mar},
pages = {184},
pmid = {20890403},
publisher = {Springer International Publishing},
series = {Studies in Systems, Decision and Control},
title = {{Structural Equation Models}},
url = {http://link.springer.com/10.1007/978-3-319-16507-3},
volume = {22},
year = {2015}
}
@techreport{Wojtusiak2006,
author = {Wojtusiak, Janusz and Michalski, Ryszard S. and Kaufman, Kenneth A. and Pietrzykowski, Jaroslaw},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Wojtusiak et al/2006/Multitype Pattern Discovery via AQ21. A Brief Description of the Method and Its Novel Features.pdf:pdf},
pages = {1--25},
title = {{Multitype Pattern Discovery via AQ21. A Brief Description of the Method and Its Novel Features}},
year = {2006}
}
@article{Hinton2013,
abstract = {It is possible to learn multiple layers of non-linear features by backpropagating error derivatives through a feedforward neural network. This is a very effective learning procedure when there is a huge amount of labeled training data, but for many learning tasks very few labeled examples are available. In an effort to overcome the need for labeled data, several different generative models were developed that learned interesting features by modeling the higher order statistical structure of a set of input vectors. One of these generative models, the restricted Boltzmann machine (RBM), has no connections between its hidden units and this makes perceptual inference and learning much simpler. More significantly, after a layer of hidden features has been learned, the activities of these features can be used as training data for another RBM. By applying this idea recursively, it is possible to learn a deep hierarchy of progressively more complicated features without requiring any labeled data. This deep hierarchy can then be treated as a feedforward neural network which can be discriminatively fine-tuned using backpropagation. Using a stack of RBMs to initialize the weights of a feedforward neural network allows backpropagation to work effectively in much deeper networks and it leads to much better generalization. A stack of RBMs can also be used to initialize a deep Boltzmann machine that has many hidden layers. Combining this initialization method with a new method for fine-tuning the weights finally leads to the first efficient way of training Boltzmann machines with many hidden layers and millions of weights.},
author = {Hinton, Geoffrey},
doi = {10.1111/cogs.12049},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Hinton/2013/Where Do Features Come From.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Backpropagation,Boltzmann machines,Contrastive divergence,Deep learning,Distributed representations,Learning features,Learning graphical models,Variational learning},
pages = {1--24},
pmid = {23800216},
title = {{Where Do Features Come From?}},
year = {2013}
}
@article{Stuart2010,
abstract = {When estimating causal effects using observational data, it is desirable to replicate a randomized experiment as closely as possible by obtaining treated and control groups with similar covariate distributions. This goal can often be achieved by choosing well-matched samples of the original treated and control groups, thereby reducing bias due to the covariates. Since the 1970's, work on matching methods has examined how to best choose treated and control subjects for comparison. Matching methods are gaining popularity in fields such as economics, epidemiology, medicine, and political science. However, until now the literature and related advice has been scattered across disciplines. Researchers who are interested in using matching methods-or developing methods related to matching-do not have a single place to turn to learn about past and current research. This paper provides a structure for thinking about matching methods and guidance on their use, coalescing the existing research (both old and new) and providing a summary of where the literature on matching methods is now and where it should be headed.},
author = {Stuart, Elizabeth A.},
doi = {10.1214/09-STS313},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Stuart/2010/Matching methods for causal inference A review and a look forward.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Observational study,and phrases,fication,observational study,propensity scores,subclassi-,subclassif,weighting},
number = {1},
pages = {1--21},
pmid = {20871802},
title = {{Matching methods for causal inference: A review and a look forward}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2943670{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {25},
year = {2010}
}
@article{Seen2008,
author = {Gorder, P. F.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Gorder/2008/Computer Vision, Inspared by the Human Brain.pdf:pdf},
journal = {Computing in Science and Engineering},
title = {{Computer Vision, Inspared by the Human Brain}},
volume = {6},
year = {2008}
}
@inproceedings{Michalski1973,
author = {Michalski, Ryszard S.},
booktitle = {Proceeding of the First International Joint Conference on Artificial Intelligence (IJCAI'73)},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Michalski/1973/AQVAL1 - Computer Implementation of a Variable-Valued Logic System VL1 and Examples of its Application to Pattern Recognition.pdf:pdf},
pages = {3--17},
title = {{AQVAL/1 - Computer Implementation of a Variable-Valued Logic System VL1 and Examples of its Application to Pattern Recognition}},
year = {1973}
}
@article{Kostavelis2012,
author = {Kostavelis, Ioannis and Gasteratos, Antonios},
doi = {10.1016/j.patrec.2011.11.017},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kostavelis, Gasteratos/2012/On the optimization of Hierarchical Temporal Memory.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
number = {5},
pages = {670--676},
publisher = {Elsevier B.V.},
title = {{On the optimization of Hierarchical Temporal Memory}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865511004077},
volume = {33},
year = {2012}
}
@phdthesis{Shshkin2010,
author = {Шашкин, Леонид Олегович},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Шашкин/2010/Приближенные средства установления сходств для ДСМ-метода автоматического порождения гипотез.pdf:pdf},
language = {russian},
pages = {26},
school = {Российский государственный гуманитарный университет},
title = {{Приближенные средства установления сходств для ДСМ-метода автоматического порождения гипотез}},
year = {2010}
}
@article{Pietron2016,
author = {Pietron, Marcin and Wielgosz, Maciej and Wiatr, Kazimierz},
doi = {10.5220/0005706603460353},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Pietron, Wielgosz, Wiatr/2016/Parallel Implementation of Spatial Pooler in Hierarchical Temporal Memory.pdf:pdf},
isbn = {978-989-758-172-4},
journal = {Proceedings of the 8th International Conference on Agents and Artificial Intelligence},
number = {Icaart},
pages = {346--353},
title = {{Parallel Implementation of Spatial Pooler in Hierarchical Temporal Memory}},
url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0005706603460353},
volume = {2},
year = {2016}
}
@incollection{Kubat1996,
author = {Kubat, Miroslav and Bratko, Ivan and Michalski, Ryszard S.},
booktitle = {Machine Learning and Data Mining: Methods and Applications},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kubat, Bratko, Michalski/1996/A Review of Machine Learning Methods.pdf:pdf},
pages = {1--72},
title = {{A Review of Machine Learning Methods}},
year = {1996}
}
@unpublished{Goertzel2011,
author = {Goertzel, Ben and Chen, Shuo},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Goertzel, Chen/2016/Notes on Semantic Hierarchical Temporal Memory for Perceptual, Motoric and Intentional Intelligence.pdf:pdf},
title = {{Notes on Semantic Hierarchical Temporal Memory for Perceptual, Motoric and Intentional Intelligence}},
year = {2016}
}
@article{McCall2013,
abstract = {Human-level intelligent agents must autonomously navigate complex, dynamic, uncertain environments with bounded time and memory. This requires that they continually update a hierarchical, dynamic, probabilistic (uncertain) internal model of their current situation, via approximate Bayesian inference, incorporating both the sensory data and a generative model of its causes. Such modeling requires suitable representation at multiple levels of abstraction from the sub-symbolic, sensory level to the most abstract conceptual representation. To guide our approach, we identify principles for perceptual representation, perceptual inference, and the associated learning processes. Based on these, a predictive coding extension to the HTM Cortical Learning Algorithms, termed PC-CLA, is proposed as a foundational building block for the systems-level LIDA cognitive architecture. PC-CLA fleshes out LIDA's internal representations, memory, learning and attentional processes; and takes an initial step towards the comprehensive use of distributed and probabilistic (uncertain) representation throughout the architecture.},
author = {McCall, Ryan James and Franklin, Stan},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/McCall, Franklin/2013/Cortical Learning Algorithms with Predictive Coding for a Systems-Level Cognitive Architecture.pdf:pdf},
journal = {Second Annual Conference on Advances in Cognitive Systems},
pages = {149--166},
title = {{Cortical Learning Algorithms with Predictive Coding for a Systems-Level Cognitive Architecture}},
year = {2013}
}
@phdthesis{Ignatov2010,
author = {Игнатов, Д. И.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Игнатов/2010/Модели, алгоритмы и программные средства бикластеризации на основе замкнутых множеств.pdf:pdf},
language = {russian},
pages = {26},
school = {Высшая школа экономики},
title = {{Модели, алгоритмы и программные средства бикластеризации на основе замкнутых множеств}},
year = {2010}
}
@unpublished{Byrne2015,
abstract = {In the decade since Jeff Hawkins proposed Hierarchical Temporal Memory (HTM) as a model of neocortical computation, the theory and the algorithms have evolved dramatically. This paper presents a detailed description ofHTM's Cortical Learning Algorithm (CLA), including for the first time a rigorous mathematical formulation of all aspects of the computations. Prediction As- sisted CLA (paCLA), a refinement of the CLA, is presented, which is both closer to the neuroscience and adds significantly to the computational power. Finally, we summarise the key functions of neocortex which are expressed in paCLA implementations. An Open Source project, Comportex, is the leading implementation of this evolving theory of the brain.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.08255v2},
author = {Byrne, Fergal},
eprint = {arXiv:1509.08255v2},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Byrne/2015/Encoding Reality Prediction-Assisted Cortical Learning Algorithm in Hierarchical Temporal Memory.pdf:pdf},
pages = {1--28},
title = {{Encoding Reality: Prediction-Assisted Cortical Learning Algorithm in Hierarchical Temporal Memory}},
year = {2015}
}
@article{Vorontcov2004b,
author = {Воронцов, К. В.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Воронцов/2004/Комбинаторные обоснования обучаемых обучаемых алгоритмов.pdf:pdf},
journal = {Журнал вычислительной матемематики и математической физики},
language = {russian},
number = {11},
pages = {1997--2009},
title = {{Комбинаторные обоснования обучаемых обучаемых алгоритмов}},
volume = {44},
year = {2004}
}
@article{Zhang2012,
abstract = {Bio-inspired mapping methods have started a new trend in the robotics navigation area. In this paper, we propose a new map building framework based on the neocortex model: Hierarchical Temporary Memory (HTM). HTM has tree-shaped hierarchical structure and demonstrates structural and algorithmic properties of the human brain neocortex. We first treat the mapping problem as the object recognition problem, and design HTM network hierarchical structure. Secondly, the Speed Up Robust Features (SURF) descriptors were extracted from the grabbed images. These descriptors were further projected into visual words. The presence or absence of visual words consists of input data of HTM in the form of binary sequences. With the binary visual words sequences, HTM network stored or recognized the scene information which were reflected in the visual words, and the output of HTM was the related environment map. After training the HTM network, we evaluated it by two sets of environment data. The results show that the HTM based mapping strategy can build the environment map successfully and handle the loop closing problem with high performance.},
author = {Zhang, Xinzheng and Zhang, Jianfen and Rad, Ahmad B. and Mai, Xiaochun and Jin, Yichen},
doi = {10.1109/ROBIO.2012.6491012},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Zhang et al/2012/A novel mapping strategy based on neocortex model Pre-liminary results by hierarchical temporal memory.pdf:pdf},
isbn = {9781467321273},
journal = {Proceedings of the 2012 IEEE International Conference on Robotics and Biomimetics},
pages = {476--481},
title = {{A novel mapping strategy based on neocortex model: Pre-liminary results by hierarchical temporal memory}},
year = {2012}
}
@inproceedings{Fayyad1993,
author = {Fayyad, Usama M. and Irani, Keki B.},
booktitle = {IJCAI 1993},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Fayyad, Irani/1993/Multi-Interval Discretization of Continuous-Valued Attributes for Classigication Learning.pdf:pdf},
pages = {1022--1027},
title = {{Multi-Interval Discretization of Continuous-Valued Attributes for Classigication Learning}},
year = {1993}
}
@unpublished{Fu2004a,
abstract = {Concept lattice is an effective tool and platform for data analysis and knowledge discovery such as classification or association rules mining. The lattice algorithm to build formal concepts and concept lattice plays an essential role in the application of concept lattice. In fact, more than ten algorithms for generating concept lattices were published. As real data sets for data mining are very large, concept lattice structure suffers from its complexity issues on such data. The efficiency and performance of concept lattices algorithms are very different from one to another.In order to increase the efficiency of concept lattice-based algorithms in data mining, it is necessary to make use of an efficient algorithm to build concept lattices.So we need to com- pare the existing lattice algorithms and develop more efficient algorithm. We implemented the four first algorithms in Java environment and compared these algorithms on about 30 datasets of the UCI repository that are well established to be used to compare ML algorithms. Preliminary results give preference to Ganter's algorithm, and then to Bordat's algorithm, nevertheless these algorithms still suffers when dealing with huge datasets. We analyzed the duality of the lattice-based algorithms. Furthermore, we propose a new efficient scalable lattice-based algorithm: ScalingNextClosure to decompose the search space of any huge data in some partitions, and then generate independently concepts in each partition. The experimental results show the efficiency of this algorithm.},
author = {Fu, Huaiguo and Nguifo, Em},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Fu, Nguifo/2004/A lattice algorithm for data mining.pdf:pdf},
keywords = {Concept lattice,data mining,lattice algorithm},
pages = {20},
title = {{A lattice algorithm for data mining}},
url = {http://www.cril.univ-artois.fr/{~}mephu/fu-mephu{\_}ISI{\_}04.pdf},
year = {2004}
}
@article{Yu2014,
abstract = {We introduce the proto-object model of visual clutter perception. This unsupervised model segments an image into superpixels, then merges neighboring superpixels that share a common color cluster to obtain proto-objects-defined here as spatially extended regions of coherent features. Clutter is estimated by simply counting the number of proto-objects. We tested this model using 90 images of realistic scenes that were ranked by observers from least to most cluttered. Comparing this behaviorally obtained ranking to a ranking based on the model clutter estimates, we found a significant correlation between the two (Spearman's $\rho$ = 0.814, p {\textless} 0.001). We also found that the proto-object model was highly robust to changes in its parameters and was generalizable to unseen images. We compared the proto-object model to six other models of clutter perception and demonstrated that it outperformed each, in some cases dramatically. Importantly, we also showed that the proto-object model was a better predictor of clutter perception than an actual count of the number of objects in the scenes, suggesting that the set size of a scene may be better described by proto-objects than objects. We conclude that the success of the proto-object model is due in part to its use of an intermediate level of visual representation-one between features and objects-and that this is evidence for the potential importance of a proto-object representation in many common visual percepts and tasks.},
author = {Yu, Chen-Ping and Samaras, Dimitris and Zelinsky, Gregory J.},
doi = {10.1167/14.7.4},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Yu, Samaras, Zelinsky/2014/Modeling visual clutter perception using proto-object segmentation.pdf:pdf},
issn = {1534-7362},
journal = {Journal of vision},
keywords = {Adolescent,Adult,Attention,Attention: physiology,Computer Simulation,Crowding,Eye Movements,Eye Movements: physiology,Humans,Visual Perception,Visual Perception: physiology,Young Adult},
number = {7},
pages = {1--16},
pmid = {24904121},
title = {{Modeling visual clutter perception using proto-object segmentation}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24904121},
volume = {14},
year = {2014}
}
@article{Zabezhailo2014a,
author = {Забежайло, М. И.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Забежайло/2014/О некоторых возможностях управления перебором в ДСМ - методе. Часть II.pdf:pdf;:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Забежайло/2014/О некоторых возможностях управления перебором в ДСМ - методе. Часть II.pdf:pdf},
journal = {Искусственный интеллект и принятие решений},
language = {russian},
number = {2},
pages = {3--18},
title = {{О некоторых возможностях управления перебором в ДСМ - методе. Часть II}},
year = {2014}
}
@article{Nguen2012,
author = {Нгуен, Т. Т. and Болотова, Ю. А. and Спицын, В. Г.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Нгуен, Болотова, Спицын/2012/Обработка данных видеопоследовательности в режиме реального времени на основе иерархической временной сети.pdf:pdf},
journal = {Научный вестник НГТУ},
language = {russian},
number = {2},
pages = {33--43},
title = {{Обработка данных видеопоследовательности в режиме реального времени на основе иерархической временной сети}},
volume = {47},
year = {2012}
}
@article{Oliva2001,
author = {Oliva, Aude and Torralba, Antonio},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Oliva, Torralba/2001/Modeling the Shape of the Scene A Holistic Representation of the Spatial Envelope.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {energy spectrum,natural images,principal components,scene recognition,spatial layout},
number = {3},
pages = {145--175},
title = {{Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope}},
volume = {42},
year = {2001}
}
@article{Friston2009,
abstract = {This paper considers prediction and perceptual categorization as an inference problem that is solved by the brain. We assume that the brain models the world as a hierarchy or cascade of dynamical systems that encode causal structure in the sensorium. Perception is equated with the optimization or inversion of these internal models, to explain sensory data. Given a model of how sensory data are generated, we can invoke a generic approach to model inversion, based on a free energy bound on the model's evidence. The ensuing free-energy formulation furnishes equations that prescribe the process of recognition, i.e. the dynamics of neuronal activity that represent the causes of sensory input. Here, we focus on a very general model, whose hierarchical and dynamical structure enables simulated brains to recognize and predict trajectories or sequences of sensory states. We first review hierarchical dynamical models and their inversion. We then show that the brain has the necessary infrastructure to implement this inversion and illustrate this point using synthetic birds that can recognize and categorize birdsongs.},
author = {Friston, Karl and Kiebel, Stefan},
doi = {10.1098/rstb.2008.0300},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Friston, Kiebel/2009/Predictive coding under the free-energy principle.pdf:pdf},
isbn = {0962-8436},
issn = {0962-8436},
journal = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},
keywords = {birdsong,generative models,hierarchical,predictive coding},
number = {1521},
pages = {1211--1221},
pmid = {19528002},
title = {{Predictive coding under the free-energy principle}},
volume = {364},
year = {2009}
}
@article{Gopnik2004,
abstract = {The authors outline a cognitive and computational account of causal learning in children. They propose that children use specialized cognitive systems that allow them to recover an accurate “causal map” of the world: an abstract, coherent, learned representation of the causal relations among events. This kind of knowledge can be perspicuously understood in terms of the formalism of directed graphical causal models, or Bayes nets. Children's causal learning and inference may involve computations similar to those for learning causal Bayes nets and for predicting with them. Experimental results suggest that 2to 4-year-old children construct new causal maps and that their learning is consistent with the Bayes net formalism.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gopnik, A and Glymour, C and Sobel, D M and Schulz, L E and Kushnir, T and Danks, D},
doi = {10.1037/0033-295X.111.1.3},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Gopnik et al/2004/A theory of causal learning in children.pdf:pdf},
isbn = {0033-295X},
issn = {0033-295X},
journal = {Psychological Review},
keywords = {SLC, complex systems},
pages = {3--32},
pmid = {14756583},
title = {{A theory of causal learning in children}},
year = {2004}
}
@article{Kaufman2000,
author = {Kaufman, Kenneth A. and Michalski, Ryszard S.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kaufman, Michalski/2000/An Adjustable Description Quality Measure for Pattern Discovery in Large Databases Using the AQ Methodology.pdf:pdf},
journal = {Journal of Intelligent Information Systems},
keywords = {aq learning,data mining,decision rules,learning from noisy data,machine learning,natural induction,separate and conquer},
pages = {199--216},
title = {{An Adjustable Description Quality Measure for Pattern Discovery in Large Databases Using the AQ Methodology}},
volume = {14},
year = {2000}
}
@inproceedings{Maloof,
author = {Maloof, Marcus A. and Michalski, Ryszard S.},
booktitle = {Proceedings of the Intelligent Information Systems Workshop},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Maloof, Michalski/1999/AQ-PM A System for Partial Memory Learning.pdf:pdf},
pages = {70--79},
title = {{AQ-PM: A System for Partial Memory Learning}},
year = {1999}
}
@article{Efremova2013,
author = {Ефремова, Н А and Инуи, Тошио},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Ефремова, Инуи/2013/Модель зрительной коры головного мозга для распознавания и классификации образов.pdf:pdf},
journal = {Искусственный интеллект и принятие решений},
language = {russian},
number = {1},
pages = {55--62},
title = {{Модель зрительной коры головного мозга для распознавания и классификации образов}},
year = {2013}
}
@techreport{Michalski1986,
author = {Michalski, Ryszard S. and Mozetic, I. and Hong, J.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Michalski, Mozetic, Hong/1986/The AQ15 Inductive Learning System an Overview and Experiments.pdf:pdf},
institution = {University of Illinois},
pages = {36},
title = {{The AQ15 Inductive Learning System: an Overview and Experiments}},
year = {1986}
}
@incollection{1995,
author = {Стейджер, Джеймс},
booktitle = {STATISTICA},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Стейджер/1995/Моделирование структурными уравнениями.PDF:PDF},
pages = {3535--3698},
title = {{Моделирование структурными уравнениями}},
year = {1995}
}
@article{Zhdanov2011,
author = {Жданов, А. А.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Жданов/2011/Биологически инспирированное техническое зрение в системах автономного искусственного интеллекта.pdf:pdf},
journal = {Механика, управление и информатика},
language = {russian},
number = {6},
pages = {245--267},
title = {{Биологически инспирированное техническое зрение в системах автономного искусственного интеллекта}},
year = {2011}
}
@techreport{Michalski1975a,
author = {Michalski, Ryszard S. and Larson, James},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Michalski, Larson/1975/AQVAL1 (AQ7) User's Guide and Program Description.pdf:pdf},
institution = {University of Illinois},
pages = {94},
title = {{AQVAL/1 (AQ7) User's Guide and Program Description}},
year = {1975}
}
@incollection{Wojtusiak2006a,
author = {Wojtusiak, Janusz and Michalski, Ryszard S.},
booktitle = {Intelligent Information Processing and Web Mining},
editor = {Klopotek, Mieczys{\l}aw A. and Wierzchon, S{\l}awomir T. and Trojanowski, Krzysztof},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Wojtusiak, Michalski/2006/The Use of Compound Attributes in AQ Learning.pdf:pdf},
pages = {189--198},
publisher = {Springer-Verlag},
title = {{The Use of Compound Attributes in AQ Learning}},
year = {2006}
}
@incollection{Leeuwenberg2015,
abstract = {In this paper we explore the possibility of defining an original pattern structure for managing syntactic trees. More precisely, we are interested in the extraction of relations such as drug-drug interactions (DDIs) in medical texts where sentences are represented as syntactic trees. In this specific pattern structure, called STPS, the similarity operator is based on rooted tree intersection. Moreover, we introduce “Lazy Pattern Structure Classification” (LPSC), which is a symbolic method able to extract and classify DDI sentences w.r.t. STPS. To decrease computation time, a projection and a set of tree-simplification operations are proposed. We evaluated the method by means of a 10-fold cross validation on the corpus of the DDI extraction challenge 2011, and we obtained very encouraging results that are reported at the end of the paper.},
author = {Leeuwenberg, Artuur and Buzmakov, Aleksey and Toussaint, Yannick and Napoli, Amedeo},
booktitle = {Formal Concept Analysis},
doi = {10.1007/978-3-319-19545-2_10},
editor = {Baixeries, Jaume and Sacarea, Christian and Ojeda-Aciego, Manuel},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Leeuwenberg et al/2015/Exploring Pattern Structures of Syntactic Trees for Relation Extraction.pdf:pdf},
keywords = {analysis,ddi extraction,formal concept,pattern structures,relation extraction},
pages = {153--168},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Exploring Pattern Structures of Syntactic Trees for Relation Extraction}},
year = {2015}
}
@article{Lee2003,
abstract = {Traditional views of visual processing suggest that early visual neurons in areas V1 and V2 are static spatiotemporal filters that extract local features from a visual scene. The extracted information is then channeled through a feedforward chain of modules in successively higher visual areas for further analysis. Recent electrophysiological recordings from early visual neurons in awake behaving monkeys reveal that there are many levels of complexity in the information processing of the early visual cortex, as seen in the long-latency responses of its neurons. These new findings suggest that activity in the early visual cortex is tightly coupled and highly interactive with the rest of the visual system. They lead us to propose a new theoretical setting based on the mathematical framework of hierarchical Bayesian inference for reasoning about the visual system. In this framework, the recurrent feedforward/feedback loops in the cortex serve to integrate top-down contextual priors and bottom-up observations so as to implement concurrent probabilistic inference along the visual hierarchy. We suggest that the algorithms of particle filtering and Bayesian-belief propagation might model these interactive cortical computations. We review some recent neurophysiological evidences that support the plausibility of these ideas.},
author = {Lee, Tai Sing and Mumford, David},
doi = {10.1364/JOSAA.20.001434},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Lee, Mumford/2003/Hierarchical Bayesian inference in the visual cortex.pdf:pdf},
isbn = {1084-7529 (Print)},
issn = {1084-7529},
journal = {Journal of the Optical Society of America. A, Optics, image science, and vision},
number = {7},
pages = {1434--1448},
pmid = {12868647},
title = {{Hierarchical Bayesian inference in the visual cortex}},
volume = {20},
year = {2003}
}
@unpublished{Vorontcov2009a,
author = {Воронцов, К. В.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Воронцов/2009/Лекции по статистическим (байесовским) алгоритмам классификации.pdf:pdf},
language = {russian},
pages = {39},
title = {{Лекции по статистическим (байесовским) алгоритмам классификации}},
year = {2009}
}
@article{Finn2014,
author = {Финн, В. К.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Финн/2014/Эпистемологические принципы порождения гипотез.pdf:pdf},
journal = {Вопросы философии},
keywords = {2014 г,abduction,analogy,dsm-,dsm-reasoning,induction,method formalisms,natural-scientifi c problem of,the problem of induction,к,финн в},
language = {russian},
number = {2},
pages = {83--96},
title = {{Эпистемологические принципы порождения гипотез}},
year = {2014}
}
@article{Mehta2008,
abstract = {Transfer learning seeks to leverage previously learned tasks to achieve faster learning in a new task. In this paper, we consider transfer learning in the context of related but distinct Reinforcement Learning (RL) problems. In particular, our RL problems are derived from Semi-Markov Decision Processes (SMDPs) that share the same transition dynamics but have different reward functions that are linear in a set of reward features. We formally define the transfer learning problem in the context of RL as learning an efficient algorithm to solve any SMDP drawn from a fixed distribution after experiencing a finite number of them. Furthermore, we introduce an online algorithm to solve this problem, Variable-Reward Reinforcement Learning (VRRL), that compactly stores the optimal value functions for several SMDPs, and uses them to optimally initialize the value function for a new SMDP. We generalize our method to a hierarchical RL setting where the different SMDPs share the same task hierarchy. Our experimental results in a simplified real-time strategy domain show that significant transfer learning occurs in both flat and hierarchical settings. Transfer is especially effective in the hierarchical setting where the overall value functions are decomposed into subtask value functions which are more widely amenable to transfer across different SMDPs.},
author = {Mehta, Neville and Natarajan, Sriraam and Tadepalli, Prasad and Fern, Alan},
doi = {10.1007/s10994-008-5061-y},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Mehta et al/2008/Transfer in variable-reward hierarchical reinforcement learning.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Average-reward learning,Hierarchical reinforcement learning,Multi-criteria learning,Transfer learning},
number = {3},
pages = {289--312},
title = {{Transfer in variable-reward hierarchical reinforcement learning}},
volume = {73},
year = {2008}
}
@article{Vityaev2012,
author = {Витяев, Е. Е. and Неупокоев, Н. В.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Витяев, Неупокоев/2012/Формальная модель восприятия и образа как неподвижной точки предвосхищений.pdf:pdf},
journal = {Нейроинформатика},
language = {russian},
number = {1},
pages = {28--41},
title = {{Формальная модель восприятия и образа как неподвижной точки предвосхищений}},
volume = {6},
year = {2012}
}
@article{1999a,
author = {Объедков, С. А.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Объедков/1999/Алгоритмические аспекты ДСМ-метода автоматического порождения гипотез(2).pdf:pdf;:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Объедков/1999/Алгоритмические аспекты ДСМ-метода автоматического порождения гипотез.pdf:pdf},
journal = {Научно-техническая информация. сер. 2. Информационные процессы и системы},
language = {russian},
number = {1-2},
pages = {64--75},
title = {{Алгоритмические аспекты ДСМ-метода автоматического порождения гипотез}},
volume = {1-2},
year = {1999}
}
@techreport{Michalski1978,
author = {Michalski, Ryszard S.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Michalski/1978/Pattern Recognition as Knowledge-Guided Computer Induction.pdf:pdf},
institution = {University of Illinois},
pages = {44},
title = {{Pattern Recognition as Knowledge-Guided Computer Induction}},
year = {1978}
}
@article{Hochreiter2001,
author = {Hochreiter, Sepp and Younger, A Steven and Conwell, Peter R},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Hochreiter, Younger, Conwell/2001/Learning to Learn Using Gradient Descent.pdf:pdf},
pages = {87--94},
title = {{Learning to Learn Using Gradient Descent}},
year = {2001}
}
@phdthesis{Kozhuhova2009,
author = {Кожунова, О. С.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Кожунова/2009/Технология разработки семантического словаря системы информационного мониторинга.pdf:pdf},
language = {russian},
pages = {21},
school = {Институт проблем информатики},
title = {{Технология разработки семантического словаря системы информационного мониторинга}},
year = {2009}
}
@inproceedings{Teo,
author = {Teo, Ching L. and Myers, Austin and Ferm, Cornelia and Aloimonos, Yiannis},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Teo et al/2013/Embedding High-Level Information into Low Level Vision Efficient Object Search in Clutter.pdf:pdf},
pages = {126--132},
title = {{Embedding High-Level Information into Low Level Vision: Efficient Object Search in Clutter}},
year = {2013}
}
@article{Shtovba2004,
author = {Штовба, С. Д.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Штовба/2004/Муравьиные алгоритмы.pdf:pdf},
journal = {Математика в приложениях},
language = {russian},
number = {4},
pages = {70--75},
title = {{Муравьиные алгоритмы}},
year = {2004}
}
@article{Kleyko2017,
abstract = {Modality corresponding to medical images is a vital filter in medical image retrieval systems. This article presents the classification of modalities of medical images based on the usage of principles of hyper-dimensional computing and reservoir computing. It is demonstrated that the highest classification accuracy of the proposed method is on a par with the best classical method for the given dataset (83{\%} vs. 84{\%}). The major positive property of the proposed method is that it does not require any optimization routine during the training phase and naturally allows for incremental learning upon the availability of new training data},
author = {Kleyko, Denis and Khan, Sumeer and Osipov, Evgeny and Yong, Suet Peng},
doi = {10.1109/ISBI.2017.7950697},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kleyko et al/2017/Modality classification of medical images with distributed representations based on cellular automata reservoir computing.pdf:pdf},
isbn = {9781509011711},
issn = {19458452},
journal = {Proceedings - International Symposium on Biomedical Imaging},
keywords = {Classification,Machine learning},
pages = {1053--1056},
title = {{Modality classification of medical images with distributed representations based on cellular automata reservoir computing}},
year = {2017}
}
@book{Kline2015,
author = {Kline, Rex B.},
edition = {4th},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kline/2015/Principles and practices of structural equation modelling.pdf:pdf},
isbn = {9781609182304},
pages = {554},
publisher = {The Guilford Press},
title = {{Principles and practices of structural equation modelling}},
year = {2015}
}
@article{Kim2009,
author = {Kim, Nancy S. and Park, Edward Y.},
doi = {10.3758/BRM.41.1.128},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kim, Park/2009/ConceptBuilder An open-source software tool for measuring, depicting, and quantifying causal models.pdf:pdf},
issn = {1554-3528},
journal = {Behavior Research Methods},
month = {feb},
number = {1},
pages = {128--136},
title = {{ConceptBuilder: An open-source software tool for measuring, depicting, and quantifying causal models}},
url = {http://link.springer.com/10.3758/BRM.41.1.128},
volume = {41},
year = {2009}
}
@article{Carpenter1987,
author = {Carpenter, Gaila and Grossberg, Stephen},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Carpenter, Grossberg/1987/A Massively Parallel Architecture for a Self-Organizing Neural Pattern Recognition Machine.pdf:pdf},
journal = {Computer Vision, Graphics and Image Processing},
keywords = {nn},
pages = {54--115},
title = {{A Massively Parallel Architecture for a Self-Organizing Neural Pattern Recognition Machine}},
volume = {37},
year = {1987}
}
@book{Bishop2006,
author = {Bishop, Christopher M},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Bishop/2006/Pattern Recognition and Machine Learning.pdf:pdf},
pages = {758},
publisher = {Springer},
title = {{Pattern Recognition and Machine Learning}},
year = {2006}
}
@article{Norris1978,
author = {Norris, E. M.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Norris/1978/An Algorithm for Computing the Maximal Rectangles in a Binary Relation.pdf:pdf},
journal = {Revue Roumaine de Math{\'{e}}matiques Pures et Appliqu{\'{e}}es},
number = {2},
pages = {243--250},
title = {{An Algorithm for Computing the Maximal Rectangles in a Binary Relation}},
volume = {23},
year = {1978}
}
@inproceedings{Kuznetsov2001,
author = {Kuznetsov, Sergei O. and Ob''edkov, Sergei A.},
booktitle = {ICCS'01 International Workshop on Concept Lattices-based KDD},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kuznetsov, Ob''edkov/2001/Comparing Performance of Algorithms for Generating Concept Lattices.pdf:pdf},
pages = {35--47},
title = {{Comparing Performance of Algorithms for Generating Concept Lattices}},
year = {2001}
}
@article{Lerner2012,
abstract = {Localist models of spreading activation (SA) and models assuming distributed representations offer very different takes on semantic priming, a widely investigated paradigm in word recognition and semantic memory research. In this study, we implemented SA in an attractor neural network model with distributed representations and created a unified framework for the two approaches. Our models assume a synaptic depression mechanism leading to autonomous transitions between encoded memory patterns (latching dynamics), which account for the major characteristics of automatic semantic priming in humans. Using computer simulations, we demonstrated how findings that challenged attractor-based networks in the past, such as mediated and asymmetric priming, are a natural consequence of our present model's dynamics. Puzzling results regarding backward priming were also given a straightforward explanation. In addition, the current model addresses some of the differences between semantic and associative relatedness and explains how these differences interact with stimulus onset asynchrony in priming experiments.},
author = {Lerner, Itamar and Bentin, Shlomo and Shriki, Oren},
doi = {10.1111/cogs.12007},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Lerner, Bentin, Shriki/2012/Spreading activation in an attractor network with latching dynamics automatic semantic priming revisited.pdf:pdf},
issn = {1551-6709},
journal = {Cognitive science},
keywords = {Humans,Memory,Neural Networks (Computer),Reaction Time,Repetition Priming,Semantics},
number = {8},
pages = {1339--82},
pmid = {23094718},
title = {{Spreading activation in an attractor network with latching dynamics: automatic semantic priming revisited}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3490422{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {36},
year = {2012}
}
@article{Kleyko2017a,
abstract = {This article proposes the use of Vector Symbolic Architectures for implementing Hierarchical Graph Neuron, an architecture for memorizing patterns of generic sensor stimuli. The adoption of a Vector Symbolic representation ensures a one-layered design for the approach, while maintaining the previously reported properties and performance characteristics of Hierarchical Graph Neuron, and also improving the noise resistance of the architecture. The proposed architecture enables a linear (with respect to the number of stored entries) time search for an arbitrary sub-pattern.},
archivePrefix = {arXiv},
arxivId = {1501.03784},
author = {Kleyko, Denis and Osipov, Evgeny and Senior, Alexander and Khan, Asad I. and Şekercioǧlu, Yaşar Ahmet},
doi = {10.1109/TNNLS.2016.2535338},
eprint = {1501.03784},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kleyko et al/2017/Holographic Graph Neuron A Bioinspired Architecture for Pattern Processing.pdf:pdf},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Associative memory (AM),holographic graph neuron (HoloGN),hyperdimensional computing,pattern recognition,vector symbolic architectures (VSAs)},
number = {6},
pages = {1250--1252},
title = {{Holographic Graph Neuron: A Bioinspired Architecture for Pattern Processing}},
volume = {28},
year = {2017}
}
@article{Hawkins2009,
abstract = {In this paper, we propose a mechanism which the neocortex may use to store sequences of patterns. Storing and recalling sequences are necessary for making predictions, recognizing time-based patterns and generating behaviour. Since these tasks are major functions of the neocortex, the ability to store and recall time-based sequences is probably a key attribute of many, if not all, cortical areas. Previously, we have proposed that the neocortex can be modelled as a hierarchy of memory regions, each of which learns and recalls sequences. This paper proposes how each region of neocortex might learn the sequences necessary for this theory. The basis of the proposal is that all the cells in a cortical column share bottom-up receptive field properties, but individual cells in a column learn to represent unique incidences of the bottom-up receptive field property within different sequences. We discuss the proposal, the biological constraints that led to it and some results modelling it.},
author = {Hawkins, Jeff and George, Dileep and Niemasik, Jamie},
doi = {10.1098/rstb.2008.0322},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Hawkins, George, Niemasik/2009/Sequence memory for prediction, inference and behaviour.pdf:pdf},
isbn = {0962-8436},
issn = {0962-8436},
journal = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},
keywords = {1,during,function of the brain,hierarchical temporal memory,prediction,prediction and sequence memory,prediction is a ubiquitous,sequence memory,state-splitting,variable-order markov model},
pages = {1203--1209},
pmid = {19528001},
title = {{Sequence memory for prediction, inference and behaviour}},
volume = {364},
year = {2009}
}
@article{Sergin2006,
author = {Сергин, А. В.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Сергин/2006/Компьютерная модель восприятия иерархия объемлющих сенсорных характеристик.pdf:pdf},
journal = {Нейроинформатика},
language = {russian},
number = {06},
pages = {189--195},
title = {{Компьютерная модель восприятия: иерархия объемлющих сенсорных характеристик}},
volume = {26},
year = {2006}
}
@article{Hao2007,
abstract = {Automatic categorization of documents into pre-defined topic hierarchies or taxonomies is a crucial step in knowledge and content management. Standard machine learning techniques like support vector machines and related large margin methods have been successfully applied for this task, albeit the fact is that they ignore the inter-class relationships. Unfortunately, in the context of document categorization, we face a large number of classes and a huge number of relevant features needed to distinguish between them. The computational cost of training a classifier for a problem of this size is prohibitive. It has also been observed that obtaining a classifier that discriminates between two groups of classes is much easier than distinguishing simultaneously among all classes. This has prompted substantial research in using hierarchical classifiers to address single multi-class problems. In this paper, we propose a novel hierarchical classification method that generalizes support vector machine learning that is based on the results of support vector clustering method, and are structured in a way that mirrors the class hierarchy. Compared to previous non-hierarchical SVM classifier and famous documents categorization systems, the proposed hierarchical SVM classification has a better improvement in classification accuracy in the standard Reuters corpus.},
author = {Hao, P and Chiang, J and Tu, Y},
doi = {10.1016/j.eswa.2006.06.009},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Hao, Chiang, Tu/2007/Hierarchically SVM classification based on support vector clustering method and its application to document categorization.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {document categorization,hierarchical classification,information retrieval,support vector clustering method,support vector machines},
number = {3},
pages = {627--635},
title = {{Hierarchically SVM classification based on support vector clustering method and its application to document categorization}},
volume = {33},
year = {2007}
}
@article{Wang2015a,
author = {Wang, Jinjun and Hou, Qiqi and Liu, Nan and Zhang, Shizhou},
doi = {10.1109/BigMM.2015.29},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Wang et al/2015/Model of Human Visual Cortex Inspired Computational Models for Visual Recognition.pdf:pdf},
isbn = {978-1-4799-8688-0},
journal = {2015 IEEE International Conference on Multimedia Big Data},
pages = {88--91},
title = {{Model of Human Visual Cortex Inspired Computational Models for Visual Recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7153860},
year = {2015}
}
@article{Dehaene2003,
abstract = {The subjective experience of perceiving visual stimuli is accompanied by objective neuronal activity patterns such as sustained activity in primary visual area (V1), amplification of perceptual processing, correlation across distant regions, joint parietal, frontal, and cingulate activation, gamma-band oscillations, and P300 waveform. We describe a neuronal network model that aims at explaining how those physiological parameters may cohere with conscious reports. The model proposes that the step of conscious perception, referred to as access awareness, is related to the entry of processed visual stimuli into a global brain state that links distant areas including the prefrontal cortex through reciprocal connections, and thus makes perceptual information reportable by multiple means. We use the model to simulate a classical psychological paradigm: the attentional blink. In addition to reproducing the main objective and subjective features of this paradigm, the model predicts an unique property of nonlinear transition from nonconscious processing to subjective perception. This all-or-none dynamics of conscious perception was verified behaviorally in human subjects.},
author = {Dehaene, Stanislas and Sergent, Claire and Changeux, Jean-Pierre},
doi = {10.1073/pnas.1332574100},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Dehaene, Sergent, Changeux/2003/A neuronal network model linking subjective reports and objective physiological data during conscious perception.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Adult,Animals,Attention,Attention: physiology,Cerebral Cortex,Cerebral Cortex: physiology,Consciousness,Consciousness: physiology,Haplorhini,Humans,Models,Neural Networks (Computer),Neurological,Neurons,Neurons: physiology,Psychological,Visual Perception,Visual Perception: physiology},
number = {14},
pages = {8520--5},
pmid = {12829797},
title = {{A neuronal network model linking subjective reports and objective physiological data during conscious perception}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=166261{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {100},
year = {2003}
}
@article{Finn2010a,
author = {Финн, В. К.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Финн/2010/Об определении эмпирических закономерностей посредством ДСМ - метода автоматического порождения гипотез.pdf:pdf},
journal = {Искусственный интеллект и принятие решений},
language = {russian},
number = {4},
pages = {41--48},
title = {{Об определении эмпирических закономерностей посредством ДСМ - метода автоматического порождения гипотез}},
year = {2010}
}
@inproceedings{Pongaksorn2009,
author = {Pongaksorn, Prachya and Rakthanmanon, Thanawin and Waiyamai, Kitsana},
booktitle = {Proceedings of the International Conference on Quality issues, measures of interstingness and evaluation of Data minig model (QIMIE)},
editor = {Garcia, Salvador and Luengo, Julian and Herrera, Francisco},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Pongaksorn, Rakthanmanon, Waiyamai/2009/DCR Discretization using Class Information to Reduce Number of Intervals.pdf:pdf},
keywords = {classification,continuous feature,data mining,discretization},
pages = {17--28},
publisher = {Springer-Verlag},
title = {{DCR : Discretization using Class Information to Reduce Number of Intervals}},
year = {2009}
}
@article{Sutton2010,
abstract = {Often we wish to predict a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling, combining the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This tutorial describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large scale CRFs. We do not assume previous knowledge of graphical modeling, so this tutorial is intended to be useful to practitioners in a wide variety of fields.},
archivePrefix = {arXiv},
arxivId = {1011.4088},
author = {Sutton, Charles and McCallum, Andrew},
eprint = {1011.4088},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Sutton, McCallum/2010/An Introduction to Conditional Random Fields.pdf:pdf},
journal = {arXiv},
month = {nov},
title = {{An Introduction to Conditional Random Fields}},
url = {http://arxiv.org/abs/1011.4088},
year = {2010}
}
@article{Finn2010b,
author = {Финн, В. К.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Финн/2010/Индуктивные методы Д . С . Милля в системах искусственного интеллекта. Часть II.pdf:pdf},
journal = {Искусственный интеллект и принятие решений},
language = {russian},
number = {4},
pages = {14--40},
title = {{Индуктивные методы Д . С . Милля в системах искусственного интеллекта. Часть II}},
year = {2010}
}
@techreport{Maloof1994,
author = {Maloof, Marcus A. and Michalski, Ryszard S.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Maloof, Michalski/1994/Learning Descriptions of 2D Blob-Like Shapes for Object Recognition in X-Ray Images An Initial Study.pdf:pdf},
keywords = {concept learning,machine learning,machine vision,shape recognition},
number = {August},
pages = {14},
title = {{Learning Descriptions of 2D Blob-Like Shapes for Object Recognition in X-Ray Images: An Initial Study}},
year = {1994}
}
@inproceedings{Morse,
author = {Morse, Anthony F and Ziemke, Tom},
booktitle = {Proceedings of the 31th Annual Conference of the Cognitive Sosciety},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Morse, Ziemke/2009/Action, Detection, and Perception A Computational Model of the Relation Between Movement and Orientation Selectivity in the Cerebral Cor.pdf:pdf},
keywords = {action,actionism,computational modeling,cortical,detection,embodiment,enaction,perception},
pages = {585--590},
title = {{Action, Detection, and Perception: A Computational Model of the Relation Between Movement and Orientation Selectivity in the Cerebral Cortex}},
year = {2009}
}
@article{Kleyko2016,
abstract = {This article presents a modification of the recently proposed Holographic Graph Neuron approach for memorizing patterns of generic sensor stimuli. The original approach represents patterns as dense binary vectors, where zeros and ones are equiprobable. The presented modification employs sparse binary distributed representations where the number of ones is less than zeros. Sparse representations are more biologically plausible because activities of real neurons are sparse. Performance was studied comparing approaches for different sizes of dimensionality.},
author = {Kleyko, Denis and Osipov, Evgeny and Rachkovskij, Dmitri A.},
doi = {10.1016/j.procs.2016.07.404},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kleyko, Osipov, Rachkovskij/2016/Modification of Holographic Graph Neuron Using Sparse Distributed Representations.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {hyperdimensional computing,sparse distributed representation},
pages = {39--45},
publisher = {The Author(s)},
title = {{Modification of Holographic Graph Neuron Using Sparse Distributed Representations}},
url = {http://dx.doi.org/10.1016/j.procs.2016.07.404},
volume = {88},
year = {2016}
}
@techreport{Michalski1991,
author = {Michalski, Ryszard S. and Kaufman, Kenneth A. and Wnek, J.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Michalski, Kaufman, Wnek/1991/The AQ Family of Learning Programs A Review of Recent Developments and an Exemplary Application.pdf:pdf},
institution = {George Mason University},
pages = {34},
title = {{The AQ Family of Learning Programs: A Review of Recent Developments and an Exemplary Application}},
year = {1991}
}
@article{AswaniKumar2015,
author = {{Aswani Kumar}, Ch. and Ishwarya, M.S. and Loo, Chu Kiong},
doi = {10.1016/j.bica.2015.04.003},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Aswani Kumar, Ishwarya, Loo/2015/Formal concept analysis approach to cognitive functionalities of bidirectional associative memory.pdf:pdf},
issn = {2212683X},
journal = {Biologically Inspired Cognitive Architectures},
keywords = {associative memories},
publisher = {Elsevier B.V.},
title = {{Formal concept analysis approach to cognitive functionalities of bidirectional associative memory}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S2212683X15000122},
year = {2015}
}
@inproceedings{Guerin2013,
author = {Guerin, Clement and Bertet, Karell and Revel, Arnaud},
booktitle = {"Concept Lattices and their Applications, La Rochelle : France (2013)},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Guerin, Bertet, Revel/2013/An efficient Java implementation of the immediate successors calculation.pdf:pdf},
keywords = {immediate successors,java implementation,lattice,loa},
pages = {81--92},
title = {{An efficient Java implementation of the immediate successors calculation}},
year = {2013}
}
@inproceedings{Molodchenkov2010,
address = {Рыбинск},
author = {Молдоченков, А. И.},
booktitle = {Теория и практика системного анализа: Труды I Всероссийской научной конференции молодых учёных с международным участием},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Молдоченков/2010/Применение AQ-алгоритма для персонификации лечебно-диагностических процессов.pdf:pdf},
language = {russian},
pages = {79--84},
publisher = {РГАТА им. Соловьева},
title = {{Применение AQ-алгоритма для персонификации лечебно-диагностических процессов}},
year = {2010}
}
@inproceedings{Michalski1969,
author = {Michalski, Ryszard S.},
booktitle = {Proceedings of the V International Symposium on Information Processing (FCIP 69)},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Michalski/1969/On the Quasi-Minimal Solution of the General Covering Problem.pdf:pdf},
pages = {125--128},
title = {{On the Quasi-Minimal Solution of the General Covering Problem}},
year = {1969}
}
@unpublished{Vorontcov2009b,
author = {Воронцов, К В},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Воронцов/2009/Методы машинного обучения, основанные на индукции правил (логические методы классификации ).pdf:pdf},
language = {russian},
pages = {41},
title = {{Методы машинного обучения, основанные на индукции правил (логические методы классификации )}},
year = {2009}
}
@techreport{Michalski1979,
author = {Michalski, Ryszard S.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Michalski/1979/Detection of Conceptual Patterns Through Inductive Inference.pdf:pdf},
pages = {45},
title = {{Detection of Conceptual Patterns Through Inductive Inference}},
year = {1979}
}
@inproceedings{Kerber1992,
author = {Kerber, Randy},
booktitle = {AAAI-92 Proceedings},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kerber/1992/ChiMerge Discretization of Numeric Attributes.pdf:pdf},
pages = {123--128},
title = {{ChiMerge: Discretization of Numeric Attributes}},
year = {1992}
}
@inproceedings{Mai2013,
abstract = {This paper presents a simple strategy for perception-action of robots in indoor environments using Hierarchical Temporal Memory which is the theory of modeling the rationale of the neocortex. The main idea of the present study is that the input of the HTM network is images of objects that robot perceives in environment, and the output of HTM network is action, such as moving along the wall, moving away, opening, and moving forward, etc. Experiments results show that the proposed method can be applied for robot learning and navigation because it imitates humans' thinking mode to process the information it receives.},
author = {Mai, Xiaochun and Zhang, Xinzheng and Jin, Yichen and Yang, Yi and Zhang, Jianfen},
booktitle = {Proceeding of the IEEE International Conference on Robotics and Biomimetics (ROBIO)},
doi = {10.1109/ROBIO.2013.6739722},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Mai et al/2013/Simple Perception-Action Strategy Based on Hierarchical Temporal Memory.pdf:pdf},
isbn = {9781479927449},
pages = {1759--1764},
title = {{Simple Perception-Action Strategy Based on Hierarchical Temporal Memory}},
year = {2013}
}
@unpublished{Kuznetcov2007,
author = {Кузнецов, С. О.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Кузнецов/2007/Теория решеток замкнутых множеств.pdf:pdf},
language = {russian},
pages = {152},
title = {{Теория решеток замкнутых множеств}},
year = {2007}
}
@incollection{Martis2014a,
abstract = {Successful application of machine learning in healthcare requires accuracy, transparency, acceptability, ability to deal with complex data, ability to deal with background knowledge, efficiency, and exportability. Rule learning is known to satisfy the above criteria. This chapter introduces rule learning in healthcare, presents very expressive attributional rules, briefly describes the AQ21 rule learning system, and discusses three application areas in healthcare and health services research.},
author = {Wojtusiak, Janusz},
booktitle = {Machine Learning in Healthcare Informatics},
doi = {10.1007/978-3-642-40017-9},
editor = {Dua, Sumeet and Acharya, U. Rajendra and Dua, Prerna},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Wojtusiak/2014/Rule Learning in Healthcare and Health Services Research.pdf:pdf},
isbn = {978-3-642-40016-2},
keywords = {aq21 system health services,billing data,research aggregated data healthcare,rule learning attributional calculus},
pages = {131--145},
series = {Intelligent Systems Reference Library},
title = {{Rule Learning in Healthcare and Health Services Research}},
url = {http://link.springer.com/10.1007/978-3-642-40017-9},
year = {2014}
}
@article{Pearl1999,
author = {Greenland, Sander and Pearl, Judea and Robins, James M.},
doi = {10.1214/ss/1009211805},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Greenland, Pearl, Robins/1999/Confounding and Collapsibility in Causal Inference.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,bias,causation,collapsibility,con-,confounding,exchangeability,much of epidemiologic and,observational studies,odds ratio,rela-,risk assessment,s paradox,simpson,social science research,tingency tables,tive risk},
number = {1},
pages = {29--46},
title = {{Confounding and Collapsibility in Causal Inference}},
url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.ss/1009211805/},
volume = {14},
year = {1999}
}
@article{Rebhan2011,
abstract = {Humans selectively process and store details about the vicinity based on their knowledge about the scene, the world and their current task. In doing so, only those pieces of information are extracted from the visual scene that is required for solving a given task. In this paper, we present a flexible system architecture along with a control mechanism that allows for a task-dependent representation of a visual scene. Contrary to existing approaches, our system is able to acquire information selectively according to the demands of the given task and based on the system's knowledge. The proposed control mechanism decides which properties need to be extracted and how the independent processing modules should be combined, based on the knowledge stored in the system's long-term memory. Additionally, it ensures that algorithmic dependencies between processing modules are resolved automatically, utilizing procedural knowledge which is also stored in the long-term memory. By evaluating a proof-of-concept implementation on a real-world table scene, we show that, while solving the given task, the amount of data processed and stored by the system is considerably lower compared to processing regimes used in state-of-the-art systems. Furthermore, our system only acquires and stores the minimal set of information that is relevant for solving the given task.},
author = {Rebhan, Sven and Eggert, Julian},
doi = {10.1007/s12559-010-9077-9},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Rebhan, Eggert/2011/Dynamic, Task-Related and Demand-Driven Scene Representation.pdf:pdf},
issn = {1866-9956},
journal = {Cognitive computation},
keywords = {30,attention {\'{a}} visual search,carl-legien-str,control {\'{a}},eggert,honda research institute europe,rebhan,s,scene representation {\'{a}} cognitive,{\'{a}} j},
number = {1},
pages = {124--145},
pmid = {21475691},
title = {{Dynamic, Task-Related and Demand-Driven Scene Representation}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3059823{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {3},
year = {2011}
}
@unpublished{Romanenko2014,
abstract = {В работе рассматривается применение аппарата условных случайных (CRF) полей к двум задачам обработки естественного языка: выделению временн´ ых выражений и нормализации цифровой записи числительных. Для решения этих задач применяется линейная модель CRF. Также в работе предлагается способ модификации линейной модели CRF для задач разметки последовательностей некоторого специального вида. Прове- дено сравнение модифицированной и классической моделей на примере задачи нормализации цифровой записи числительного.},
author = {Романенко, Александр Александрович},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Романенко/2014/Применение условных случайных полей в задачах обработки текстов на естественном языке.pdf:pdf},
language = {russian},
pages = {27},
title = {{Применение условных случайных полей в задачах обработки текстов на естественном языке}},
year = {2014}
}
@techreport{Michalski2007a,
author = {Michalski, Ryszard S. and Wojtusiak, Janusz},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Michalski, Wojtusiak/2007/Semantic and Syntactic Attribute Types in AQ Learning.pdf:pdf},
institution = {George Mason University},
keywords = {AQ learning,Attribute types,Machine learning,Na},
pages = {15},
title = {{Semantic and Syntactic Attribute Types in AQ Learning}},
year = {2007}
}
@article{Garalevicius2007,
abstract = {This paper explores an inferential system for recognizing visual patterns. The system is inspired by a recent memory- prediction theory and models the high-level architecture of the human neocortex. The paper describes the hierarchical architecture and recognition performance of this Bayesian model. A number of possibilities are analyzed for bringing the model closer to the theory, making it uniform, scalable, less biased and able to learn a larger variety of images and their transformations. The effect of these modifications on recognition accuracy is explored. We identify and discuss a number of both conceptual and practical challenges to the Bayesian approach as well as missing details in the theory that are needed to design a scalable and universal model.},
author = {Garalevicius, Saulius J},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Garalevicius/2007/Memory-Prediction Framework for Pattern Recognition Performance and Suitability of the Bayesian Model of Visual Cortex.pdf:pdf},
journal = {LAIRS Conference, Florida},
keywords = {HTM,Hierarchical Temporal Memory,Jeff Hawkins,MPF,Memory-Prediction Framework,Numenta,On Intelligence,Saulius Garalevicius,artificial intelligence,belief propagation,belief revision,computer vision,cortical modeling,lista{\_}filtrada,neocortex,neural networks,neuroscience,pattern recognition},
pages = {92--97.},
title = {{Memory-Prediction Framework for Pattern Recognition: Performance and Suitability of the Bayesian Model of Visual Cortex}},
url = {https://www.aaai.org/Papers/FLAIRS/2007/Flairs07-018.pdf{\%}5Cnhttp://www.phillylac.org/prediction/Memory-Prediction paper.pdf{\%}5Cnhttp://www.phillylac.org/prediction/Memory-Prediction{\%}5Cnpaper.pdf},
year = {2007}
}
@techreport{Wnek1991,
author = {Wnek, J. and Michalski, Ryszard S.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Wnek, Michalski/1991/Hypothesis-Driven Constructive Induction in AQ17 a Method and Experiments.pdf:pdf},
institution = {George Mason University},
pages = {17},
title = {{Hypothesis-Driven Constructive Induction in AQ17: a Method and Experiments}},
year = {1991}
}
@article{Pearl2000,
author = {Pearl, Judea},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Pearl/2000/Causality Second Edition.pdf:pdf},
isbn = {0521773628},
pages = {386},
title = {{Causality Second Edition}},
year = {2000}
}
@phdthesis{Bolotova2013,
author = {Болотова, Ю. А.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Болотова/2013/Алгоритмы обработки и анализа изображений иерархической временной сетью.pdf:pdf},
language = {russian},
pages = {24},
school = {Томский Государственный университет},
title = {{Алгоритмы обработки и анализа изображений иерархической временной сетью}},
year = {2013}
}
@inproceedings{Dietterich1983,
author = {Dietterich, Tom and Michalski, Ryszard S.},
booktitle = {Proceedings of the International Machine Learning Workshop},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Dietterich, Michalski/1983/Discovering Patterns in Seuences of Objects.pdf:pdf},
pages = {41--57},
title = {{Discovering Patterns in Seuences of Objects}},
year = {1983}
}
@unpublished{Cui2015,
abstract = {The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, a recent study proposed hierarchical temporal memory (HTM) sequence memory as a theoretical framework for sequence learning in the cortex. In this paper, we analyze properties of HTM sequence memory and apply it to various sequence learning and prediction problems. We show the model is able to continuously learn a large number of variable-order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory and other sequence learning algorithms, including the autoregressive integrated moving average (ARIMA) model and long short-term memory (LSTM), on sequence prediction problems with both artificial and real-world data. The HTM model not only achieves comparable or better accuracy than state-of-the-art algorithms, but also exhibits a set of properties that is critical for sequence learning. These properties include continuous online learning, the ability to handle multiple predictions and branching sequences, robustness to sensor noise and fault tolerance, and good performance without task-specific hyper-parameters tuning. Therefore the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem, but is also applicable to a wide range of real-world problems such as discrete and continuous sequence prediction, anomaly detection, and sequence classification.},
archivePrefix = {arXiv},
arxivId = {1512.05463},
author = {Cui, Yuwei and Surpur, Chetan and Ahmad, Subutai and Hawkins, Jeff},
eprint = {1512.05463},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cui et al/2015/Continuous online sequence learning with an unsupervised neural network model.pdf:pdf},
pages = {12},
title = {{Continuous online sequence learning with an unsupervised neural network model}},
year = {2015}
}
@article{Alcala-fdez2010a,
author = {Alcala-fdez, Jesus},
doi = {10.3233/FI-2010-213},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Alcala-fdez/2010/Analysis of the Effectiveness of the Genetic Algorithms based on Extraction of Association Rules.pdf:pdf},
journal = {Knowledge Creation Diffusion Utilization},
pages = {1--14},
title = {{Analysis of the Effectiveness of the Genetic Algorithms based on Extraction of Association Rules}},
volume = {98},
year = {2010}
}
@article{CatenacciVolpi2014,
abstract = {We propose a computational model of perceptual categorization that fuses elements of grounded and sensorimotor theories of cognition with dynamic models of decision-making. We assume that category information consists in anticipated patterns of agent-environment interactions that can be elicited through overt or covert (simulated) eye movements, object manipulation, etc. This information is firstly encoded when category information is acquired, and then re-enacted during perceptual categorization. The perceptual categorization consists in a dynamic competition between attractors that encode the sensorimotor patterns typical of each category; action prediction success counts as "evidence" for a given category and contributes to falling into the corresponding attractor. The evidence accumulation process is guided by an active perception loop, and the active exploration of objects (e.g., visual exploration) aims at eliciting expected sensorimotor patterns that count as evidence for the object category. We present a computational model incorporating these elements and describing action prediction, active perception, and attractor dynamics as key elements of perceptual categorizations. We test the model in three simulated perceptual categorization tasks, and we discuss its relevance for grounded and sensorimotor theories of cognition. {\textcopyright} 2014 Elsevier Ltd.},
author = {{Catenacci Volpi}, Nicola and Quinton, Jean Charles and Pezzulo, Giovanni},
doi = {10.1016/j.neunet.2014.06.008},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Catenacci Volpi, Quinton, Pezzulo/2014/How active perception and attractor dynamics shape perceptual categorization A computational model.pdf:pdf},
issn = {18792782},
journal = {Neural Networks},
keywords = {Active vision,Dynamic choice,Hopfield networks,Perceptual categorization,Prediction},
pages = {1--16},
pmid = {25105744},
publisher = {Elsevier Ltd},
title = {{How active perception and attractor dynamics shape perceptual categorization: A computational model}},
url = {http://dx.doi.org/10.1016/j.neunet.2014.06.008},
volume = {60},
year = {2014}
}
@article{Chiang2003,
abstract = {In this paper, the support vector clustering is ex- tended to an adaptive cell growing model which maps data points to a high dimensional feature space through a desired kernel function. This generalized model is called multiple spheres sup- port vector clustering, which essentially identifies dense regions in the original space by finding their corresponding spheres with minimal radius in the feature space. A multisphere clustering algorithm based on adaptive cluster cell growing method is devel- oped, whereby it is possible to obtain the grade of memberships, as well as cluster prototypes in partition. The effectiveness of the proposed algorithm is demonstrated for the problem of arbitrary cluster shapes and for prototype identification in an actual application to a handwritten digit data set.},
author = {Chiang, Jung-Hsien and Hao, Pei-Yi},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Chiang, Hao/2003/A new kernelbased fuzzy clustering approach support vector clustering with cell growing.pdf:pdf},
journal = {IEEE Transactions on Fuzzy Systems},
keywords = {Fuzzy clustering,kernel-based learning,quadratic programming,support vector machine},
number = {4},
pages = {518--527},
title = {{A new kernelbased fuzzy clustering approach support vector clustering with cell growing}},
volume = {11},
year = {2003}
}
@incollection{Choi2006,
abstract = {In this paper, we present a fast algorithm for constructing a concept (Galois) lattice of a binary relation, including computing all concepts and their lattice order. We also present two efficient variants of the algorithm, one for computing all concepts only, and one for constructing a frequent closed itemset lattice. The running time of our algorithms depends on the lattice structure and is faster than all other existing algorithms for these problems.},
archivePrefix = {arXiv},
arxivId = {cs/0602069},
author = {Choi, Vicky},
booktitle = {Clustering Challenges in Biological Networks},
eprint = {0602069},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Choi/2006/Faster Algorithms for Constructing a Concept (Galois) Lattice.pdf:pdf},
pages = {15},
primaryClass = {cs},
title = {{Faster Algorithms for Constructing a Concept (Galois) Lattice}},
url = {http://arxiv.org/abs/cs/0602069},
year = {2006}
}
@article{Wettschereck1997,
author = {Wettschereck, Dietrich and Aha, David W. and Mohri, Takao},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Wettschereck, Aha, Mohri/1997/A Review and Empirical Evaluation of Feature Weighting Methods for a Class of Lazy Learning Algorithms.pdf:pdf},
journal = {Artificial Intelligence Review},
keywords = {empirical comparison,feature weights,k-nearest neighbor,lazy learning},
pages = {273--314},
title = {{A Review and Empirical Evaluation of Feature Weighting Methods for a Class of Lazy Learning Algorithms}},
volume = {11},
year = {1997}
}
@article{Finn2010c,
author = {Финн, В. К.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Финн/2010/Индуктивные методы Д . С . Милля в системах искусственного интеллекта. Часть I.pdf:pdf},
journal = {Искусственный интеллект и принятие решений},
language = {russian},
number = {3},
pages = {3--21},
title = {{Индуктивные методы Д . С . Милля в системах искусственного интеллекта. Часть I}},
year = {2010}
}
@article{Rehder2001,
author = {Rehder, Bob and Hastie, Reid},
doi = {10.1037/0096-3445.130.3.323},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Rehder, Hastie/2001/Causal knowledge and categories The effects of causal beliefs on categorization, induction, and similarity.pdf:pdf},
issn = {1939-2222},
journal = {Journal of Experimental Psychology: General},
number = {3},
pages = {323--360},
title = {{Causal knowledge and categories: The effects of causal beliefs on categorization, induction, and similarity.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-3445.130.3.323},
volume = {130},
year = {2001}
}
@article{Bolotova2011,
author = {Болотова, Ю. А. and Спицын, В. Г. and Фомин, А. Э.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Болотова, Спицын, Фомин/2011/Применение модели иерархической временной памяти в распознавании изображений.pdf:pdf},
journal = {Известия Томского политехнического университета},
keywords = {evolutionary algorithm,hierarchical temporal memory model,in vision,pattern recognition,time and hierarchical components,в пространстве,задачи распознавания образов,понимания речи и ориентации,чтения текстов,яв},
language = {russian},
number = {5},
pages = {60--63},
title = {{Применение модели иерархической временной памяти в распознавании изображений}},
volume = {318},
year = {2011}
}
@techreport{Kaufman1999,
author = {Kaufman, Kenneth A. and Michalski, Ryszard S.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kaufman, Michalski/1999/Learning in an inconsistent world.pdf:pdf},
institution = {George Mason University},
pages = {19},
title = {{Learning in an inconsistent world}},
year = {1999}
}
@book{Goos1977,
editor = {Goos, G. and Hartmanis, J.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/1977/Lecture Notes in Computer Science. Fundamentals of Computation Theory.pdf:pdf},
isbn = {9783662391617},
publisher = {Springer-Verlag},
title = {{Lecture Notes in Computer Science. Fundamentals of Computation Theory}},
year = {1977}
}
@article{Ignatov2013,
author = {Ignatov, Dmitry I. and Kuznetsov, Sergei O. and Poelmans, Jonas and Zhukov, Leonid E.},
doi = {10.1080/03081079.2013.798899},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Ignatov et al/2013/Can triconcepts become triclusters.pdf:pdf},
issn = {0308-1079},
journal = {International Journal of General Systems},
number = {6},
pages = {572--593},
title = {{Can triconcepts become triclusters?}},
url = {http://www.tandfonline.com/doi/abs/10.1080/03081079.2013.798899},
volume = {42},
year = {2013}
}
@inproceedings{Ignatov2011,
author = {Игнатов, Д.И. and Кузнецов, С.О. and Пульманс, Й.},
booktitle = {Математические методы распознавания образов: 15-я Всероссийская конференция},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Игнатов, Кузнецов, Пульманс/2011/Разработка данных систем совместного пользования ресурсами от трипонятий к трикластерам.pdf:pdf},
language = {russian},
pages = {258--261},
title = {{Разработка данных систем совместного пользования ресурсами: от трипонятий к трикластерам}},
year = {2011}
}
@article{Kording2007,
abstract = {Perceptual events derive their significance to an animal from their meaning about the world, that is from the information they carry about their causes. The brain should thus be able to efficiently infer the causes underlying our sensory events. Here we use multisensory cue combination to study causal inference in perception. We formulate an ideal-observer model that infers whether two sensory cues originate from the same location and that also estimates their location(s). This model accurately predicts the nonlinear integration of cues by human subjects in two auditory-visual localization tasks. The results show that indeed humans can efficiently infer the causal structure as well as the location of causes. By combining insights from the study of causal inference with the ideal-observer approach to sensory cue combination, we show that the capacity to infer causal structure is not limited to conscious, high-level cognition; it is also performed continually and effortlessly in perception.},
author = {K{\"{o}}rding, Konrad P. and Beierholm, Ulrik and Ma, Wei Ji and Quartz, Steven and Tenenbaum, Joshua B. and Shams, Ladan},
doi = {10.1371/journal.pone.0000943},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/K{\"{o}}rding et al/2007/Causal inference in multisensory perception.pdf:pdf},
issn = {1932-6203},
journal = {PloS ONE},
keywords = {Afferent,Afferent: physiology,Auditory Perception,Auditory Perception: physiology,Bayes Theorem,Brain,Brain: physiology,Cues,Humans,Neurons,Space Perception,Space Perception: physiology,Task Performance and Analysis,Visual Perception,Visual Perception: physiology},
number = {9},
pages = {e943},
pmid = {17895984},
title = {{Causal inference in multisensory perception}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1978520{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {2},
year = {2007}
}
@article{Bitzer2015,
abstract = {Even for simple perceptual decisions, the mechanisms that the brain employs are still under debate. Although current consensus states that the brain accumulates evidence extracted from noisy sensory information, open questions remain about how this simple model relates to other perceptual phenomena such as flexibility in decisions, decision-dependent modulation of sensory gain, or confidence about a decision. We propose a novel approach of how perceptual decisions are made by combining two influential formalisms into a new model. Specifically, we embed an attractor model of decision making into a probabilistic framework that models decision making as Bayesian inference. We show that the new model can explain decision making behaviour by fitting it to experimental data. In addition, the new model combines for the first time three important features: First, the model can update decisions in response to switches in the underlying stimulus. Second, the probabilistic formulation accounts for top-down effects that may explain recent experimental findings of decision-related gain modulation of sensory neurons. Finally, the model computes an explicit measure of confidence which we relate to recent experimental evidence for confidence computations in perceptual decision tasks.},
author = {Bitzer, Sebastian and Bruineberg, Jelle and Kiebel, Stefan J},
doi = {10.1371/journal.pcbi.1004442},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Bitzer, Bruineberg, Kiebel/2015/A Bayesian Attractor Model for Perceptual Decision Making.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
number = {8},
pages = {e1004442},
pmid = {26267143},
publisher = {Public Library of Science},
title = {{A Bayesian Attractor Model for Perceptual Decision Making}},
url = {http://dx.doi.org/10.1371/journal.pcbi.1004442},
volume = {11},
year = {2015}
}
@inproceedings{Andrews2009,
author = {Andrews, Simon},
booktitle = {International Conference on Conceptual Structures (ICCS)},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Andrews/2009/In-Close , a fast algorithm for computing formal concepts.pdf:pdf},
title = {{In-Close , a fast algorithm for computing formal concepts}},
year = {2009}
}
@book{Bollen1989,
abstract = {In multiple regression or Anova we learn that the regression coefficients or the error variance estimates derive form the minimization of the sum of squeared differneces of the predicted and observed dependent variable of each case. Residual analyses may display discrepancies between fitted and observed values for every member of the sample. SEM is based on covariances. Reproduce the sample covariances. Lantent variables Examples: biological species (not always directly measurable), deseases in medicin (observe symptoms), time and space (see reletivity theory). P{\aa} omr{\aa}der hvor lite er kjent kan EFA v{\ae}re verdifull og foresl{\aa} unerliggende m{\o}nstre i data. Hvis derimot hypoteser foreligger om underliggende m{\o}nstre, kan EFA frustrate tests (viser her til eksempel med {\aa}tte ledd for en politisk holdning, fire m{\aa}lt p{\aa} ett tidspunkt, fire p{\aa} et annet.) Problem med EFA: sett med ledd for holdning p{\aa} to tidspunkt vil gi {\'{e}}n faktor... Men glidende overgang EFA til CFA: antall faktorer kan v{\ae}re bestemt p{\aa} forh{\aa}nd i EFA. Men EFA brukes for {\aa} identifisere antall faktorer. Selv n{\aa}r antall faktorer er definert p{\aa} forh{\aa}nd i EFA, har man tvetydighet (s. 230f). Begge faktorer vil korrelere med alle ledd (ogs{\aa} n{\aa}r ledd m{\aa}lt p{\aa} ulike tidspunkter, som i eksempel her). 1. EFA tillater ikke at noen faktorladniner er satt til 0. 2. EFA tillater ikke at m{\aa}lefeil er korrelert (viktig hvis for eksempel ved gjentatte m{\aa}liner eller fordi m{\aa}l kommer fra samme kilde). 3. Ved bruk av flere faktorer forutsetter EFA at alle faktorer enten er korrelert eller ikke er korrelert. Hvor realistisk er det?},
author = {Bollen, Kenneth A.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Bollen/1989/Structural equations with latent variables.pdf:pdf},
pages = {528},
publisher = {Wiley},
title = {{Structural equations with latent variables}},
year = {1989}
}
@article{Ma2012,
abstract = {Probability has played a central role in models of perception for more than a century, but a look at probabilistic concepts in the literature raises many questions. Is being Bayesian the same as being optimal? Are recent Bayesian models fundamentally different from classic signal detection theory models? Do findings of near-optimal inference provide evidence that neurons compute with probability distributions? This review aims to disentangle these concepts and to classify empirical evidence accordingly.},
author = {Ma, Wei Ji},
doi = {10.1016/j.tics.2012.08.010},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Ma/2012/Organizing probabilistic models of perception.pdf:pdf},
issn = {1879-307X},
journal = {Trends in cognitive sciences},
keywords = {Algorithms,Bayes Theorem,Decision Making,Humans,Models,Perception,Perception: physiology,Psychological,Signal Detection,Statistical},
number = {10},
pages = {511--8},
pmid = {22981359},
publisher = {Elsevier Ltd},
title = {{Organizing probabilistic models of perception}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22981359},
volume = {16},
year = {2012}
}
@article{Ghavamzadeh2015,
author = {Ghavamzadeh, Mohammed and Mannor, Shie and Pineau, Joelle and Tamar, Aviv},
doi = {10.1561/2200000049},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Ghavamzadeh et al/2015/Bayesian Reinforcement Learning A Survey.pdf:pdf},
isbn = {2200000049},
issn = {1935-8237},
journal = {Foundations and Trends in Machine Learning},
number = {5-6},
pages = {359--483},
title = {{Bayesian Reinforcement Learning: A Survey}},
url = {http://www.nowpublishers.com/article/Details/MAL-049},
volume = {8},
year = {2015}
}
@book{Barber2011,
abstract = {Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Barber, David},
doi = {10.1109/MSP.2013.2259911},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Barber/2012/Bayesian Reasoning and Machine Learning.pdf:pdf;:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Barber/2012/Bayesian Reasoning and Machine Learning.pdf:pdf;:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Barber/2012/Bayesian Reasoning and Machine Learning(2).pdf:pdf},
isbn = {9780511804779},
issn = {1053-5888},
pages = {726},
pmid = {16931139},
publisher = {Cambridge University Press},
title = {{Bayesian Reasoning and Machine Learning}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511804779 http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/181115.pdf},
year = {2012}
}
@incollection{Merwe2004,
author = {Merwe, Dean Van Der and Obiedkov, Sergei and Kourie, Derrick},
booktitle = {Concept Lattices},
editor = {Eklund, Peter},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Merwe, Obiedkov, Kourie/2004/AddIntent A new incremental algorithm for constructing concept lattices.pdf:pdf},
isbn = {978-3-540-21043-6, 978-3-540-24651-0},
issn = {03029743},
pages = {372--385},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{AddIntent: A new incremental algorithm for constructing concept lattices}},
url = {http://www.springerlink.com/index/10.1007/978-3-540-24651-0{\_}31{\%}5Cnhttp://www.springerlink.com/index/6r03tfahg6y9wt1r.pdf},
year = {2004}
}
@article{Rachkovskij2013,
abstract = {We present a new cognitive architecture named Associative-Projective Neural Networks (APNNs). APNNs have a multi-module, multi-level, and multi-modal design that works with an original scheme of sparse binary distributed representations to construct world models of varied complexity required for both task-specific and more general cognitive modeling. APNNs provide scalability and flexibility due to a number of design features. Internal representations of APNNs are sparse binary vectors of fixed dimensionality for items of various complexity and generality. Representations of input scalars, vectors, or compositional relational structures are constructed on-the-fly, so that similar items produce representations similar in terms of vector dot-products. Thus, for example, similarity of relational structures (taking into account similarity of their components, their grouping and order) can be estimated by dot-products of their representations, without the need to follow edges or to match vertices of underlying graphs. Decoding distributed representations through the input representations is also possible. Storage, retrieval, and decoding of distributed representations are implemented by efficient auto-associative memories; using distributed memories based on the idea of Hebb's cell assemblies additionally provides a natural tool for emergence of generalization hierarchies. In addition, we consider how APNNs account for representation grounding, deal with recent challenges for distributed representations, and present some open problems. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Rachkovskij, Dmitri A. and Kussul, Ernst M. and Baidyk, Tatiana N.},
doi = {10.1016/j.bica.2012.09.004},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Rachkovskij, Kussul, Baidyk/2013/Building a world model with structure-sensitive sparse binary distributed representations.pdf:pdf},
issn = {2212683X},
journal = {Biologically Inspired Cognitive Architectures},
keywords = {Associative memory,Associative-Projective Neural Networks,Binding,Distributed representations,Hierarchical relational structures,Vector symbolic architectures},
month = {jan},
pages = {64--86},
title = {{Building a world model with structure-sensitive sparse binary distributed representations}},
url = {http://dx.doi.org/10.1016/j.bica.2012.09.004 http://linkinghub.elsevier.com/retrieve/pii/S2212683X12000552},
volume = {3},
year = {2013}
}
@inproceedings{Zou2011,
abstract = {Natural scenes in a video stream contain rich collections of visual transformations. In this paper, a generic neural network is built to learn visual invariance from videos in an unsupervised manner. We use temporal coherence to learn both visual transformations and features with complex invariances. Without fine tuning with labels, our invariant features are superior for classifying object in still images. The learned features out-perform features learned with sparsity in vision benchmarks Caltech-101, STL-10 and COIL-100.},
author = {Zou, Will Y. and Ng, Andrew Y. and Yu, Kai},
booktitle = {Neural Information Processing Systems (NIPS) Workshop on Deep Learning and Unsupervised Feature Learning},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Zou, Ng, Yu/2011/Unsupervised learning of visual invariance with temporal coherence.pdf:pdf},
keywords = {Temporal Coherence},
title = {{Unsupervised learning of visual invariance with temporal coherence}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Unsupervised+learning+of+visual+invariance+with+temporal+coherence{\#}3},
year = {2011}
}
@article{Pollack1990,
author = {Pollack, Jordan B.},
doi = {10.1016/0004-3702(90)90005-K},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Pollack/1990/Recursive distributed representations.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
number = {1-2},
pages = {77--105},
title = {{Recursive distributed representations}},
url = {http://linkinghub.elsevier.com/retrieve/pii/000437029090005K},
volume = {46},
year = {1990}
}
@article{Miao2000,
author = {Miao, Yuan and Liu, Zhi-qiang},
doi = {10.1109/91.824780},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Miao, Liu/2000/On causal inference in fuzzy cognitive maps.pdf:pdf},
issn = {10636706},
journal = {IEEE Transactions on Fuzzy Systems},
number = {1},
pages = {107--119},
title = {{On causal inference in fuzzy cognitive maps}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=824780},
volume = {8},
year = {2000}
}
@article{Serre2007,
abstract = {We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex.},
author = {Serre, Thomas and Wolf, Lior and Bileschi, Stanley and Riesenhuber, Maximilian and Poggio, Tomaso},
doi = {10.1109/TPAMI.2007.56},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Serre et al/2007/Robust object recognition with cortex-like mechanisms.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Model,Neural network,Object recognition,Scene understanding,Visual cortex},
number = {3},
pages = {411--426},
pmid = {17224612},
title = {{Robust object recognition with cortex-like mechanisms}},
volume = {29},
year = {2007}
}
@article{Andrychowicz2016a,
abstract = {In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM). It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory. We show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples. In particular, it learns to sort n numbers in time O(n log n) and generalizes well to input sequences much longer than the ones seen during the training. We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue.},
archivePrefix = {arXiv},
arxivId = {1602.03218},
author = {Andrychowicz, Marcin and Kurach, Karol},
eprint = {1602.03218},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Andrychowicz, Kurach/2016/Learning Efficient Algorithms with Hierarchical Attentive Memory.pdf:pdf},
journal = {International Conference on Machine Learning (ICML)},
keywords = {ICML,attention,machine learning},
title = {{Learning Efficient Algorithms with Hierarchical Attentive Memory}},
url = {http://arxiv.org/abs/1602.03218},
year = {2016}
}
@article{Schein2015,
abstract = {We present a Bayesian tensor factorization model for inferring latent group structures from dynamic pairwise interaction patterns. For decades, political scientists have collected and analyzed records of the form “country i took action a toward country j at time t”—known as dyadic events—in order to form and test theories of international relations. We represent these event data as a tensor of counts and develop Bayesian Poisson tensor factorization to infer a lowdimensional, interpretable representation of their salient patterns. We demonstrate that our model's predictive performance is better than that of standard non-negative tensor factorization methods. We also provide a comparison of our variational updates to their maximum likelihood counterparts. In doing so, we identify a better way to form point estimates of the latent factors than that typically used in Bayesian Poisson matrix factorization. Finally, we showcase our model as an exploratory analysis tool for political scientists. We show that the inferred latent factor matrices capture interpretable multilateral relations that both conform to and inform our knowledge of international affairs.},
archivePrefix = {arXiv},
arxivId = {1506.03493},
author = {Schein, Aaron and Paisley, John and Blei, David M and Wallach, Hanna},
doi = {10.1145/2783258.2783414},
eprint = {1506.03493},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Schein et al/2015/Bayesian Poisson Tensor Factorization for Inferring Multilateral Relations from Sparse Dyadic Event Counts.pdf:pdf},
isbn = {9781450336642},
issn = {9781450336642},
journal = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {all or part of,bayesian inference,dyadic data,international relations,or hard copies of,permission to make digital,poisson tensor factorization,this work for per-},
pages = {1045--1054},
title = {{Bayesian Poisson Tensor Factorization for Inferring Multilateral Relations from Sparse Dyadic Event Counts}},
url = {http://www.cs.columbia.edu/{~}blei/papers/ScheinPaisleyBleiWallach2015.pdf},
year = {2015}
}
@incollection{Michalski2001,
author = {Michalski, Ryszard S. and Kaufman, Kenneth A.},
booktitle = {Machine Learning and Its Applications},
editor = {Paliouras, Georgios and Karkaletsis, Vangelis and Spyropoulos, Constantine D.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Michalski, Kaufman/2001/Learning Patterns in Noisy Data The AQ Approach.pdf:pdf},
pages = {22--38},
publisher = {Springer-Verlag},
title = {{Learning Patterns in Noisy Data : The AQ Approach}},
year = {2001}
}
@book{Vorontcov2011b,
author = {Воронцов, К. В.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Воронцов/2011/Математические методы обучения по прецедентам (теория обучения машин).pdf:pdf},
language = {russian},
pages = {141},
title = {{Математические методы обучения по прецедентам (теория обучения машин)}},
year = {2011}
}
@techreport{Furnkranz1996,
author = {Furnkranz, Johannes},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Furnkranz/1996/Separate-and-Conquer Rule Learning.pdf:pdf},
institution = {Austrian Research Institute for Artificial Intelligence},
pages = {1--46},
title = {{Separate-and-Conquer Rule Learning}},
year = {1996}
}
@article{Cervone2010,
author = {Cervone, Guido and Franzese, Pasquale and Keesee, Allen P. K.},
doi = {10.1002/wics.78},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cervone, Franzese, Keesee/2010/Algorithm quasi-optimal (AQ) learning.pdf:pdf},
issn = {19395108},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
number = {2},
pages = {218--236},
title = {{Algorithm quasi-optimal (AQ) learning}},
url = {http://doi.wiley.com/10.1002/wics.78},
volume = {2},
year = {2010}
}
@article{Yu2015,
author = {Yu, Qiang and Member, Student and Yan, Rui and Tang, Huajin and Tan, Kay Chen and Li, Haizhou},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Yu et al/2015/A Spiking Neural Network System for Robust Sequence Recognition.pdf:pdf},
journal = {IEEE transactions on neural networks and learning systems},
pages = {1--15},
title = {{A Spiking Neural Network System for Robust Sequence Recognition}},
year = {2015}
}
@inproceedings{Kononenko1995,
author = {Kononenko, Igor},
booktitle = {IJCAI'95 Proceedings of the 14th international joint conference on Artificial intelligence - Volume 2},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kononenko/1995/On Biases in Estimating Multi-Valued Attributes.pdf:pdf},
pages = {1034--1040},
title = {{On Biases in Estimating Multi-Valued Attributes}},
year = {1995}
}
@inproceedings{Drewitz2011,
author = {Drewitz, Uwe and Brandenburg, Stefan},
booktitle = {Proceedings of the 11th International Conference on Cognitive Modeling},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Drewitz, Brandenburg/2011/Memory and Contextual Change in Causal Learning.pdf:pdf},
keywords = {contextual,inference,knowledge},
pages = {265--270},
title = {{Memory and Contextual Change in Causal Learning}},
year = {2011}
}
@inproceedings{Suri2003a,
abstract = {A novel concept is proposed that uses active information gathering for recognizing objects on images. This novel concept mimics recent neurobiological insights on human control of eye movements (TD algorithm). TD algorithms are predictive reinforcement algorithms for learning of action sequences. In the proposed framework, standard techniques, such as template matching, are used as processing steps. Each processing step compares a template of one part of the object with image locations and computes a value that describes how well the template matches. A TD algorithm is trained on many images to optimize the sequence of processing steps by providing feedback whether the final object recognition was correct or incorrect. After training, the algorithm searches for template matches with the sequence of templates and locations that are most promising for recognition of a certain object. This object recognition strategy resembles active information gathering by saccadic eye movements.},
author = {Suri, Roland E.},
booktitle = {International Conference on Integration of Knowledge Intensive Multi-Agent Systems, 2003.},
doi = {10.1109/KIMAS.2003.1245074},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Suri/2003/A biologically-inspired concept for active image recognition.pdf:pdf},
keywords = {active,algorithm,algorithms,analysis,artificial,concept,eye,gathering,image,information,intelligence},
pages = {379--384},
title = {{A biologically-inspired concept for active image recognition}},
year = {2003}
}
@phdthesis{Elomaa1996,
author = {Elomaa, Tapio},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Elomaa/1996/Tools and Techniques for Decision Tree Learning.pdf:pdf},
isbn = {9514573897},
pages = {140},
school = {University of Helsinki},
title = {{Tools and Techniques for Decision Tree Learning}},
year = {1996}
}
@inproceedings{Krajca2010,
author = {Krajca, Petr and Outrata, Jan and Vychodil, Vilem},
booktitle = {Proceedings of the 7th International Conference on Concept Lattices and Their Applications},
editor = {Kryszkiewicz, Marzena and Obiedkov, Sergei},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Krajca, Outrata, Vychodil/2010/Advances in algorithms based on CbO.pdf:pdf},
issn = {16130073},
pages = {325--337},
publisher = {CEUR},
title = {{Advances in algorithms based on CbO}},
year = {2010}
}
@article{Briscoe2012,
author = {Briscoe, Robert and Schwenkler, John},
doi = {10.1111/cogs.12226},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Briscoe, Schwenkler/2012/Conscious Vision in Action.pdf:pdf},
issn = {03640213},
keywords = {attention,consciousness,dorsal stream,dual visual systems,motor control,vision},
pages = {1--21},
title = {{Conscious Vision in Action}},
year = {2012}
}
@inproceedings{Diligenti2010,
author = {Diligenti, Michelangelo and Gori, Marco and Maggini, Marco and Rigutini, Leonardo},
booktitle = {ECAI 2010: 19th European Conference on Artificial Intelligence, 16-20 August 2010, Lisbon, Portugal : Including Prestigious Applications of Artificial Intelligence (PAIS-2010) : Proceedings},
doi = {10.3233/978-1-60750-606-5-433},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Diligenti et al/2010/Multitask Kernel-based Learning with Logic Constraints.pdf:pdf},
isbn = {9781607506065},
pages = {433--438},
title = {{Multitask Kernel-based Learning with Logic Constraints}},
year = {2010}
}
@article{Finn1988,
author = {Финн, В. К.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Финн/1988/Правдоподобные выводы и правдоподобные рассуждения.pdf:pdf;:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Финн/1988/Правдоподобные выводы и правдоподобные рассуждения(2).pdf:pdf},
journal = {Итоги науки и техники. Серия: Теория вероятностей Математическая стататистика Теоретическая кибернетика},
language = {russian},
pages = {3--84},
title = {{Правдоподобные выводы и правдоподобные рассуждения}},
volume = {28},
year = {1988}
}
@phdthesis{Greff2010,
abstract = {This thesis tackles the problem of sequence learning using Hierarchical Temporal Memory as a first step towards a framework for combined temoral and spatial inference. Hierarchical Temporal Memory (HTM) is a quite new technology (2008) inspired by the human cortex to do (spatial) classification. In this thesis, we extend the theoretical framework of HTMs enabling them to do sequence classification. The improved framework is implemented and used to evaluate the algorithms on artificial data. We show this approach to be a viable first step towards a joint inference.},
author = {Greff, Klaus},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Greff/2010/Extending Hierarchical Temporal Memory for Sequence Classification.pdf:pdf},
keywords = {HTM,Master Thesis,lista{\_}filtrada},
pages = {93},
school = {Technische Universit{\"{a}}t Kaiserslautern},
title = {{Extending Hierarchical Temporal Memory for Sequence Classification}},
url = {http://trac.assembla.com/qhtm/export/114/Thesis/DiplomaThesis.pdf},
year = {2010}
}
@article{Cariani2004,
abstract = {This paper considers a space of possible temporal codes, surveys neurophysiological and psychological evidence for their use in nervous systems, and presents examples of neural timing networks that operate in the time-domain. Sensory qualities can be encoded temporally by means of two broad strategies: stimulus-driven temporal correlations (phase-locking) and stimulus-triggering of endogenous temporal response patterns. Evidence for stimulus-related spike timing patterns exists in nearly every sensory modality, and such information can be potentially utilized for representation of stimulus qualities, localization of sources, and perceptual grouping. Multiple strategies for temporal (time, frequency, and code-division) multiplexing of information for transmission and grouping are outlined. Using delays and multiplications (coincidences), neural timing networks perform time-domain signal processing operations to compare, extract and separate temporal patterns. Separation of synthetic double vowels by a recurrent neural timing network is used to illustrate how coherences in temporal fine structure can be exploited to build up and separate periodic signals with different fundamentals. Timing nets constitute a time-domain scene analysis strategy based on temporal pattern invariance rather than feature-based labeling, segregation and binding of channels. Further potential implications of temporal codes and computations for new kinds of neural networks are explored.},
author = {Cariani, Peter A.},
doi = {10.1109/TNN.2004.833305},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cariani/2004/Temporal codes and computations for sensory representation and scene analysis.pdf:pdf},
issn = {1045-9227},
journal = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
keywords = {Action Potentials,Action Potentials: physiology,Afferent,Afferent: physiology,Animals,Auditory Pathways,Auditory Pathways: physiology,Auditory Perception,Auditory Perception: physiology,Brain,Brain: physiology,Humans,Models,Nerve Net,Nerve Net: physiology,Neural Networks (Computer),Neural Pathways,Neural Pathways: physiology,Neurological,Neurons,Neurons: physiology,Reaction Time,Reaction Time: physiology,Synapses,Synapses: physiology,Synaptic Transmission,Synaptic Transmission: physiology,Taste,Taste: physiology,Time Factors,Visceral Afferents,Visceral Afferents: physiology,Visual Perception,Visual Perception: physiology},
number = {5},
pages = {1100--11},
pmid = {15484887},
title = {{Temporal codes and computations for sensory representation and scene analysis}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15484887},
volume = {15},
year = {2004}
}
@article{Ferro2010,
author = {Ferro, Marcello and Ognibene, Dimitri and Pezzulo, Giovanni and Pirrelli, Vito},
doi = {10.3389/fnbot.2010.00006},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Ferro et al/2010/Reading as active sensing a computational model of gaze planning in word recognition.pdf:pdf},
journal = {Frontiers in Neurorobotics},
keywords = {active sensing,lexical representation network,prediction,reading,serial order encoding,som},
number = {June},
pages = {1--16},
title = {{Reading as active sensing : a computational model of gaze planning in word recognition}},
volume = {4},
year = {2010}
}
@inproceedings{Ignatov2012,
author = {Игнатов, Д. И.},
booktitle = {Доклады Всероссийской научно-практической конференции «Анализ Изображений, Сетей и Текстов» (АИСТ, Екатеринбург, 2012)},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Игнатов/2012/Анализ формальных понятий от теории к практике.pdf:pdf},
language = {russian},
pages = {3--15},
title = {{Анализ формальных понятий: от теории к практике}},
year = {2012}
}
@techreport{Michalski2005,
author = {Michalski, Ryszard S. and Wojtusiak, Janusz},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Michalski, Wojtusiak/2005/Reasoning with Meta-values in AQ Learning.pdf:pdf},
institution = {George Mason University},
pages = {24},
title = {{Reasoning with Meta-values in AQ Learning}},
year = {2005}
}
@book{Loehlin2017,
author = {Loehlin, John C. and Beaujean, A. Alexander},
editor = {5th},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Loehlin, Beaujean/2017/Latent Variable Models An Introduction to Factor, Path, and Structural Equation Analysis.pdf:pdf},
isbn = {978-1-315-64319-9},
pages = {376},
publisher = {Taylor {\&} Francis},
title = {{Latent Variable Models: An Introduction to Factor, Path, and Structural Equation Analysis}},
year = {2017}
}
@phdthesis{Vilalta1998,
author = {Vilalta, Ricardo},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Vilalta/1998/On the Development of Inductive Learning Algorithms Generating Flexible and Adaptable Concept Representations.pdf:pdf},
pages = {194},
school = {University of Illinois},
title = {{On the Development of Inductive Learning Algorithms: Generating Flexible and Adaptable Concept Representations}},
year = {1998}
}
@phdthesis{Volkova2014,
author = {Волкова, А. Ю.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Волкова/2014/Разработка алгоритмических и программных средств для реализации стратегий ДСМ-метода автоматического порождения гипотез.pdf:pdf},
language = {russian},
pages = {305},
school = {Российский государственный гуманитарный университет},
title = {{Разработка алгоритмических и программных средств для реализации стратегий ДСМ-метода автоматического порождения гипотез}},
year = {2014}
}
@article{Bengio1996,
abstract = {Hidden Markov Models (HMMs) are statistical models of sequential data that have been used successfully in many machine learning applications, especially for speech recognition. We first summarize the basics of HMMs, and then review several recent related learning algorithms and extensions of HMMs, including hybrids of HMMs with artificial neural networks, Input-Output HMMs (which are conditional HMMs using neural networks to compute probabilities), weighted transducers, variable-length Markov models and Markov switching state-space models. Finally, we discuss some of the challenges of future research in this area. 1 Introduction Hidden Markov Models (HMMs) are statistical models of sequential data that have been used successfully in many applications in artificial intelligence, pattern recognition, speech recognition, and modeling of biological sequences. The focus of this paper is on learning algorithms which have been developed for HMMs and many related models, such as hybrids of HM...},
author = {Bengio, Y.},
doi = {10.1.1.54.1926},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Bengio/1996/Markovian Models for Sequential Data.pdf:pdf},
journal = {Neural Computing Surveys},
keywords = {arti cial neural networks,hidden markov models,input-output hidden markov models,learning algorithms,markov switching models,state space models,transducers,weighted},
pages = {129----162},
title = {{Markovian Models for Sequential Data}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.54.1926},
volume = {2},
year = {1996}
}
@article{Sergin2009,
author = {Сергин, В. Я.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Сергин/2009/Психофизиологические механизмы восприятия концепция объемлющих сенсорных характеристик.pdf:pdf},
journal = {Успехи физиологических наук},
language = {russian},
number = {4},
pages = {42--63},
title = {{Психофизиологические механизмы восприятия: концепция объемлющих сенсорных характеристик}},
volume = {40},
year = {2009}
}
@article{1999,
author = {Кузнецов, С. О.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Кузнецов/1999/О некоторых вопросах анализа понятий.pdf:pdf},
journal = {Научно-техническая информация. сер. 2. Информационные процессы и системы},
language = {russian},
number = {1-2},
pages = {57--61},
title = {{О некоторых вопросах анализа понятий}},
year = {1999}
}
@inproceedings{Langley1992,
author = {Langley, Pat and Iba, Wayne and Thomposn, Kevin},
booktitle = {Proceedings of the Tenth National Conference on Artificial Intelligence},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Langley, Iba, Thomposn/1992/An Analysis of Bayesian Classifers.pdf:pdf},
number = {415},
pages = {223--228},
title = {{An Analysis of Bayesian Classifers}},
year = {1992}
}
@article{Kuznetsov1996,
author = {Kuznetsov, S. O.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kuznetsov/1996/Mathematical aspects of concept analysis.pdf:pdf},
journal = {Journal of Mathematical Sciences},
number = {2},
pages = {1654--1698},
title = {{Mathematical aspects of concept analysis}},
url = {http://link.springer.com/article/10.1007/BF02362847},
volume = {80},
year = {1996}
}
@article{Lehmann1995a,
abstract = {Formal views concepts as means of intersubjective understand- ing in situations of purpose-oriented action. The formalization of concepts and systems shall especially support the interpretation and communication of conceptual relationships in different},
author = {Lehmann, Fritz and Wille, Rudolf},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Lehmann, Wille/1995/A triadic approach to formal concept analysis.pdf:pdf},
journal = {Conceptual structures: applications, Implementation and Theory},
pages = {32----43},
title = {{A triadic approach to formal concept analysis}},
url = {http://www.springerlink.com/index/p1l33w22701j5638.pdf},
year = {1995}
}
@article{Vorontcov2004a,
author = {Воронцов, К. В.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Воронцов/2004/Комбинаторные оценки качества обучения по прецедентам.pdf:pdf},
journal = {Доклады РАН},
language = {russian},
number = {2},
pages = {175--178},
title = {{Комбинаторные оценки качества обучения по прецедентам}},
volume = {394},
year = {2004}
}
@article{Tamar2016,
abstract = {We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.},
archivePrefix = {arXiv},
arxivId = {1602.02867},
author = {Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
eprint = {1602.02867},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Tamar et al/2016/Value Iteration Networks.pdf:pdf},
journal = {arXiv},
month = {feb},
pages = {1--14},
title = {{Value Iteration Networks}},
url = {http://arxiv.org/abs/1602.02867},
year = {2016}
}
@unpublished{Kuznetcov2011,
author = {Кузнецов, С. О.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Кузнецов/2011/Решетки формальных понятий в современных методах анализа и разработки данных.pdf:pdf},
language = {russian},
pages = {90},
title = {{Решетки формальных понятий в современных методах анализа и разработки данных}},
year = {2011}
}
