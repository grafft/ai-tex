Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Wang2012a,
abstract = {Reinforcement learning has been an important category of machine learning approaches exhibiting self-learning and online learning characteristics. Using reinforcement learning, an agent can learn its behaviors through trial-and-error interactions with a dynamic environment and finally come up with an optimal strategy. Reinforcement learning suffers the curse of dimensionality, though there has been significant progress to overcome this issue in recent years. MAXQ is one of the most common approaches for reinforcement learning. To function properly, MAXQ requires a decomposition of the agent's task into a task hierarchy. Previously, the decomposition can only be done manually. In this paper, we propose a mechanism for automatic subtask discovery. The mechanism applies clustering to automatically construct task hierarchy required by MAXQ, such that MAXQ can be fully automated. We present the design of our mechanism, and demonstrate its effectiveness through theoretical analysis and an extensive experimental evaluation. {\textcopyright} 2012 IEEE.},
author = {Wang, Hongbing and Li, Wenya and Zhou, Xuan},
doi = {10.1109/ICTAI.2012.165},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of International Conference on Tools with Artificial Intelligence, ICTAI/Wang, Li, Zhou/Wang, Li, Zhou - 2012 - Automatic discovery and transfer of MAXQ hierarchies in a complex system.pdf:pdf},
isbn = {9780769549156},
issn = {10823409},
journal = {Proceedings of International Conference on Tools with Artificial Intelligence, ICTAI},
keywords = {Clustering,MAXQ,Reinforcement Learning,System of Systems},
pages = {1157--1162},
title = {{Automatic discovery and transfer of MAXQ hierarchies in a complex system}},
volume = {1},
year = {2012}
}
@inproceedings{Hengst2002,
abstract = {An open problem in reinforcement learning is discovering hierarchical structure. HEXQ, an algorithm which automatically attempts to decompose and solve a model-free fac- tored MDP hierarchically is described. By searching for aliased Markov sub-space re- gions based on the state variables the algo- rithm uses temporal and state abstraction to construct a hierarchy of interlinked smaller MDPs.},
author = {Hengst, Bernhard},
booktitle = {Proceedings of the International Conference on Machine Learning ICML 2002},
doi = {10.1.1.9.5839},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the International Conference on Machine Learning ICML 2002/Hengst/Hengst - 2002 - Discovering hierarchy in reinforcement learning with HEXQ.pdf:pdf},
isbn = {1-55860-873-7},
pages = {243--250},
title = {{Discovering hierarchy in reinforcement learning with HEXQ}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.5839},
year = {2002}
}
@inproceedings{Kumar2017,
abstract = {We present a framework combining hierarchical and multi-agent deep reinforcement learning approaches to solve coordination problems among a multitude of agents using a semi-decentralized model. The framework extends the multi-agent learning setup by introducing a meta-controller that guides the communication between agent pairs, enabling agents to focus on communicating with only one other agent at any step. This hierarchical decomposition of the task allows for efficient exploration to learn policies that identify globally optimal solutions even as the number of collaborating agents increases. We show promising initial experimental results on a simulated distributed scheduling problem.},
archivePrefix = {arXiv},
arxivId = {1712.08266},
author = {Kumar, Saurabh and Shah, Pararth and Hakkani-Tur, Dilek and Heck, Larry},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
eprint = {1712.08266},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Kumar et al/Kumar et al. - 2017 - Federated Control with Hierarchical Multi-Agent Deep Reinforcement Learning.pdf:pdf},
title = {{Federated Control with Hierarchical Multi-Agent Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1712.08266},
year = {2017}
}
@article{Hayes2016,
abstract = {Collaboration between humans and robots requires solutions to an array of challenging problems, including multi-agent planning, state estimation, and goal inference. There already exist feasible solutions for many of these challenges, but they depend upon having rich task models. In this work we detail a novel type of Hierarchical Task Network we call a Clique/Chain HTN (CC-HTN), alongside an algorithm for autonomously constructing them from topological properties derived from graphical task representations. As the presented method relies on the structure of the task itself, our work imposes no particular type of symbolic insight into motor primitives or environmental representation, making it applicable to a wide variety of use cases critical to human-robot interaction. We present evaluations within a multi-resolution goal inference task and a transfer learning application showing the utility of our approach.},
author = {Hayes, Bradley and Scassellati, Brian},
doi = {10.1109/ICRA.2016.7487760},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings - IEEE International Conference on Robotics and Automation/Hayes, Scassellati/Hayes, Scassellati - 2016 - Autonomously constructing hierarchical task networks for planning and human-robot collabor.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {5469--5476},
title = {{Autonomously constructing hierarchical task networks for planning and human-robot collaboration}},
year = {2016}
}
@article{Kaplan2004,
abstract = {This chapter presents a generic internal reward system that drives an agent to increase the complexity of its behavior. This reward system does not reinforce a predefined task. Its purpose is to drive the agent to progress in learning given its embodiment and the environment in which it is placed. The dynamics created by such a system are studied first in a simple environment and then in the context of active vision.},
author = {Kaplan, Frederic and Oudeyer, Pierre-Yves},
doi = {10.1007/b99075},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Embodied artificial intelligence/Kaplan, Oudeyer/Kaplan, Oudeyer - 2004 - Maximizing learning progress An internal reward system for development.pdf:pdf},
isbn = {3-540-22484-X},
issn = {03029743},
journal = {Embodied artificial intelligence},
keywords = {Active vision,Artificial intelligence,Reward},
pages = {259--270},
title = {{Maximizing learning progress: An internal reward system for development}},
volume = {3139},
year = {2004}
}
@inproceedings{Alexander2016,
abstract = {We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub- sequences by learning for how long the plan can be committed to - i.e. followed without re-planing. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro- actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.},
archivePrefix = {arXiv},
arxivId = {1606.04695},
author = {Vezhnevets, Alexander and Mnih, Volodymyr and Agapiou, John and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Kavukcuoglu, Koray},
booktitle = {Proceeding of NIPS 2016},
doi = {10.3847/0004-637X/832/1/56},
eprint = {1606.04695},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/arXiv/Alexander et al/Alexander et al. - 2016 - Strategic Attentive Writer for Learning Macro-Actions.pdf:pdf},
issn = {10495258},
title = {{Strategic Attentive Writer for Learning Macro-Actions}},
url = {http://arxiv.org/abs/1606.04695},
year = {2016}
}
@article{Daniel2016,
abstract = {Tasks that require many sequential decisions are hard to solve using conventional rein- forcement learning algorithms. Based on the option framework, we propose a model which aims to alleviate these concerns. Instead of learning a single monolithic policy, the agent learns a set of simpler sub-policies as well as the initiation and termination probabili- ties for each of those sub-policies. While ex- isting option learning algorithms frequently require manual specification of components such as the sub-policies, we present an algo- rithm which infers all relevant components of the option framework from data. We present results on SMDPs with discrete as well as continuous state-action spaces. The results show that the presented algorithm can combine simple sub-policies to solve complex tasks and can improve learning performance on simpler tasks.},
author = {Daniel, Christian and van Hoof, Herke and Peters, Jan and Neumann, Gerhard},
doi = {10.1007/s10994-016-5580-x},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Machine Learning/Daniel et al/Daniel et al. - 2016 - Probabilistic inference for determining options in reinforcement learning.pdf:pdf},
issn = {15730565},
journal = {Machine Learning},
keywords = {Options,Reinforcement learning,Robot learning,Semi Markov decision process},
number = {2-3},
pages = {337--357},
publisher = {Springer US},
title = {{Probabilistic inference for determining options in reinforcement learning}},
volume = {104},
year = {2016}
}
@incollection{Hengst2012,
abstract = {Hierarchical decomposition tackles complex problems by reducing them to a smaller set of interrelated problems. The smaller problems are solved separately and the results re-combined to find a solution to the original problem. It is well known that the na{\"{i}}ve application of reinforcement learning (RL) techniques fails to scale to more complex domains. This Chapter introduces hierarchical approaches to reinforcement learning that hold out the promise of reducing a reinforcement learning problems to a manageable size. Hierarchical Reinforcement Learning (HRL) rests on finding good re-usable temporally extended actions that may also provide opportunities for state abstraction. Methods for reinforcement learning can be extended to work with abstract states and actions over a hierarchy of subtasks that decompose the original problem, potentially reducing its computational complexity. We use a four-room task as a running example to illustrate the various concepts and approaches, including algorithms that can automatically learn the hierarchical structure from interactions with the domain.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.02971},
author = {Hengst, Bernhard},
booktitle = {Reinforcement Learning},
doi = {10.1007/978-3-642-27645-3_9},
eprint = {arXiv:1509.02971},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Reinforcement Learning/Hengst/Hengst - 2012 - Hierarchical Approaches.pdf:pdf},
isbn = {978-3-642-27644-6},
issn = {18726240},
pages = {293--323},
title = {{Hierarchical Approaches}},
url = {http://dx.doi.org/10.1007/978-3-642-27645-3{\_}9 http://link.springer.com/10.1007/978-3-642-27645-3{\_}9},
year = {2012}
}
@article{Botvinick2009a,
abstract = {Research on human and animal behavior has long emphasized its hierarchical structure-the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior.},
author = {Botvinick, Matthew M. and Niv, Yael and Barto, Andrew C.},
doi = {10.1016/j.cognition.2008.08.011},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Cognition/Botvinick, Niv, Barto/Botvinick, Niv, Barto - 2009 - Hierarchically organized behavior and its neural foundations a reinforcement learning perspective.pdf:pdf},
issn = {1873-7838},
journal = {Cognition},
keywords = {Animals,Humans,Models,Nerve Net,Nerve Net: physiology,Prefrontal Cortex,Prefrontal Cortex: physiology,Problem Solving,Problem Solving: physiology,Psychological,Reinforcement (Psychology)},
number = {3},
pages = {262--80},
pmid = {18926527},
title = {{Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2783353{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {113},
year = {2009}
}
@article{Konidaris2012,
abstract = {We present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to significantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that significantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-specific skills.},
author = {Konidaris, George and Scheidwasser, Ilya and Barto, Andrew G.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Journal of Machine Learning Research/Konidaris, Scheidwasser, Barto/Konidaris, Scheidwasser, Barto - 2012 - Transfer in reinforcement learning via shared features.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {reinforcement learning,shaping,skills,transfer},
number = {1},
pages = {1333--1371},
title = {{Transfer in reinforcement learning via shared features}},
url = {http://dl.acm.org/citation.cfm?id=2503308.2343689{\%}5Cnhttp://dl.acm.org/ft{\_}gateway.cfm?id=2343689{\&}type=pdf},
volume = {13},
year = {2012}
}
@inproceedings{Barto2004,
abstract = {Humans and other animals often engage in activities for their own sakes rather than as steps toward solving practical problems. Psychologists call these intrinsically motivated behaviors. What we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy. At the core of the model are recent theoretical and algorithmic advances in computational reinforcement learning, specifically, new concepts related to skills and new learning algorithms for learning with skill hierarchies. 1},
author = {Barto, Andrew G. and Singh, Satinder},
booktitle = {Proceedings of the 3rd International Conference on Development and Learning (ICDL 2004)},
doi = {10.1.1.123.395},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 3rd International Conference on Development and Learning (ICDL 2004)/Barto, Singh/Barto, Singh - 2004 - Intrinsically motivated learning of hierarchical collections of skills.pdf:pdf},
pages = {112--119},
title = {{Intrinsically motivated learning of hierarchical collections of skills}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.117.6436{\%}5Cnhttp://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A083E47D2FE080D11716EC249E464BE2?doi=10.1.1.117.6436{\&}rep=rep1{\&}type=pdf},
year = {2004}
}
@inproceedings{Shu2017,
abstract = {Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task corresponds to a human language description. Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices. In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills. We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.},
archivePrefix = {arXiv},
arxivId = {1712.07294},
author = {Shu, Tianmin and Xiong, Caiming and Socher, Richard},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
doi = {10.1051/0004-6361/201527329},
eprint = {1712.07294},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Shu, Xiong, Socher/Shu, Xiong, Socher - 2017 - Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pmid = {23459267},
title = {{Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning}},
url = {http://arxiv.org/abs/1712.07294},
year = {2017}
}
@inproceedings{Roderick2017,
abstract = {We examine the problem of learning and planning on high-dimensional domains with long horizons and sparse rewards. Recent approaches have shown great successes in many Atari 2600 domains. However, domains with long horizons and sparse rewards, such as Montezuma's Revenge and Venture, remain challenging for existing methods. Methods using abstraction (Dietterich 2000; Sutton, Precup, and Singh 1999) have shown to be useful in tackling long-horizon problems. We combine recent techniques of deep reinforcement learning with existing model-based approaches using an expert-provided state abstraction. We construct toy domains that elucidate the problem of long horizons, sparse rewards and high-dimensional inputs, and show that our algorithm significantly outperforms previous methods on these domains. Our abstraction-based approach outperforms Deep Q-Networks (Mnih et al. 2015) on Montezuma's Revenge and Venture, and exhibits backtracking behavior that is absent from previous methods.},
archivePrefix = {arXiv},
arxivId = {1710.00459},
author = {Roderick, Melrose and Grimm, Christopher and Tellex, Stefanie},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
eprint = {1710.00459},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Roderick, Grimm, Tellex/Roderick, Grimm, Tellex - 2017 - Deep Abstract Q-Networks.pdf:pdf},
title = {{Deep Abstract Q-Networks}},
url = {http://arxiv.org/abs/1710.00459},
year = {2017}
}
@inproceedings{Jonsson2001,
abstract = {Learning a complex task can be significantly facilitated by defining a hierarchy of subtasks. An agent can learn to choose between various temporally abstract actions, each solving an assigned subtask, to accomplish the overall task. In this paper, we study hierarchical learning using the framework of options. We argue that to take full advantage of hierarchical structure, one should perform option-specific state abstraction, and that if this is to scale to larger tasks, state abstraction ...},
author = {Jonsson, Anders and Barto, Andrew G.},
booktitle = {Proceedings of NIPS 2001},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of NIPS 2001/Jonsson, Barto/Jonsson, Barto - 2001 - Automated State Abstraction for Options using the U-Tree Algorithm.pdf:pdf},
isbn = {0262122413},
issn = {1049-5258},
pages = {1054--1060},
title = {{Automated State Abstraction for Options using the U-Tree Algorithm}},
year = {2001}
}
@article{Singh2010,
abstract = {There is great interest in building intrinsic motivation into artificial systems using the reinforcement learning framework. Yet, what intrinsic motivation may mean computationally, and how it may differ from extrinsic motivation, remains a murky and controversial subject. In this paper, we adopt an evolutionary perspective and define a new optimal reward framework that captures the pressure to design good primary reward functions that lead to evolutionary success across environments. The results of two computational experiments show that optimal primary reward signals may yield both emergent intrinsic and extrinsic motivation. The evolutionary perspective and the associated optimal reward framework thus lead to the conclusion that there are no hard and fast features distinguishing intrinsic and extrinsic reward computationally. Rather, the directness of the relationship between rewarding behavior and evolutionary success varies along a continuum.},
author = {Singh, Satinder and Lewis, Richard L. and Barto, Andrew G. and Sorg, Jonathan},
doi = {10.1109/TAMD.2010.2051031},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IEEE Transactions on Autonomous Mental Development/Singh et al/Singh et al. - 2010 - Intrinsically Motivated Reinforcement Learning An Evolutionary Perspective.pdf:pdf},
isbn = {0262195348, 9780262195348},
issn = {1943-0604},
journal = {IEEE Transactions on Autonomous Mental Development},
number = {2},
pages = {70--82},
title = {{Intrinsically Motivated Reinforcement Learning: An Evolutionary Perspective}},
url = {http://ieeexplore.ieee.org/document/5471106/},
volume = {2},
year = {2010}
}
@article{Simsek2005,
abstract = {We present a new subgoal-based method for automatically creating useful skills in reinforcement learning. Our method identifies subgoals by partitioning local state transition graphs—those that are constructed using only the most recent experiences of the agent. The local scope of our subgoal discovery method allows it to successfully identify the type of subgoals we seek—states that lie between two densely-connected regions of the state space—while producing an algorithm with low computational cost.},
author = {Şimşek, {\"{O}}zg{\"{u}}r and Wolfe, Alicia P. and Barto, Andrew G.},
doi = {10.1145/1102351.1102454},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 22nd international conference on Machine learning - ICML '05/Şimşek, Wolfe, Barto/Şimşek, Wolfe, Barto - 2005 - Identifying useful subgoals in reinforcement learning by local graph partitio.pdf:pdf},
isbn = {1595931805},
journal = {Proceedings of the 22nd international conference on Machine learning  - ICML '05},
pages = {816--823},
title = {{Identifying useful subgoals in reinforcement learning by local graph partitioning}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102454},
year = {2005}
}
@incollection{Castro2012,
abstract = {Abstract. Temporally extended actions are usually effective in speeding up reinforcement learning. In this paper we present a mechanism for automatically constructing such actions, expressed as options [Sutton et al., 1999], in a finite Markov Decision Process (MDP). To do this, we compute a bisimulation metric [Ferns et al., 2004] between the states in a small MDP and the states in a large MDP, which we want to solve. The shape of this metric is then used to completely define a set of options for the large MDP. We demonstrate empirically that our approach is able to improve the speed of reinforcement learning, and is generally not sensitive to parameter tuning.},
author = {Castro, Pablo Samuel and Precup, Doina},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-29946-9_16},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Castro, Precup/Castro, Precup - 2012 - Automatic Construction of Temporally.pdf:pdf},
isbn = {9783642299452},
issn = {03029743},
pages = {140--152},
title = {{Automatic Construction of Temporally Extended Actions for MDPs Using Bisimulation Metrics}},
url = {http://link.springer.com/10.1007/978-3-642-29946-9{\_}16},
volume = {7188 LNAI},
year = {2012}
}
@inproceedings{Mehta2008b,
abstract = {We present an algorithm, HI-MAT (Hierar- chy Induction via Models And Trajectories), that discovers MAXQ task hierarchies by ap- plying dynamic Bayesian network models to a successful trajectory from a source rein- forcement learning task. HI-MAT discovers subtasks by analyzing the causal and tem- poral relationships among the actions in the trajectory. Under appropriate assumptions, HI-MAT induces hierarchies that are consis- tent with the observed trajectory and have compact value-function tables employing safe state abstractions. We demonstrate empir- ically that HI-MAT constructs compact hi- erarchies that are comparable to manually- engineered hierarchies and facilitate signifi- cant speedup in learning when transferred to a target task.},
address = {New York, New York, USA},
author = {Mehta, Neville and Ray, Soumya and Tadepalli, Prasad and Dietterich, Thomas},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390238},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 25th international conference on Machine learning - ICML '08/Mehta et al/Mehta et al. - 2008 - Automatic discovery and transfer of MAXQ hierarchies.pdf:pdf},
isbn = {9781605582054},
pages = {648--655},
publisher = {ACM Press},
title = {{Automatic discovery and transfer of MAXQ hierarchies}},
url = {http://portal.acm.org/citation.cfm?id=1390238 http://portal.acm.org/citation.cfm?doid=1390156.1390238},
year = {2008}
}
@inproceedings{MacGlashan2010,
abstract = {We present skill bootstrapping, a proposed new research direction for agent learning and planning that allows an agent to start with low-level primitive actions, and develop skills that can be used for higher-level planning. Skills are developed over the course of solving many different problems in a domain, using reinforcement learning techniques to complement the benefits and disadvantages of heuristic-search planning. We describe the overall architecture of the proposed approach, discuss how it relates to other work, and give motivating examples for why this approach would be successful.},
author = {MacGlashan, James and DesJArdins, Marie},
booktitle = {24th AAAI Conference on Artificial Intelligence},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/24th AAAI Conference on Artificial Intelligence/MacGlashan, DesJArdins/MacGlashan, DesJArdins - 2010 - Hierarchical Skill Learning for High-Level Planning.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/24th AAAI Conference on Artificial Intelligence/MacGlashan, DesJArdins/MacGlashan, DesJArdins - 2010 - Hierarchical Skill Learning for High-Level Planning.pdf:pdf},
isbn = {9781577354666},
keywords = {abstraction,approximation,planning,reinforcement learning},
pages = {1988--1989},
title = {{Hierarchical Skill Learning for High-Level Planning}},
volume = {3},
year = {2010}
}
@article{Cao2012,
abstract = {We describe an approach to incorporating Bayesian priors in the MAXQ framework for hierarchical reinforcement learning (HRL). We define priors on the primitive environment model and on task pseudo-rewards. Since models for composite tasks can be complex, we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy. We show empirically that (i) our approach results in improved convergence over non-Bayesian baselines, (ii) using both task hierarchies and Bayesian priors is better than either alone, (iii) taking advantage of the task hierarchy reduces the computational cost of Bayesian reinforcement learning and (iv) in this framework, task pseudo-rewards can be learned instead of being manually specified, leading to hierarchically optimal rather than recursively optimal policies.},
author = {Cao, F and Ray, Soumya},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Advances in Neural Information Processing Systems/Cao, Ray/Cao, Ray - 2012 - Bayesian hierarchical reinforcement learning.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {73--81},
title = {{Bayesian hierarchical reinforcement learning}},
url = {http://papers.nips.cc/paper/4752-bay},
year = {2012}
}
@inproceedings{Goel2017,
author = {Goel, Karan and Mu, Tong and Brunskill, Emma},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Goel, Mu, Brunskill/Goel, Mu, Brunskill - 2017 - Optimal Hierarchical Policy Extraction From Noisy Imperfect Demonstrations.pdf:pdf},
title = {{Optimal Hierarchical Policy Extraction From Noisy Imperfect Demonstrations}},
url = {https://drive.google.com/file/d/101FsZkczKMfGeUBTP-089mhkTeepj8IW/view},
year = {2017}
}
@inproceedings{Singh2005,
author = {Singh, Satinder and Barto, Andrew G. and Chentanez, Nuttapong},
booktitle = {Proceeding of NIPS 2005},
doi = {10.1.1.123.395},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceeding of NIPS 2005/Singh, Barto, Chentanez/Singh, Barto, Chentanez - 2005 - Intrinsically motivated reinforcement learning.pdf:pdf},
pages = {1281--1288},
title = {{Intrinsically motivated reinforcement learning}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2005{\_}724.pdf},
year = {2005}
}
@article{Rasmussen2017a,
abstract = {We present the first model capable of performing hierarchical reinforcement learning in a general, neurally detailed imple-mentation. We show that this model is able to learn a spatial pickup and delivery task more quickly than one without hier-archical abilities. In addition, we show that this model is able to leverage its hierarchical structure to transfer learned knowl-edge between related tasks. These results point towards the advantages to be gained by using a hierarchical RL framework to understand the brain's powerful learning ability.},
author = {Rasmussen, Daniel and Voelker, Aaron and Eliasmith, Chris},
doi = {10.1371/journal.pone.0180234},
editor = {Cymbalyuk, Gennady},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/PLOS ONE/Rasmussen, Voelker, Eliasmith/Rasmussen, Voelker, Eliasmith - 2017 - A neural model of hierarchical reinforcement learning.pdf:pdf},
isbn = {1111111111},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {brain could achieve its,hierarchical,hrl into a theory,neural engineering framework,neural model,of neural function,providing a hypothe-,reinforcement learning,sis for how the,strengths in scaling},
month = {jul},
number = {7},
pages = {e0180234},
title = {{A neural model of hierarchical reinforcement learning}},
url = {http://dx.plos.org/10.1371/journal.pone.0180234},
volume = {12},
year = {2017}
}
@incollection{Menache2002,
abstract = {We present the Q-Cut algorithm, a graph theoretic approach for automatic detection of sub-goals in a dynamic environment, which is used for acceleration of the Q-Learning algorithm. The learning agent creates an on-line map of the process history, and uses an efficient Max-Flow/Min-Cut algorithm for identifying bottlenecks. The policies for reaching bottlenecks are separately learned and added to the model in a form of options (macro-actions). We then extend the basic Q-Cut algorithm to the Segmented Q-Cut algorithm, which uses previously identified bottlenecks for state space partitioning, necessary for finding additional bottlenecks in complex environments. Experiments show significant performance improvements, particulary in the initial learning phase.},
author = {Menache, Ishai and Mannor, Shie and Shimkin, Nahum},
booktitle = {ECML 2002: Machine Learning: ECML 2002},
doi = {10.1007/3-540-36755-1_25},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/ECML 2002 Machine Learning ECML 2002/Menache, Mannor, Shimkin/Menache, Mannor, Shimkin - 2002 - Q-Cut—Dynamic Discovery of Sub-goals in Reinforcement Learning.pdf:pdf},
isbn = {978-3-540-44036-9, 978-3-540-36755-0},
issn = {16113349},
pages = {295--306},
title = {{Q-Cut—Dynamic Discovery of Sub-goals in Reinforcement Learning}},
url = {http://link.springer.com/10.1007/3-540-36755-1{\_}25},
year = {2002}
}
@inproceedings{Mannor2004,
abstract = {We consider a graph theoretic approach for automatic construction of options in a dynamic environment. A map of the environment is generated on-line by the learning agent, representing the topological structure of the state transitions. A clustering algorithm is then used to partition the state space to different regions. Policies for reaching the different parts of the space are separately learned and added to the model in a form of options (macro-actions). The options are used for accelerating the Q-Learning algorithm. We extend the basic algorithm and consider building a map that includes preliminary indication of the location of "interesting" regions of the state space, where the value gradient is significant and additional exploration might be beneficial. Experiments indicate significant speedups, especially in the initial learning phase.},
address = {New York, New York, USA},
author = {Mannor, Shie and Menache, Ishai and Hoze, Amit and Klein, Uri},
booktitle = {Twenty-first international conference on Machine learning - ICML '04},
doi = {10.1145/1015330.1015355},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Twenty-first international conference on Machine learning - ICML '04/Mannor et al/Mannor et al. - 2004 - Dynamic abstraction in reinforcement learning via clustering.pdf:pdf},
isbn = {1581138285},
keywords = {clustering,hierarchical reinforcement learning,options,q-learning,reinforcement learning},
pages = {71},
publisher = {ACM Press},
title = {{Dynamic abstraction in reinforcement learning via clustering}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015355},
year = {2004}
}
@article{Konidaris2016,
abstract = {We describe a framework for building abstraction hierarchies whereby an agent alternates skill- and representation-acquisition phases to construct a sequence of increasingly abstract Markov decision processes. Our formulation builds on recent results showing that the appropriate abstract representation of a problem is specified by the agent's skills. We describe how such a hierarchy can be used for fast planning, and illustrate the construction of an appropriate hierarchy for the Taxi domain.},
archivePrefix = {arXiv},
arxivId = {1509.07582},
author = {Konidaris, George},
doi = {10.1016/j.surg.2015.03.052.Rate},
eprint = {1509.07582},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IJCAI International Joint Conference on Artificial Intelligence/Konidaris/Konidaris - 2016 - Constructing abstraction hierarchies using a skill-symbol loop.pdf:pdf},
isbn = {6176366127},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning},
pages = {1648--1654},
pmid = {28579718},
title = {{Constructing abstraction hierarchies using a skill-symbol loop}},
volume = {2016-Janua},
year = {2016}
}
