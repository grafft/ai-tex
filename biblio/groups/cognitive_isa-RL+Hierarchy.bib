Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Barto2003,
abstract = {Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of {\{}semi-Markov{\}} decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.},
author = {Barto, Andrew G. and Mahadevan, Sridhar},
doi = {10.1023/A:1025696116075},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Discrete Event Dynamic Systems/Barto, Mahadevan/Barto, Mahadevan - 2003 - Recent Advances in Hierarchical Reinforcement Learning.pdf:pdf},
isbn = {0924-6703},
issn = {09246703},
journal = {Discrete Event Dynamic Systems},
pages = {341--379},
title = {{Recent Advances in Hierarchical Reinforcement Learning}},
url = {http://dx.doi.org/10.1023/A:1025696116075},
volume = {13},
year = {2003}
}
@article{Wang2012a,
abstract = {Reinforcement learning has been an important category of machine learning approaches exhibiting self-learning and online learning characteristics. Using reinforcement learning, an agent can learn its behaviors through trial-and-error interactions with a dynamic environment and finally come up with an optimal strategy. Reinforcement learning suffers the curse of dimensionality, though there has been significant progress to overcome this issue in recent years. MAXQ is one of the most common approaches for reinforcement learning. To function properly, MAXQ requires a decomposition of the agent's task into a task hierarchy. Previously, the decomposition can only be done manually. In this paper, we propose a mechanism for automatic subtask discovery. The mechanism applies clustering to automatically construct task hierarchy required by MAXQ, such that MAXQ can be fully automated. We present the design of our mechanism, and demonstrate its effectiveness through theoretical analysis and an extensive experimental evaluation. {\textcopyright} 2012 IEEE.},
author = {Wang, Hongbing and Li, Wenya and Zhou, Xuan},
doi = {10.1109/ICTAI.2012.165},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of International Conference on Tools with Artificial Intelligence, ICTAI/Wang, Li, Zhou/Wang, Li, Zhou - 2012 - Automatic discovery and transfer of MAXQ hierarchies in a complex system.pdf:pdf},
isbn = {9780769549156},
issn = {10823409},
journal = {Proceedings of International Conference on Tools with Artificial Intelligence, ICTAI},
keywords = {Clustering,MAXQ,Reinforcement Learning,System of Systems},
pages = {1157--1162},
title = {{Automatic discovery and transfer of MAXQ hierarchies in a complex system}},
volume = {1},
year = {2012}
}
@article{James2018a,
author = {James, Steven and Rosman, Benjamin and Africa, South and Konidaris, George},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/James et al/James et al. - 2018 - Learning to Plan with Portable Symbols.pdf:pdf},
number = {July},
title = {{Learning to Plan with Portable Symbols}},
year = {2018}
}
@article{Chen2016a,
abstract = {Abstract Lifelong Machine Learning (or Lifelong Learning) is an advanced machine learning paradigm that learns continuously, accumulates the knowledge learned in previous tasks, and uses it to help future learning. In the process, the learner becomes more and more knowledgeable and effective at learning. This learning ability is one of the hallmarks of human intelligence. However, the current dominant machine learning paradigm learns in isolation: given a training dataset, it runs a machine learning algorithm on the dataset to produce a model. It makes no attempt to retain the learned knowledge and use it in future learning. Although this isolated learning paradigm has been very successful, it requires a large number of training examples, and is only suitable for well-defined and narrow tasks. In comparison, we humans can learn effectively with a few examples because we have accumulated so much knowledge in the past which enables us to learn with little data or effort. Lifelong learning aims to achieve th...},
author = {Chen, Zhiyuan and Liu, Bing},
doi = {10.2200/S00737ED1V01Y201610AIM033},
file = {::},
isbn = {9781627055017},
issn = {1939-4608},
journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
number = {3},
pages = {1--145},
title = {{Lifelong Machine Learning}},
url = {http://www.morganclaypool.com/doi/10.2200/S00737ED1V01Y201610AIM033},
volume = {10},
year = {2016}
}
@phdthesis{Mehta2011,
abstract = {Acting intelligently to efficiently solve sequential decision problems requires the ability to extract hierarchical structure from the underlying domain dynamics, exploit it for optimal or near-optimal decision-making, and transfer it to related problems instead of solving every problem in isolation. This dissertation makes three contributions toward this goal. The first contribution is the introduction of two frameworks for the transfer of hi- erarchical structure in sequential decision problems. The MASH framework facilitates transfer among multiple agents coordinating within a domain. The VRHRL framework allows an agent to transfer its knowledge across a family of domains that share the same transition dynamics but have differing reward dynamics. Both MASH and VRHRL are validated empirically in large domains and the results demonstrate significant speedup in the solutions due to transfer. The second contribution is a new approach to the discovery of hierarchical structure in sequential decision problems. HI-MAT leverages action models to analyze the relevant dependencies in a hierarchically-generated trajectory and it discovers hierarchical struc- ture that transfers to all problems whose actions share the same relevant dependencies as the single source problem. HierGen advances HI-MAT by learning simple action models, leveraging these models to analyze non-hierarchically-generated trajectories from mul- tiple source problems in a robust causal fashion, and discovering hierarchical structure that transfers to all problems whose actions share the same causal dependencies as those in the source problems. Empirical evaluations in multiple domains demonstrate that the discovered hierarchical structures are comparable to manually-designed structures in quality and performance. Action models are essential to hierarchical structure discovery and other aspects of intelligent behavior. The third contribution of this dissertation is the introduction of two general frameworks for learning action models in sequential decision problems. In the MBP framework, learning is user-driven; in the PLEX framework, the learner generates its own problems. The frameworks are formally analyzed and reduced to concept learning with one-sided error. A general action-modeling language is shown to be efficiently learnable in both frameworks.},
author = {Mehta, Neville},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Mehta/Mehta - 2011 - Hierarchical Structure Discovery and Transfer in Sequential Decision Problems.pdf:pdf},
pages = {180},
title = {{Hierarchical Structure Discovery and Transfer in Sequential Decision Problems}},
year = {2011}
}
@article{Kaelbling1993,
abstract = {This paper presents the HDG learning algorithm, which uses a hierarchical decomposition of the state space to make learning to achieve goals more efficient with a small penalty in path qual-ity. Special care must be taken when performing hierarchical planning and learning in stochastic domains, because macro-operators cannot be ex-ecuted ballistically. The HDG algorithm, which is a descendent of Watkins' Q-learning algorithm, is described here and preliminary empirical re-sults are presented.},
author = {Kaelbling, Leslie},
file = {::},
journal = {Proceedings of the Tenth International Conference on Machine Learning},
pages = {167--173},
title = {{Hierarchical Learning in Stochastic Domains: Preliminary Results}},
url = {http://people.csail.mit.edu/lpk/papers/ml93.ps},
year = {1993}
}
@article{Pardo2017,
abstract = {In reinforcement learning, it is common to let an agent interact for a fixed amount of time with its environment before resetting it and repeating the process in a series of episodes. The task that the agent has to learn can either be to maximize its performance over (i) that fixed period, or (ii) an indefinite period where time limits are only used during training to diversify experience. In this paper, we provide a formal account for how time limits could effectively be handled in each of the two cases and explain why not doing so can cause state-aliasing and invalidation of experience replay, leading to suboptimal policies and training instability. In case (i), we argue that the terminations due to time limits are in fact part of the environment, and thus a notion of the remaining time should be included as part of the agent's input to avoid violation of the Markov property. In case (ii), the time limits are not part of the environment and are only used to facilitate learning. We argue that this insight should be incorporated by bootstrapping from the value of the state at the end of each partial episode. For both cases, we illustrate empirically the significance of our considerations in improving the performance and stability of existing reinforcement learning algorithms, showing state-of-the-art results on several control tasks.},
archivePrefix = {arXiv},
arxivId = {1712.00378},
author = {Pardo, Fabio and Tavakoli, Arash and Levdik, Vitaly and Kormushev, Petar},
eprint = {1712.00378},
file = {::},
issn = {1938-7228},
title = {{Time Limits in Reinforcement Learning}},
url = {http://arxiv.org/abs/1712.00378},
year = {2017}
}
@article{Dubey2018,
abstract = {What makes humans so good at solving seemingly complex video games? Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors on human performance. We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors. We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play. Videos and the game manipulations are available at https://rach0012.github.io/humanRL{\_}website/},
archivePrefix = {arXiv},
arxivId = {1802.10217},
author = {Dubey, Rachit and Agrawal, Pulkit and Pathak, Deepak and Griffiths, Thomas L. and Efros, Alexei A.},
doi = {arXiv:1802.10217v1},
eprint = {1802.10217},
file = {::},
title = {{Investigating Human Priors for Playing Video Games}},
url = {http://arxiv.org/abs/1802.10217},
year = {2018}
}
@article{Ring1994,
abstract = {Continual learning$\backslash$n$\backslash$nis the constant development of complex behaviors with no nal end in$\backslash$n$\backslash$nmind. It is the process of learning ever more complicated skills by$\backslash$nbuilding on those skills al-$\backslash$n$\backslash$nready developed. In order for learning at one stage of development$\backslash$nto serve as the foundation$\backslash$n$\backslash$nfor later learning, a continual-learning agent should learn hierarchically.$\backslash$nCHILD, an agent$\backslash$n$\backslash$ncapable of$\backslash$n$\backslash$nContinual, Hierarchical, Incremental Learning$\backslash$n$\backslash$nand$\backslash$n$\backslash$nDevelopment$\backslash$n$\backslash$nis proposed,$\backslash$n$\backslash$ndescribed, tested, and evaluated in this dissertation. CHILD accumulates$\backslash$nuseful behaviors$\backslash$n$\backslash$nin reinforcement environments by using the$\backslash$n$\backslash$nTemporal Transition Hierarchies$\backslash$n$\backslash$nlearning algo-$\backslash$n$\backslash$nrithm, also derived in the dissertation. This constructive algorithm$\backslash$ngenerates a hierarchical,$\backslash$n$\backslash$nhigher-order neural network that can be used for predicting context-dependent$\backslash$ntemporal se-$\backslash$n$\backslash$nquences and can learn sequential-task benchmarks more than two orders$\backslash$nof magnitude faster$\backslash$n$\backslash$nthan competing neural-network systems. Consequently, CHILD can quickly$\backslash$nsolve compli-$\backslash$n$\backslash$ncated non-Markovian reinforcement-learning tasks and can then transfer$\backslash$nits skills to similar$\backslash$n$\backslash$nbut even more complicated tasks, learning these faster still. This$\backslash$ncontinual-learning ap-$\backslash$n$\backslash$nproach is made possible by the unique properties of Temporal Transition$\backslash$nHierarchies, which$\backslash$n$\backslash$nallow existing skills to be amended and augmented in precisely the$\backslash$nsame way that they were$\backslash$n$\backslash$nconstructed in the rst place.},
author = {Ring, M B},
file = {::},
journal = {Psychology},
pages = {44},
title = {{Continual learning in reinforcement environments}},
volume = {1},
year = {1994}
}
@inproceedings{Hengst2002,
abstract = {An open problem in reinforcement learning is discovering hierarchical structure. HEXQ, an algorithm which automatically attempts to decompose and solve a model-free fac- tored MDP hierarchically is described. By searching for aliased Markov sub-space re- gions based on the state variables the algo- rithm uses temporal and state abstraction to construct a hierarchy of interlinked smaller MDPs.},
author = {Hengst, Bernhard},
booktitle = {Proceedings of the International Conference on Machine Learning ICML 2002},
doi = {10.1.1.9.5839},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the International Conference on Machine Learning ICML 2002/Hengst/Hengst - 2002 - Discovering hierarchy in reinforcement learning with HEXQ.pdf:pdf},
isbn = {1-55860-873-7},
pages = {243--250},
title = {{Discovering hierarchy in reinforcement learning with HEXQ}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.5839},
year = {2002}
}
@inproceedings{Kumar2017,
abstract = {We present a framework combining hierarchical and multi-agent deep reinforcement learning approaches to solve coordination problems among a multitude of agents using a semi-decentralized model. The framework extends the multi-agent learning setup by introducing a meta-controller that guides the communication between agent pairs, enabling agents to focus on communicating with only one other agent at any step. This hierarchical decomposition of the task allows for efficient exploration to learn policies that identify globally optimal solutions even as the number of collaborating agents increases. We show promising initial experimental results on a simulated distributed scheduling problem.},
archivePrefix = {arXiv},
arxivId = {1712.08266},
author = {Kumar, Saurabh and Shah, Pararth and Hakkani-Tur, Dilek and Heck, Larry},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
eprint = {1712.08266},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Kumar et al/Kumar et al. - 2017 - Federated Control with Hierarchical Multi-Agent Deep Reinforcement Learning.pdf:pdf},
title = {{Federated Control with Hierarchical Multi-Agent Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1712.08266},
year = {2017}
}
@article{Dayan1993,
abstract = {One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time. This paper shows how to create a Q-learning managerial hierarchy in which high level managers learn how to set tasks to their sub-managers who, in turn, learn how to satisfy them. Sub-managers need not initially understand their managers' commands. They simply learn to maximise their reinforcement in the context of the current command. We illustrate the system using a simple maze task.. As the system learns how to get around, satisfying commands at the multiple levels, it explores more efficiently than standard, flat, Q-learning and builds a more comprehensive map.},
author = {Dayan, Peter and Hinton, Geoffrey},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Advances in neural information processing systems/Dayan, Hinton/Dayan, Hinton - 1993 - Feudal Reinforcement Learning.pdf:pdf},
isbn = {1-55860-274-7},
issn = {1049-5258},
journal = {Advances in neural information processing systems},
keywords = {Reinforcement Learning},
pages = {271--278},
title = {{Feudal Reinforcement Learning}},
url = {http://www.cs.utoronto.ca/{~}hinton/absps/dh93.pdf},
year = {1993}
}
@article{Co-Reyes2018a,
abstract = {In this work, we take a representation learning perspective on hierarchical reinforcement learning, where the problem of learning lower layers in a hierarchy is transformed into the problem of learning trajectory-level generative models. We show that we can learn continuous latent representations of trajectories, which are effective in solving temporally extended and multi-stage problems. Our proposed model, SeCTAR, draws inspiration from variational autoencoders, and learns latent representations of trajectories. A key component of this method is to learn both a latent-conditioned policy and a latent-conditioned model which are consistent with each other. Given the same latent, the policy generates a trajectory which should match the trajectory predicted by the model. This model provides a built-in prediction mechanism, by predicting the outcome of closed loop policy behavior. We propose a novel algorithm for performing hierarchical RL with this model, combining model-based planning in the learned latent space with an unsupervised exploration objective. We show that our model is effective at reasoning over long horizons with sparse rewards for several simulated tasks, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration.},
archivePrefix = {arXiv},
arxivId = {1806.02813},
author = {Co-Reyes, John D. and Liu, YuXuan and Gupta, Abhishek and Eysenbach, Benjamin and Abbeel, Pieter and Levine, Sergey},
eprint = {1806.02813},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Co-Reyes et al/Co-Reyes et al. - 2018 - Self-Consistent Trajectory Autoencoder Hierarchical Reinforcement Learning with Trajectory Embeddings.pdf:pdf},
issn = {1938-7228},
title = {{Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings}},
url = {http://arxiv.org/abs/1806.02813},
year = {2018}
}
@book{Szepesvari2010,
abstract = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective.What distin- guishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the al- gorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book,we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming.We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.},
author = {Szepesv{\'{a}}ri, Csaba},
booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
doi = {10.2200/S00268ED1V01Y201005AIM009},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Synthesis Lectures on Artificial Intelligence and Machine Learning/Szepesv{\'{a}}ri/Szepesv{\'{a}}ri - 2010 - Algorithms for Reinforcement Learning.pdf:pdf},
isbn = {9781608454921},
issn = {1939-4608},
number = {1},
pages = {1--103},
title = {{Algorithms for Reinforcement Learning}},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009},
volume = {4},
year = {2010}
}
@article{Hayes2016,
abstract = {Collaboration between humans and robots requires solutions to an array of challenging problems, including multi-agent planning, state estimation, and goal inference. There already exist feasible solutions for many of these challenges, but they depend upon having rich task models. In this work we detail a novel type of Hierarchical Task Network we call a Clique/Chain HTN (CC-HTN), alongside an algorithm for autonomously constructing them from topological properties derived from graphical task representations. As the presented method relies on the structure of the task itself, our work imposes no particular type of symbolic insight into motor primitives or environmental representation, making it applicable to a wide variety of use cases critical to human-robot interaction. We present evaluations within a multi-resolution goal inference task and a transfer learning application showing the utility of our approach.},
author = {Hayes, Bradley and Scassellati, Brian},
doi = {10.1109/ICRA.2016.7487760},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings - IEEE International Conference on Robotics and Automation/Hayes, Scassellati/Hayes, Scassellati - 2016 - Autonomously constructing hierarchical task networks for planning and human-robot collabor.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {5469--5476},
title = {{Autonomously constructing hierarchical task networks for planning and human-robot collaboration}},
year = {2016}
}
@article{Kaplan2004,
abstract = {This chapter presents a generic internal reward system that drives an agent to increase the complexity of its behavior. This reward system does not reinforce a predefined task. Its purpose is to drive the agent to progress in learning given its embodiment and the environment in which it is placed. The dynamics created by such a system are studied first in a simple environment and then in the context of active vision.},
author = {Kaplan, Frederic and Oudeyer, Pierre-Yves},
doi = {10.1007/b99075},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Embodied artificial intelligence/Kaplan, Oudeyer/Kaplan, Oudeyer - 2004 - Maximizing learning progress An internal reward system for development.pdf:pdf},
isbn = {3-540-22484-X},
issn = {03029743},
journal = {Embodied artificial intelligence},
keywords = {Active vision,Artificial intelligence,Reward},
pages = {259--270},
title = {{Maximizing learning progress: An internal reward system for development}},
volume = {3139},
year = {2004}
}
@article{Vien2016,
abstract = {Reinforcement learning (RL) is an area of machine learning that is concerned with how an agent learns to make decisions sequentially in order to optimize a par-ticular performance measure. For achieving such a goal, the agent has to choose either 1) exploiting previously known knowledge that might end up at local optimality or 2) exploring to gather new knowledge that expects to improve the current performance. Among other RL algo-rithms, Bayesian model-based RL (BRL) is well-known to be able to trade-off between exploitation and explo-ration optimally via belief planning, i.e. partially observ-able Markov decision process (POMDP). However, solving that POMDP often suffers from curse of dimensionality and curse of history. In this paper, we make two major contributions which are: 1) an integration framework of temporal abstraction into BRL that eventually results in a hierarchical POMDP formulation, which can be solved online using a hierarchical sample-based planning solver; 2) a subgoal discovery method for hierarchical BRL that automatically discovers useful macro actions to accelerate learning. In the experiment section, we demonstrate that the proposed approach can scale up to much larger prob-lems. On the other hand, the agent is able to discover useful subgoals for speeding up Bayesian reinforcement learning.},
author = {Vien, Ngo Anh and Lee, Seung Gwan and Chung, Tae Choong},
doi = {10.1007/s10489-015-0742-2},
file = {::},
issn = {15737497},
journal = {Applied Intelligence},
keywords = {Bayesian reinforcement learning,Hierarchical Monte-Carlo planning,Hierarchical reinforcement learning,MDP,Monte-Carlo tree search,POMDP,POSMDP,Reinforcement learning},
number = {1},
pages = {112--126},
publisher = {Applied Intelligence},
title = {{Bayes-adaptive hierarchical MDPs}},
url = {http://dx.doi.org/10.1007/s10489-015-0742-2},
volume = {45},
year = {2016}
}
@article{Andre2002,
abstract = {Hierarchical reinforcement learning ALisp - a language to support represent hierarchical model. The authors present way to organize information in hierarchical fashion. In which, at each level, each state contains only necessary information for RL, and speed up learning process then. That state is called abstract state in the paper.},
author = {Andre, David and Russell, Stuart},
file = {::},
isbn = {0-262-51129-0},
issn = {1049-5258},
journal = {Aaai},
number = {Dietterich 2000},
pages = {119--125},
title = {{State Abstraction for Programmable Reinforcement Learning Agents}},
url = {http://www.aaai.org/Papers/AAAI/2002/AAAI02-019.pdf},
year = {2002}
}
@article{Vezhnevets2017a,
abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.},
archivePrefix = {arXiv},
arxivId = {1703.01161},
author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
eprint = {1703.01161},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Vezhnevets et al/Vezhnevets et al. - 2017 - FeUdal Networks for Hierarchical Reinforcement Learning.pdf:pdf},
issn = {1938-7228},
title = {{FeUdal Networks for Hierarchical Reinforcement Learning}},
url = {http://arxiv.org/abs/1703.01161},
year = {2017}
}
@inproceedings{Alexander2016,
abstract = {We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub- sequences by learning for how long the plan can be committed to - i.e. followed without re-planing. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro- actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.},
archivePrefix = {arXiv},
arxivId = {1606.04695},
author = {Vezhnevets, Alexander and Mnih, Volodymyr and Agapiou, John and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Kavukcuoglu, Koray},
booktitle = {Proceeding of NIPS 2016},
doi = {10.3847/0004-637X/832/1/56},
eprint = {1606.04695},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/arXiv/Alexander et al/Alexander et al. - 2016 - Strategic Attentive Writer for Learning Macro-Actions.pdf:pdf},
issn = {10495258},
title = {{Strategic Attentive Writer for Learning Macro-Actions}},
url = {http://arxiv.org/abs/1606.04695},
year = {2016}
}
@article{Bai2017a,
abstract = {In the context of hierarchical reinforcement learn-ing, the idea of hierarchies of abstract machines (HAMs) is to write a partial policy as a set of hierar-chical finite state machines with unspecified choice states, and use reinforcement learning to learn an optimal completion of this partial policy. Given a HAM with deep hierarchical structure, there of-ten exist many internal transitions where a machine calls another machine with the environment state unchanged. In this paper, we propose a new hier-archical reinforcement learning algorithm that au-tomatically discovers such internal transitions, and shortcircuits them recursively in the computation of Q values. The resulting HAMQ-INT algorithm outperforms the state of the art significantly on the benchmark Taxi domain and a much more complex RoboCup Keepaway domain.},
author = {Bai, Aijun and Russell, Stuart},
file = {::},
isbn = {9780999241103},
issn = {10450823},
journal = {Proc. of the 26th International Joint Conference on Artificial Intelligence},
keywords = {2017,HAM,HRL,Machine Learning: Reinforcement Learning,Planning and Scheduling: Markov Decisions Processe,RL,Robotics and Vision: Multi-Robot Systems,Robotics and Vision: Robotics},
mendeley-tags = {2017,HAM,HRL,RL},
pages = {1418--1424},
title = {{Efficient Reinforcement Learning with Hierarchies of Machines by Leveraging Internal Transitions}},
year = {2017}
}
@article{Digney1998,
abstract = {While the need for hierarchies within control systems is apparent, it is also clear to many researchers that such hierarchies should be learned. Learning both the structure and the component behaviors is a difficult task. The benefit of learning the hierarchical structures of behaviors is that the decomposition of the control structure into smaller transportable chunks allows previously learned knowledge to be applied to new but related tasks. Presented in this paper are improvements to Nested Q-learning (NQL) that allow more realistic learning of control hierarchies in reinforcement environments. Also presented is a simulation of a simple robot performing a series of related tasks that is used to compare both hierarchical and non-hierarchal learning techniques.},
author = {Digney, Bruce L},
file = {::},
isbn = {0-262-66144-6},
journal = {From Animals to Animats 5: Proceedings of the Fifth International Conference on the Simulation of Adaptive Behavior},
keywords = {Options,hierarchical skills,sub-goal,subgoal},
pages = {321--330},
title = {{Learning Hierarchical Control Structures for Multiple Tasks and Changing Environments}},
volume = {5},
year = {1998}
}
@article{Lee2018a,
abstract = {Value Iteration Networks (VINs) are effective differentiable path planning modules that can be used by agents to perform navigation while still maintaining end-to-end differentiability of the entire architecture. Despite their effectiveness, they suffer from several disadvantages including training instability, random seed sensitivity, and other optimization problems. In this work, we reframe VINs as recurrent-convolutional networks which demonstrates that VINs couple recurrent convolutions with an unconventional max-pooling activation. From this perspective, we argue that standard gated recurrent update equations could potentially alleviate the optimization issues plaguing VIN. The resulting architecture, which we call the Gated Path Planning Network, is shown to empirically outperform VIN on a variety of metrics such as learning speed, hyperparameter sensitivity, iteration count, and even generalization. Furthermore, we show that this performance gap is consistent across different maze transition types, maze sizes and even show success on a challenging 3D environment, where the planner is only provided with first-person RGB images.},
archivePrefix = {arXiv},
arxivId = {1806.06408},
author = {Lee, Lisa and Parisotto, Emilio and Chaplot, Devendra Singh and Xing, Eric and Salakhutdinov, Ruslan},
eprint = {1806.06408},
file = {::},
issn = {1938-7228},
title = {{Gated Path Planning Networks}},
url = {http://arxiv.org/abs/1806.06408},
year = {2018}
}
@article{Dietterich1998,
abstract = {This paper presents a new approach to hier- archical reinforcement learning based on the MAXQ decomposition of the value function. The MAXQ decomposition has both a procedu- ral semantics—as a subroutine hierarchy—and a declarative semantics—as a representation of the value function of a hierarchical policy. MAXQ unifies and extends previouswork on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. Conditions under which the MAXQ decomposition can represent the optimal value function are derived. The paper defines a hierarchicalQlearning algorithm, proves its con- vergence, and shows experimentally that it can learn much faster than ordinary “flat” Q learn- ing. Finally, the paper discusses some interest- ing issues that arise in hierarchical reinforcement learning including the hierarchical credit assign- ment problem and non-hierarchical execution of theMAXQ hierarchy.},
author = {Dietterich, Thomas G},
file = {::},
isbn = {1-55860-556-8},
journal = {Proc. of the fifteenth international conference on machine learning},
number = {c},
pages = {118--126},
title = {{The MAXQ Method for Hierarchical Reinforcement Learning}},
year = {1998}
}
@article{Gupta2017,
abstract = {We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the planner, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. Our experiments demonstrate that CMP outperforms both reactive strategies and standard memory-based architectures and performs well in novel environments. Furthermore, we show that CMP can also achieve semantically specified goals, such as "go to a chair".},
archivePrefix = {arXiv},
arxivId = {1702.03920},
author = {Gupta, Saurabh and Davidson, James and Levine, Sergey and Sukthankar, Rahul and Malik, Jitendra},
eprint = {1702.03920},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/ArXiv 1702.03920/Gupta et al/Gupta et al. - 2017 - Cognitive Mapping and Planning for Visual Navigation.pdf:pdf},
journal = {ArXiv: 1702.03920},
month = {feb},
title = {{Cognitive Mapping and Planning for Visual Navigation}},
url = {http://arxiv.org/abs/1702.03920},
year = {2017}
}
@article{Daniel2016,
abstract = {Tasks that require many sequential decisions are hard to solve using conventional rein- forcement learning algorithms. Based on the option framework, we propose a model which aims to alleviate these concerns. Instead of learning a single monolithic policy, the agent learns a set of simpler sub-policies as well as the initiation and termination probabili- ties for each of those sub-policies. While ex- isting option learning algorithms frequently require manual specification of components such as the sub-policies, we present an algo- rithm which infers all relevant components of the option framework from data. We present results on SMDPs with discrete as well as continuous state-action spaces. The results show that the presented algorithm can combine simple sub-policies to solve complex tasks and can improve learning performance on simpler tasks.},
author = {Daniel, Christian and van Hoof, Herke and Peters, Jan and Neumann, Gerhard},
doi = {10.1007/s10994-016-5580-x},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Machine Learning/Daniel et al/Daniel et al. - 2016 - Probabilistic inference for determining options in reinforcement learning.pdf:pdf},
issn = {15730565},
journal = {Machine Learning},
keywords = {Options,Reinforcement learning,Robot learning,Semi Markov decision process},
number = {2-3},
pages = {337--357},
publisher = {Springer US},
title = {{Probabilistic inference for determining options in reinforcement learning}},
volume = {104},
year = {2016}
}
@inproceedings{Rasmussen1998,
author = {Rasmussen, Daniel and Eliasmith, Chris},
booktitle = {Proceedings of the 36th Annual Conference of the Cognitive Science Society},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 36th Annual Conference of the Cognitive Science Society/Rasmussen, Eliasmith/Rasmussen, Eliasmith - 2014 - A neural model of hierarchical reinforcement learning.pdf:pdf},
keywords = {brain could achieve its,hierarchical,hrl into a theory,neural engineering framework,neural model,of neural function,providing a hypothe-,reinforcement learning,sis for how the,strengths in scaling},
number = {1},
pages = {1252--1257},
title = {{A neural model of hierarchical reinforcement learning}},
year = {2014}
}
@article{Frank2012,
abstract = {Growing evidence suggests that the prefrontal cortex (PFC) is organized hierarchically, with more anterior regions having increasingly abstract representations. How does this organization support hierarchical cognitive control and the rapid discovery of abstract action rules? We present computational models at different levels of description. A neural circuit model simulates interacting corticostriatal circuits organized hierarchically. In each circuit, the basal ganglia gate frontal actions, with some striatal units gating the inputs to PFC and others gating the outputs to influence response selection. Learning at all of these levels is accomplished via dopaminergic reward prediction error signals in each corticostriatal circuit. This functionality allows the system to exhibit conditional if-then hypothesis testing and to learn rapidly in environments with hierarchical structure. We also develop a hybrid Bayesian-reinforcement learning mixture of experts (MoE) model, which can estimate the most likely hypothesis state of individual participants based on their observed sequence of choices and rewards. This model yields accurate probabilistic estimates about which hypotheses are attended by manipulating attentional states in the generative neural model and recovering them with the MoE model. This 2-pronged modeling approach leads to multiple quantitative predictions that are tested with functional magnetic resonance imaging in the companion paper.},
author = {Frank, Michael J. and Badre, David},
doi = {10.1093/cercor/bhr114},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Cerebral cortex (New York, N.Y. 1991)/Frank, Badre/Frank, Badre - 2012 - Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1 Computational analysis.pdf:pdf},
issn = {1460-2199},
journal = {Cerebral cortex (New York, N.Y. : 1991)},
keywords = {Computer Simulation,Corpus Striatum,Corpus Striatum: cytology,Corpus Striatum: physiology,Humans,Learning,Learning: physiology,Models,Neural Pathways,Neural Pathways: cytology,Neural Pathways: physiology,Neurological,Neurons,Neurons: physiology,Prefrontal Cortex,Prefrontal Cortex: cytology,Prefrontal Cortex: physiology,Reinforcement (Psychology)},
number = {3},
pages = {509--26},
pmid = {21693490},
title = {{Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: Computational analysis}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3278315{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {22},
year = {2012}
}
@article{Jaderberg2018,
abstract = {Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.},
archivePrefix = {arXiv},
arxivId = {1807.01281},
author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
doi = {arXiv:1807.01281},
eprint = {1807.01281},
file = {:C$\backslash$:/Users/panov/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaderberg et al. - 2018 - Human-level performance in first-person multiplayer games with population-based deep reinforcement learning(2).pdf:pdf},
title = {{Human-level performance in first-person multiplayer games with population-based deep reinforcement learning}},
url = {http://arxiv.org/abs/1807.01281},
year = {2018}
}
@article{Abel2018b,
abstract = {We consider the problem of how best to use prior experience to bootstrap lifelong learning, where an agent faces a series of task instances drawn from some task distribution. First, we identify the initial policy that optimizes expected performance over the distribution of tasks for increasingly complex classes of policy and task distributions. We empirically demonstrate the relative performance of each policy class' optimal element in a variety of simple task distributions. We then consider value-function initialization methods that preserve PAC guarantees while simultaneously minimizing the learning required in two learning algorithms, yielding MaxQInit, a practical new method for value-function-based transfer. We show that MaxQInit performs well in simple lifelong RL experiments.},
author = {Abel, David and Jinnai, Yuu and Guo, Sophie Yue and Konidaris, George and Littman, Michael},
file = {::},
journal = {Proceedings of the 35th International Conference on Machine Learning},
pages = {20--29},
title = {{Policy and Value Transfer in Lifelong Reinforcement Learning}},
url = {http://proceedings.mlr.press/v80/abel18b.html},
volume = {80},
year = {2018}
}
@article{Botvinick2012,
abstract = {The hierarchical structure of human and animal behavior has been of critical interest in neuroscience for many years. Yet understanding the neural processes that give rise to such structure remains an open challenge. In recent research, a new perspective on hierarchical behavior has begun to take shape, inspired by ideas from machine learning, and in particular the framework of hierarchical reinforcement learning. Hierarchical reinforcement learning builds on traditional reinforcement learning mechanisms, extending them to accommodate temporally extended behaviors or subroutines. The resulting computational paradigm has begun to influence both theoretical and empirical work in neuroscience, conceptually aligning the study of hierarchical behavior with research on other aspects of learning and decision making, and giving rise to some thought-provoking new findings. ?? 2012.},
author = {Botvinick, Matthew Michael},
doi = {10.1016/j.conb.2012.05.008},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Current Opinion in Neurobiology/Botvinick/Botvinick - 2012 - Hierarchical reinforcement learning and decision making.pdf:pdf},
isbn = {0818653302},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
number = {6},
pages = {956--962},
pmid = {22695048},
publisher = {Elsevier Ltd},
title = {{Hierarchical reinforcement learning and decision making}},
url = {http://dx.doi.org/10.1016/j.conb.2012.05.008},
volume = {22},
year = {2012}
}
@incollection{Hengst2012,
abstract = {Hierarchical decomposition tackles complex problems by reducing them to a smaller set of interrelated problems. The smaller problems are solved separately and the results re-combined to find a solution to the original problem. It is well known that the na{\"{i}}ve application of reinforcement learning (RL) techniques fails to scale to more complex domains. This Chapter introduces hierarchical approaches to reinforcement learning that hold out the promise of reducing a reinforcement learning problems to a manageable size. Hierarchical Reinforcement Learning (HRL) rests on finding good re-usable temporally extended actions that may also provide opportunities for state abstraction. Methods for reinforcement learning can be extended to work with abstract states and actions over a hierarchy of subtasks that decompose the original problem, potentially reducing its computational complexity. We use a four-room task as a running example to illustrate the various concepts and approaches, including algorithms that can automatically learn the hierarchical structure from interactions with the domain.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.02971},
author = {Hengst, Bernhard},
booktitle = {Reinforcement Learning},
doi = {10.1007/978-3-642-27645-3_9},
eprint = {arXiv:1509.02971},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Reinforcement Learning/Hengst/Hengst - 2012 - Hierarchical Approaches.pdf:pdf},
isbn = {978-3-642-27644-6},
issn = {18726240},
pages = {293--323},
title = {{Hierarchical Approaches}},
url = {http://dx.doi.org/10.1007/978-3-642-27645-3{\_}9 http://link.springer.com/10.1007/978-3-642-27645-3{\_}9},
year = {2012}
}
@article{Botvinick2009a,
abstract = {Research on human and animal behavior has long emphasized its hierarchical structure-the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior.},
author = {Botvinick, Matthew M. and Niv, Yael and Barto, Andrew C.},
doi = {10.1016/j.cognition.2008.08.011},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Cognition/Botvinick, Niv, Barto/Botvinick, Niv, Barto - 2009 - Hierarchically organized behavior and its neural foundations a reinforcement learning perspective.pdf:pdf},
issn = {1873-7838},
journal = {Cognition},
keywords = {Animals,Humans,Models,Nerve Net,Nerve Net: physiology,Prefrontal Cortex,Prefrontal Cortex: physiology,Problem Solving,Problem Solving: physiology,Psychological,Reinforcement (Psychology)},
number = {3},
pages = {262--80},
pmid = {18926527},
title = {{Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2783353{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {113},
year = {2009}
}
@article{Konidaris2012,
abstract = {We present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to significantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that significantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-specific skills.},
author = {Konidaris, George and Scheidwasser, Ilya and Barto, Andrew G.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Journal of Machine Learning Research/Konidaris, Scheidwasser, Barto/Konidaris, Scheidwasser, Barto - 2012 - Transfer in reinforcement learning via shared features.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {reinforcement learning,shaping,skills,transfer},
number = {1},
pages = {1333--1371},
title = {{Transfer in reinforcement learning via shared features}},
url = {http://dl.acm.org/citation.cfm?id=2503308.2343689{\%}5Cnhttp://dl.acm.org/ft{\_}gateway.cfm?id=2343689{\&}type=pdf},
volume = {13},
year = {2012}
}
@article{Rabinowitz2018a,
abstract = {Theory of mind (ToM) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We design a Theory of Mind neural network {\{}–{\}} a ToMnet {\{}–{\}} which uses meta-learning to build such models of the agents it encounters. The ToMnet learns a strong prior model for agents' future behaviour, and, using only a small number of behavioural observations, can bootstrap to richer predictions about agents' characteristics and mental states. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep RL agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test of recognising that others can hold false beliefs about the world.},
archivePrefix = {arXiv},
arxivId = {1802.07740},
author = {Rabinowitz, Neil and Perbet, Frank and Song, Francis and Zhang, Chiyuan and Eslami, S M Ali and Botvinick, Matthew},
eprint = {1802.07740},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Rabinowitz et al/Rabinowitz et al. - 2018 - Machine Theory of Mind.pdf:pdf},
issn = {1938-7228},
journal = {Proceedings of the 35th International Conference on Machine Learning},
pages = {4215--4224},
title = {{Machine Theory of Mind}},
url = {http://proceedings.mlr.press/v80/rabinowitz18a.html},
volume = {80},
year = {2018}
}
@inproceedings{Barto2004,
abstract = {Humans and other animals often engage in activities for their own sakes rather than as steps toward solving practical problems. Psychologists call these intrinsically motivated behaviors. What we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy. At the core of the model are recent theoretical and algorithmic advances in computational reinforcement learning, specifically, new concepts related to skills and new learning algorithms for learning with skill hierarchies. 1},
author = {Barto, Andrew G. and Singh, Satinder},
booktitle = {Proceedings of the 3rd International Conference on Development and Learning (ICDL 2004)},
doi = {10.1.1.123.395},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 3rd International Conference on Development and Learning (ICDL 2004)/Barto, Singh/Barto, Singh - 2004 - Intrinsically motivated learning of hierarchical collections of skills.pdf:pdf},
pages = {112--119},
title = {{Intrinsically motivated learning of hierarchical collections of skills}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.117.6436{\%}5Cnhttp://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A083E47D2FE080D11716EC249E464BE2?doi=10.1.1.117.6436{\&}rep=rep1{\&}type=pdf},
year = {2004}
}
@article{Dann2017a,
abstract = {In platform videogames, players are frequently tasked with solving medium-term navigation prob-lems in order to gather items or powerups. Arti-ficial agents must generally obtain some form of direct experience before they can solve such tasks. Experience is gained either through training runs, or by exploiting knowledge of the game's physics to generate detailed simulations. Human players, on the other hand, seem to look ahead in high-level, abstract steps. Motivated by human play, we intro-duce an approach that leverages not only abstract " skills " , but also knowledge of what those skills can and cannot achieve. We apply this approach to Infinite Mario, where despite facing randomly generated, maze-like levels, our agent is capable of deriving complex plans in real-time, without rely-ing on perfect knowledge of the game's physics.},
author = {Dann, Michael and Zambetta, Fabio and Thangarajah, John},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IJCAI International Joint Conference on Artificial Intelligence/Dann, Zambetta, Thangarajah/Dann, Zambetta, Thangarajah - 2017 - Real-time navigation in classical platform games via skill reuse.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning: Reinforcement Learning,Multidisciplinary Topics and Applications: Compute},
pages = {1582--1588},
title = {{Real-time navigation in classical platform games via skill reuse}},
year = {2017}
}
@article{Verma2018a,
abstract = {We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural "oracle". We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.},
archivePrefix = {arXiv},
arxivId = {1804.02477},
author = {Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
eprint = {1804.02477},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Verma et al/Verma et al. - 2018 - Programmatically Interpretable Reinforcement Learning.pdf:pdf},
issn = {1938-7228},
title = {{Programmatically Interpretable Reinforcement Learning}},
url = {http://arxiv.org/abs/1804.02477},
year = {2018}
}
@inproceedings{Shu2017,
abstract = {Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task corresponds to a human language description. Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices. In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills. We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.},
archivePrefix = {arXiv},
arxivId = {1712.07294},
author = {Shu, Tianmin and Xiong, Caiming and Socher, Richard},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
doi = {10.1051/0004-6361/201527329},
eprint = {1712.07294},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Shu, Xiong, Socher/Shu, Xiong, Socher - 2017 - Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pmid = {23459267},
title = {{Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning}},
url = {http://arxiv.org/abs/1712.07294},
year = {2017}
}
@inproceedings{Roderick2017,
abstract = {We examine the problem of learning and planning on high-dimensional domains with long horizons and sparse rewards. Recent approaches have shown great successes in many Atari 2600 domains. However, domains with long horizons and sparse rewards, such as Montezuma's Revenge and Venture, remain challenging for existing methods. Methods using abstraction (Dietterich 2000; Sutton, Precup, and Singh 1999) have shown to be useful in tackling long-horizon problems. We combine recent techniques of deep reinforcement learning with existing model-based approaches using an expert-provided state abstraction. We construct toy domains that elucidate the problem of long horizons, sparse rewards and high-dimensional inputs, and show that our algorithm significantly outperforms previous methods on these domains. Our abstraction-based approach outperforms Deep Q-Networks (Mnih et al. 2015) on Montezuma's Revenge and Venture, and exhibits backtracking behavior that is absent from previous methods.},
archivePrefix = {arXiv},
arxivId = {1710.00459},
author = {Roderick, Melrose and Grimm, Christopher and Tellex, Stefanie},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
eprint = {1710.00459},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Roderick, Grimm, Tellex/Roderick, Grimm, Tellex - 2017 - Deep Abstract Q-Networks.pdf:pdf},
title = {{Deep Abstract Q-Networks}},
url = {http://arxiv.org/abs/1710.00459},
year = {2017}
}
@article{Jong2008,
abstract = {Hierarchical decomposition promises to help scale reinforcement learning algorithms naturally to real-world problems by exploiting their under- lying structure. Model-based algorithms, which provided the first finite-time convergence guaran- tees for reinforcement learning, may also play an important role in copingwith the relative scarcity of data in large environments. In this paper, we introduce an algorithm that fully integrates mod- ern hierarchical and model-learning methods in the standard reinforcement learning setting. Our algorithm, R-MAXQ, inherits the efficientmodel- based exploration of the R-MAX algorithm and the opportunities for abstraction provided by the MAXQframework. We analyze the sample com- plexity of our algorithm, and our experiments in a standard simulation environment illustrate the advantages of combining hierarchies andmodels},
author = {Jong, Nicholas K and Stone, Peter},
doi = {10.1145/1390156.1390211},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/25th International Conference on Machine Learning/Jong, Stone/Jong, Stone - 2008 - Hierarchical Model-Based Reinforcement Learning R- MAX MAXQ.pdf:pdf},
isbn = {9781605582054},
journal = {25th International Conference on Machine Learning},
number = {July},
title = {{Hierarchical Model-Based Reinforcement Learning : R- MAX + MAXQ}},
year = {2008}
}
@inproceedings{Jonsson2001,
abstract = {Learning a complex task can be significantly facilitated by defining a hierarchy of subtasks. An agent can learn to choose between various temporally abstract actions, each solving an assigned subtask, to accomplish the overall task. In this paper, we study hierarchical learning using the framework of options. We argue that to take full advantage of hierarchical structure, one should perform option-specific state abstraction, and that if this is to scale to larger tasks, state abstraction ...},
author = {Jonsson, Anders and Barto, Andrew G.},
booktitle = {Proceedings of NIPS 2001},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of NIPS 2001/Jonsson, Barto/Jonsson, Barto - 2001 - Automated State Abstraction for Options using the U-Tree Algorithm.pdf:pdf},
isbn = {0262122413},
issn = {1049-5258},
pages = {1054--1060},
title = {{Automated State Abstraction for Options using the U-Tree Algorithm}},
year = {2001}
}
@article{Singh2010,
abstract = {There is great interest in building intrinsic motivation into artificial systems using the reinforcement learning framework. Yet, what intrinsic motivation may mean computationally, and how it may differ from extrinsic motivation, remains a murky and controversial subject. In this paper, we adopt an evolutionary perspective and define a new optimal reward framework that captures the pressure to design good primary reward functions that lead to evolutionary success across environments. The results of two computational experiments show that optimal primary reward signals may yield both emergent intrinsic and extrinsic motivation. The evolutionary perspective and the associated optimal reward framework thus lead to the conclusion that there are no hard and fast features distinguishing intrinsic and extrinsic reward computationally. Rather, the directness of the relationship between rewarding behavior and evolutionary success varies along a continuum.},
author = {Singh, Satinder and Lewis, Richard L. and Barto, Andrew G. and Sorg, Jonathan},
doi = {10.1109/TAMD.2010.2051031},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IEEE Transactions on Autonomous Mental Development/Singh et al/Singh et al. - 2010 - Intrinsically Motivated Reinforcement Learning An Evolutionary Perspective.pdf:pdf},
isbn = {0262195348, 9780262195348},
issn = {1943-0604},
journal = {IEEE Transactions on Autonomous Mental Development},
number = {2},
pages = {70--82},
title = {{Intrinsically Motivated Reinforcement Learning: An Evolutionary Perspective}},
url = {http://ieeexplore.ieee.org/document/5471106/},
volume = {2},
year = {2010}
}
@article{Sutton2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sutton, Richard S. and Singh, Satinder},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Artificial Intelligence/Sutton, Precup, Singh/Sutton, Precup, Singh - 1999 - Between MDPs and Semi-MDPs A Framework for Temporal Abstraction in Reinforcement Learning.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {PhD Proposal},
keywords = {Option},
number = {1999},
pages = {181--211},
pmid = {25246403},
title = {{Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning}},
volume = {1},
year = {1999}
}
@article{Nichol2018,
abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
archivePrefix = {arXiv},
arxivId = {1803.02999},
author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
eprint = {1803.02999},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Nichol, Achiam, Schulman/Nichol, Achiam, Schulman - 2018 - On First-Order Meta-Learning Algorithms.pdf:pdf},
pages = {1--15},
title = {{On First-Order Meta-Learning Algorithms}},
url = {http://arxiv.org/abs/1803.02999},
year = {2018}
}
@article{Tamar2016,
abstract = {We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.},
archivePrefix = {arXiv},
arxivId = {1602.02867},
author = {Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
eprint = {1602.02867},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/arXiv/Tamar et al/Tamar et al. - 2016 - Value Iteration Networks.pdf:pdf},
journal = {arXiv},
month = {feb},
pages = {1--14},
title = {{Value Iteration Networks}},
url = {http://arxiv.org/abs/1602.02867},
year = {2016}
}
@article{Parr1998,
abstract = {We present a new approach to reinforcement learning in which the poli- cies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework inwhich knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learn- ing and “behavior-based” or “teleo-reactive” approaches to control. We present provably convergent algorithms for problem-solving and learn- ing with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states. 1},
author = {Parr, Ronald and Russell, Stuart},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Neural Information Processing Systems (NIPS)/Parr, Russell/Parr, Russell - 1998 - Reinforcement learning with hierarchies of machines.pdf:pdf},
isbn = {0-262-10076-2},
issn = {0031-8116},
journal = {Neural Information Processing Systems (NIPS)},
pages = {1043--1049},
title = {{Reinforcement learning with hierarchies of machines}},
url = {http://www.cs.berkeley.edu/{~}russell/classes/cs294/s11/readings/Parr+Russell:1998.pdf},
year = {1998}
}
@article{Silver2013,
abstract = {Lifelong Machine Learning Systems: Beyond Learning Algorithms},
author = {Silver, Daniel L and Yang, Qiang and Li, Lianghao},
file = {::},
isbn = {9781577356028},
journal = {AAAI Spring Symposium Series},
keywords = {AAAI Technical Report SS-13-05},
number = {Solomonoff 1989},
pages = {49--55},
title = {{Lifelong Machine Learning Systems : Beyond Learning Algorithms}},
year = {2013}
}
@article{Simsek2005,
abstract = {We present a new subgoal-based method for automatically creating useful skills in reinforcement learning. Our method identifies subgoals by partitioning local state transition graphs—those that are constructed using only the most recent experiences of the agent. The local scope of our subgoal discovery method allows it to successfully identify the type of subgoals we seek—states that lie between two densely-connected regions of the state space—while producing an algorithm with low computational cost.},
author = {Şimşek, {\"{O}}zg{\"{u}}r and Wolfe, Alicia P. and Barto, Andrew G.},
doi = {10.1145/1102351.1102454},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 22nd international conference on Machine learning - ICML '05/Şimşek, Wolfe, Barto/Şimşek, Wolfe, Barto - 2005 - Identifying useful subgoals in reinforcement learning by local graph partitio.pdf:pdf},
isbn = {1595931805},
journal = {Proceedings of the 22nd international conference on Machine learning  - ICML '05},
pages = {816--823},
title = {{Identifying useful subgoals in reinforcement learning by local graph partitioning}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102454},
year = {2005}
}
@article{Botvinick2009a,
abstract = {Research on human and animal behavior has long emphasized its hierarchical structure-the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior.},
author = {Botvinick, Matthew M. and Niv, Yael and Barto, Andrew C.},
doi = {10.1016/j.cognition.2008.08.011},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Cognition/Botvinick, Niv, Barto/Botvinick, Niv, Barto - 2009 - Hierarchically organized behavior and its neural foundations a reinforcement learning perspective.pdf:pdf},
issn = {1873-7838},
journal = {Cognition},
keywords = {Animals,Humans,Models,Nerve Net,Nerve Net: physiology,Prefrontal Cortex,Prefrontal Cortex: physiology,Problem Solving,Problem Solving: physiology,Psychological,Reinforcement (Psychology)},
number = {3},
pages = {262--80},
pmid = {18926527},
title = {{Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2783353{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {113},
year = {2009}
}
@article{Riedmiller2018,
abstract = {We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.},
archivePrefix = {arXiv},
arxivId = {1802.10567},
author = {Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and {Van de Wiele}, Tom and Mnih, Volodymyr and Heess, Nicolas and Springenberg, Jost Tobias},
eprint = {1802.10567},
file = {::},
issn = {1938-7228},
number = {table 1},
title = {{Learning by Playing - Solving Sparse Reward Tasks from Scratch}},
url = {http://arxiv.org/abs/1802.10567},
volume = {48},
year = {2018}
}
@article{Fox2017,
abstract = {Augmenting an agent's control with useful higher-level behaviors called options can greatly reduce the sample complexity of reinforcement learning, but manually designing options is infeasible in high-dimensional and abstract state spaces. While recent work has proposed several techniques for automated option discovery, they do not scale to multi-level hierarchies and to expressive representations such as deep networks. We present Discovery of Deep Options (DDO), a policy-gradient algorithm that discovers parametrized options from a set of demonstration trajectories, and can be used recursively to discover additional levels of the hierarchy. The scalability of our approach to multi-level hierarchies stems from the decoupling of low-level option discovery from high-level meta-control policy learning, facilitated by under-parametrization of the high level. We demonstrate that using the discovered options to augment the action space of Deep Q-Network agents can accelerate learning by guiding exploration in tasks where random actions are unlikely to reach valuable states. We show that DDO is effective in adding options that accelerate learning in 4 out of 5 Atari RAM environments chosen in our experiments. We also show that DDO can discover structure in robot-assisted surgical videos and kinematics that match expert annotation with 72{\%} accuracy.},
archivePrefix = {arXiv},
arxivId = {1703.08294},
author = {Fox, Roy and Krishnan, Sanjay and Stoica, Ion and Goldberg, Ken},
eprint = {1703.08294},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Fox et al/Fox et al. - 2017 - Multi-Level Discovery of Deep Options.pdf:pdf},
isbn = {9781510827806},
title = {{Multi-Level Discovery of Deep Options}},
url = {http://arxiv.org/abs/1703.08294},
year = {2017}
}
@incollection{Castro2012,
abstract = {Abstract. Temporally extended actions are usually effective in speeding up reinforcement learning. In this paper we present a mechanism for automatically constructing such actions, expressed as options [Sutton et al., 1999], in a finite Markov Decision Process (MDP). To do this, we compute a bisimulation metric [Ferns et al., 2004] between the states in a small MDP and the states in a large MDP, which we want to solve. The shape of this metric is then used to completely define a set of options for the large MDP. We demonstrate empirically that our approach is able to improve the speed of reinforcement learning, and is generally not sensitive to parameter tuning.},
author = {Castro, Pablo Samuel and Precup, Doina},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-29946-9_16},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Castro, Precup/Castro, Precup - 2012 - Automatic Construction of Temporally.pdf:pdf},
isbn = {9783642299452},
issn = {03029743},
pages = {140--152},
title = {{Automatic Construction of Temporally Extended Actions for MDPs Using Bisimulation Metrics}},
url = {http://link.springer.com/10.1007/978-3-642-29946-9{\_}16},
volume = {7188 LNAI},
year = {2012}
}
@article{Florensa2017a,
abstract = {Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.},
archivePrefix = {arXiv},
arxivId = {1705.06366},
author = {Florensa, Carlos and Held, David and Geng, Xinyang and Abbeel, Pieter},
doi = {arXiv:1705.06366v3},
eprint = {1705.06366},
file = {::},
isbn = {076453601X},
issn = {1938-7228},
title = {{Automatic Goal Generation for Reinforcement Learning Agents}},
url = {http://arxiv.org/abs/1705.06366},
year = {2017}
}
@inproceedings{Mehta2008b,
abstract = {We present an algorithm, HI-MAT (Hierar- chy Induction via Models And Trajectories), that discovers MAXQ task hierarchies by ap- plying dynamic Bayesian network models to a successful trajectory from a source rein- forcement learning task. HI-MAT discovers subtasks by analyzing the causal and tem- poral relationships among the actions in the trajectory. Under appropriate assumptions, HI-MAT induces hierarchies that are consis- tent with the observed trajectory and have compact value-function tables employing safe state abstractions. We demonstrate empir- ically that HI-MAT constructs compact hi- erarchies that are comparable to manually- engineered hierarchies and facilitate signifi- cant speedup in learning when transferred to a target task.},
address = {New York, New York, USA},
author = {Mehta, Neville and Ray, Soumya and Tadepalli, Prasad and Dietterich, Thomas},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390238},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 25th international conference on Machine learning - ICML '08/Mehta et al/Mehta et al. - 2008 - Automatic discovery and transfer of MAXQ hierarchies.pdf:pdf},
isbn = {9781605582054},
pages = {648--655},
publisher = {ACM Press},
title = {{Automatic discovery and transfer of MAXQ hierarchies}},
url = {http://portal.acm.org/citation.cfm?id=1390238 http://portal.acm.org/citation.cfm?doid=1390156.1390238},
year = {2008}
}
@article{Levine2016,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
archivePrefix = {arXiv},
arxivId = {1504.00702},
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1504.00702},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Journal of Machine Learning Research/Levine et al/Levine et al. - 2016 - End-to-End Training of Deep Visuomotor Policies.pdf:pdf},
isbn = {9781479969227},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Neural Networks,Optimal Control,Reinforcement Learning,Vision},
pages = {1--40},
pmid = {15003161},
title = {{End-to-End Training of Deep Visuomotor Policies}},
volume = {17},
year = {2016}
}
@inproceedings{MacGlashan2010,
abstract = {We present skill bootstrapping, a proposed new research direction for agent learning and planning that allows an agent to start with low-level primitive actions, and develop skills that can be used for higher-level planning. Skills are developed over the course of solving many different problems in a domain, using reinforcement learning techniques to complement the benefits and disadvantages of heuristic-search planning. We describe the overall architecture of the proposed approach, discuss how it relates to other work, and give motivating examples for why this approach would be successful.},
author = {MacGlashan, James and DesJArdins, Marie},
booktitle = {24th AAAI Conference on Artificial Intelligence},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/24th AAAI Conference on Artificial Intelligence/MacGlashan, DesJArdins/MacGlashan, DesJArdins - 2010 - Hierarchical Skill Learning for High-Level Planning.pdf:pdf;:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/24th AAAI Conference on Artificial Intelligence/MacGlashan, DesJArdins/MacGlashan, DesJArdins - 2010 - Hierarchical Skill Learning for High-Level Planning.pdf:pdf},
isbn = {9781577354666},
keywords = {abstraction,approximation,planning,reinforcement learning},
pages = {1988--1989},
title = {{Hierarchical Skill Learning for High-Level Planning}},
volume = {3},
year = {2010}
}
@article{Cao2012,
abstract = {We describe an approach to incorporating Bayesian priors in the MAXQ framework for hierarchical reinforcement learning (HRL). We define priors on the primitive environment model and on task pseudo-rewards. Since models for composite tasks can be complex, we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy. We show empirically that (i) our approach results in improved convergence over non-Bayesian baselines, (ii) using both task hierarchies and Bayesian priors is better than either alone, (iii) taking advantage of the task hierarchy reduces the computational cost of Bayesian reinforcement learning and (iv) in this framework, task pseudo-rewards can be learned instead of being manually specified, leading to hierarchically optimal rather than recursively optimal policies.},
author = {Cao, F and Ray, Soumya},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Advances in Neural Information Processing Systems/Cao, Ray/Cao, Ray - 2012 - Bayesian hierarchical reinforcement learning.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {73--81},
title = {{Bayesian hierarchical reinforcement learning}},
url = {http://papers.nips.cc/paper/4752-bay},
year = {2012}
}
@article{Dimakopoulou2018a,
abstract = {We consider a team of reinforcement learning agents that concurrently operate in a common environment, and we develop an approach to efficient coordinated exploration that is suitable for problems of practical scale. Our approach builds on seed sampling (Dimakopoulou and Van Roy, 2018) and randomized value function learning (Osband et al., 2016). We demonstrate that, for simple tabular contexts, the approach is competitive with previously proposed tabular model learning methods (Dimakopoulou and Van Roy, 2018). With a higher-dimensional problem and a neural network value function representation, the approach learns quickly with far fewer agents than alternative exploration schemes.},
archivePrefix = {arXiv},
arxivId = {1805.08948},
author = {Dimakopoulou, Maria and Osband, Ian and {Van Roy}, Benjamin},
doi = {10.1.1.151.8250},
eprint = {1805.08948},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Dimakopoulou, Osband, Van Roy/Dimakopoulou, Osband, Van Roy - 2018 - Scalable Coordinated Exploration in Concurrent Reinforcement Learning.pdf:pdf},
isbn = {0262042088},
issn = {1049-5258},
title = {{Scalable Coordinated Exploration in Concurrent Reinforcement Learning}},
url = {http://arxiv.org/abs/1805.08948},
year = {2018}
}
@phdthesis{Rasmussen2014,
author = {Rasmussen, Daniel},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Rasmussen/Rasmussen - 2014 - Hierarchical reinforcement learning in a biologically plausible neural architecture.pdf:pdf},
pages = {175},
school = {Unversetu of Waterloo},
title = {{Hierarchical reinforcement learning in a biologically plausible neural architecture}},
year = {2014}
}
@article{Frans2017,
abstract = {We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives---policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.},
archivePrefix = {arXiv},
arxivId = {1710.09767},
author = {Frans, Kevin and Ho, Jonathan and Chen, Xi and Abbeel, Pieter and Schulman, John},
eprint = {1710.09767},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Frans et al/Frans et al. - 2017 - Meta Learning Shared Hierarchies.pdf:pdf},
pages = {1--11},
title = {{Meta Learning Shared Hierarchies}},
url = {http://arxiv.org/abs/1710.09767},
year = {2017}
}
@inproceedings{Goel2017,
author = {Goel, Karan and Mu, Tong and Brunskill, Emma},
booktitle = {Hierarchical RL Workshop, NIPS 2017},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Hierarchical RL Workshop, NIPS 2017/Goel, Mu, Brunskill/Goel, Mu, Brunskill - 2017 - Optimal Hierarchical Policy Extraction From Noisy Imperfect Demonstrations.pdf:pdf},
title = {{Optimal Hierarchical Policy Extraction From Noisy Imperfect Demonstrations}},
url = {https://drive.google.com/file/d/101FsZkczKMfGeUBTP-089mhkTeepj8IW/view},
year = {2017}
}
@inproceedings{Singh2005,
author = {Singh, Satinder and Barto, Andrew G. and Chentanez, Nuttapong},
booktitle = {Proceeding of NIPS 2005},
doi = {10.1.1.123.395},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceeding of NIPS 2005/Singh, Barto, Chentanez/Singh, Barto, Chentanez - 2005 - Intrinsically motivated reinforcement learning.pdf:pdf},
pages = {1281--1288},
title = {{Intrinsically motivated reinforcement learning}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2005{\_}724.pdf},
year = {2005}
}
@article{Rasmussen2017a,
abstract = {We present the first model capable of performing hierarchical reinforcement learning in a general, neurally detailed imple-mentation. We show that this model is able to learn a spatial pickup and delivery task more quickly than one without hier-archical abilities. In addition, we show that this model is able to leverage its hierarchical structure to transfer learned knowl-edge between related tasks. These results point towards the advantages to be gained by using a hierarchical RL framework to understand the brain's powerful learning ability.},
author = {Rasmussen, Daniel and Voelker, Aaron and Eliasmith, Chris},
doi = {10.1371/journal.pone.0180234},
editor = {Cymbalyuk, Gennady},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/PLOS ONE/Rasmussen, Voelker, Eliasmith/Rasmussen, Voelker, Eliasmith - 2017 - A neural model of hierarchical reinforcement learning.pdf:pdf},
isbn = {1111111111},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {brain could achieve its,hierarchical,hrl into a theory,neural engineering framework,neural model,of neural function,providing a hypothe-,reinforcement learning,sis for how the,strengths in scaling},
month = {jul},
number = {7},
pages = {e0180234},
title = {{A neural model of hierarchical reinforcement learning}},
url = {http://dx.plos.org/10.1371/journal.pone.0180234},
volume = {12},
year = {2017}
}
@article{Tanaka1997,
author = {Tanaka, Fumihide and Yamamura, Masayuki},
file = {::},
journal = {6th European Workshop on Learning Robots},
pages = {93--99},
title = {{An approach to lifelong reinforcement learning through multiple environments}},
year = {1997}
}
@article{Dietterich2000,
abstract = {This paper presents a new approach to hierarchical reinforcement learning based on de-composing the target Markov decision process MDP into a hierarchy o f smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The decomposition, known as the MAXQ decom-position, has both a procedural semantics|as a subroutine hierarchy|and a declarative semantics|as a representation of the value function of a hierarchical policy. MAXQ uniies and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. It is based on the assumption that the programmer can identify useful subgoals and deene subtasks that achieve these subgoals. By deening such subgoals, the programmer constrains the set of policies that need to be considered during reinforcement learning. The MAXQ v alue function decomposition can represent the value function of any policy that is consistent with the given hierarchy. The decomposition also creates oppor-tunities to exploit state abstractions, so that individual MDPs within the hierarchy can ignore large parts of the state space. This is important for the practical application of the method. This paper deenes the MAXQ hierarchy, proves formal results on its representa-tional power, and establishes sve conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges with probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the eve kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q with state abstractions converges to a recursively optimal policy much faster than nat Q learning. The fact that MAXQ learns a representation of the value function has an important beneet: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the eeectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoos in hierarchical reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:cs/9905014v1},
author = {Dietterich, Thomas G},
doi = {10.1613/jair.639},
eprint = {9905014v1},
file = {::},
isbn = {978-3-540-67839-7},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {227--303},
primaryClass = {arXiv:cs},
title = {{Hierarchical reinforcement learning with the MAXQ value function decomp osition}},
volume = {13},
year = {2000}
}
@article{Mousavi2014,
abstract = {Reinforcement learning (RL) for solving large and complex problems faces the curse of dimensions problem. To overcome this problem, frameworks based on the temporal abstraction have been presented; each having their advantages and disadvantages. This paper proposes a new method like the strategies introduced in the hierarchical abstract machines (HAMs) to create a high-level controller layer of reinforcement learning which uses options. The proposed framework considers a non-deterministic automata as a controller to make a more effective use of temporally extended actions and state space clustering. This method can be viewed as a bridge between option and HAM frameworks, which tries to suggest a new framework to decrease the disadvantage of both by creating connection structures between them and at the same time takes advantages of them. Experimental results on different test environments show significant efficiency of the proposed method.},
author = {Mousavi, Seyed Sajad and Ghazanfari, Behzad and Mozayani, Nasser and Jahed-Motlagh, Mohammad Reza},
doi = {10.1016/j.asoc.2014.08.071},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Applied Soft Computing Journal/Mousavi et al/Mousavi et al. - 2014 - Automatic abstraction controller in reinforcement learning agent via automata.pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Cluster,Hierarchical reinforcement learning,Multi-agent learning,Reinforcement learning},
pages = {118--128},
publisher = {Elsevier B.V.},
title = {{Automatic abstraction controller in reinforcement learning agent via automata}},
url = {http://dx.doi.org/10.1016/j.asoc.2014.08.071},
volume = {25},
year = {2014}
}
@article{Mcgovern2002,
abstract = {The ability to create and to use abstractions in complex environemnts, that is, to systematically ignore irrelevant details, is a key reason that humans are effective problem solvers. Although the utility of abstraction is commonly accepted, there has been relatively little research on autonomously discovering or creating useful abstractions. A system that can create new abstractions autonomously can learn and plan in situations that its original designer was not able to anticipate. This dissertation introduces two related methods that allow an agent to autonomously discover and create temporal abstractions from its accumulated experience with its environment. A temporal abstraction is an encapsulation of a complex set of actions into a single higher-level action that allows an agent to learn and plan while ignoring details that appear at finer levels of temporal resolution. The main idea of both methods is to search for patterns that occur frequently within an agent's accumulated successful experience and that do not occur in unsuccessful experiences. These patterns are used to create the new temporal abstractions. The two types of temporal abstractions that our methods create are 1) suboals and closed-loop policies for achieving them, and 2) open-loop policies, or action sequences, that are useful "macros." We demonstrate the utility of both types of temporal abstractions in several simulated tasks, including two simulated mobile robot tasks. We use these tasks to demonstrate that the autonomously created temporal abstractions can both facilitate the learning of an agent within a task and can enable effective knowledge transfer related tasks. As a larger task, we focus on the difficult problem of scheduling the assembly instructions for computers with multiple pipelines in such a manner that the reordered instructions will execute as quickly as possible. We demonstrate that the autonomously discovered action sequences can significantly improve performance of the scheduler and can enable effective knowledge transfer across similar processors. Both methods can extract the temporal abstractions from collections of behavioral trajectories generated by different processes. In particular, we demonstrate that the methods can be effective when applied to collections generated by reinforcement learning agents, heuristic searchers, and human tele-operators.},
author = {Mcgovern, Elizabeth Amy},
file = {::},
isbn = {0-493-71665-3},
issn = {16113349},
journal = {Power},
number = {May},
title = {{Autonomous discovery of temporal abstractions from interaction with an environment}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.136.3079{\&}rep=rep1{\&}type=pdf},
year = {2002}
}
@incollection{Menache2002,
abstract = {We present the Q-Cut algorithm, a graph theoretic approach for automatic detection of sub-goals in a dynamic environment, which is used for acceleration of the Q-Learning algorithm. The learning agent creates an on-line map of the process history, and uses an efficient Max-Flow/Min-Cut algorithm for identifying bottlenecks. The policies for reaching bottlenecks are separately learned and added to the model in a form of options (macro-actions). We then extend the basic Q-Cut algorithm to the Segmented Q-Cut algorithm, which uses previously identified bottlenecks for state space partitioning, necessary for finding additional bottlenecks in complex environments. Experiments show significant performance improvements, particulary in the initial learning phase.},
author = {Menache, Ishai and Mannor, Shie and Shimkin, Nahum},
booktitle = {ECML 2002: Machine Learning: ECML 2002},
doi = {10.1007/3-540-36755-1_25},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/ECML 2002 Machine Learning ECML 2002/Menache, Mannor, Shimkin/Menache, Mannor, Shimkin - 2002 - Q-Cut—Dynamic Discovery of Sub-goals in Reinforcement Learning.pdf:pdf},
isbn = {978-3-540-44036-9, 978-3-540-36755-0},
issn = {16113349},
pages = {295--306},
title = {{Q-Cut—Dynamic Discovery of Sub-goals in Reinforcement Learning}},
url = {http://link.springer.com/10.1007/3-540-36755-1{\_}25},
year = {2002}
}
@article{Zanette2018,
abstract = {In order to make good decision under uncertainty an agent must learn from observations. To do so, two of the most common frameworks are Contextual Bandits and Markov Decision Processes (MDPs). In this paper, we study whether there exist algorithms for the more general framework (MDP) which automatically provide the best performance bounds for the specific problem at hand without user intervention and without modifying the algorithm. In particular, it is found that a very minor variant of a recently proposed reinforcement learning algorithm for MDPs already matches the best possible regret bound˜O bound˜ bound˜O(√ SAT) in the dominant term if deployed on a tabular Contextual Bandit problem despite the agent being agnostic to such setting.},
author = {Zanette, Andrea},
file = {::},
issn = {1938-7228},
journal = {Icml},
title = {{Problem Dependent Reinforcement Learning Bounds Which Can Identify Bandit Structure in MDPs}},
url = {http://proceedings.mlr.press/v80/zanette18a/zanette18a.pdf},
year = {2018}
}
@inproceedings{Mannor2004,
abstract = {We consider a graph theoretic approach for automatic construction of options in a dynamic environment. A map of the environment is generated on-line by the learning agent, representing the topological structure of the state transitions. A clustering algorithm is then used to partition the state space to different regions. Policies for reaching the different parts of the space are separately learned and added to the model in a form of options (macro-actions). The options are used for accelerating the Q-Learning algorithm. We extend the basic algorithm and consider building a map that includes preliminary indication of the location of "interesting" regions of the state space, where the value gradient is significant and additional exploration might be beneficial. Experiments indicate significant speedups, especially in the initial learning phase.},
address = {New York, New York, USA},
author = {Mannor, Shie and Menache, Ishai and Hoze, Amit and Klein, Uri},
booktitle = {Twenty-first international conference on Machine learning - ICML '04},
doi = {10.1145/1015330.1015355},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Twenty-first international conference on Machine learning - ICML '04/Mannor et al/Mannor et al. - 2004 - Dynamic abstraction in reinforcement learning via clustering.pdf:pdf},
isbn = {1581138285},
keywords = {clustering,hierarchical reinforcement learning,options,q-learning,reinforcement learning},
pages = {71},
publisher = {ACM Press},
title = {{Dynamic abstraction in reinforcement learning via clustering}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015355},
year = {2004}
}
@article{Abel2018a,
abstract = {In lifelong reinforcement learning, agents must effectively transfer knowledge across tasks while simultaneously addressing exploration, credit assignment, and generalization. State abstraction can help overcome these hurdles by compressing the representation used by an agent, thereby reducing the computational and statistical burdens of learning. To this end, we here develop theory to compute and use state abstractions in lifelong reinforcement learning. We introduce two new classes of abstractions: (1) transitive state abstractions, whose optimal form can be computed efficiently, and (2) PAC state abstractions, which are guaranteed to hold with respect to a distribution of tasks. We show that the joint family of transitive PAC abstractions can be acquired efficiently, preserve near optimal-behavior, and experimentally reduce sample complexity in simple domains, thereby yielding a family of desirable abstractions for use in lifelong reinforcement learning. Along with these positive results, we show that there are pathological cases where state abstractions can negatively impact performance.},
author = {Abel, David and Arumugam, Dilip and Lehnert, Lucas and Littman, Michael},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 35th International Conference on Machine Learning/Abel et al/Abel et al. - 2018 - State Abstractions for Lifelong Reinforcement Learning.pdf:pdf},
journal = {Proceedings of the 35th International Conference on Machine Learning},
pages = {10--19},
title = {{State Abstractions for Lifelong Reinforcement Learning}},
url = {http://proceedings.mlr.press/v80/abel18a.html},
volume = {80},
year = {2018}
}
@article{Dietterich2000a,
abstract = {Reinforcement learning addresses the problem of learning optimal policies for sequential decision-making problems involving stochastic operators and numerical reward functions rather than the more traditional deterministic operators and logical goal predicates. In many ways, reinforcement learning research is recapitulating the development of classical research in planning and problem solving. After studying the problem of solving flat problem spaces, researchers have recently turned their attention to hierarchical methods that incorporate subroutines and state abstractions. This paper gives an overview of the MAXQ value function decomposition and its support for state abstraction and action abstraction.},
archivePrefix = {arXiv},
arxivId = {cs/9905015},
author = {Dietterich, Thomas G},
doi = {10.1007/3-540-44914-0_2},
eprint = {9905015},
file = {::},
isbn = {3540678395},
issn = {16113349},
journal = {Abstraction Reformulation and Approximation},
pages = {26--44},
pmid = {25246403},
primaryClass = {cs},
title = {{An Overview of MAXQ Hierarchical Reinforcement Learning}},
url = {http://www.springerlink.com/index/W11KBY3DH4VEEHDN.pdf},
volume = {1864/2000},
year = {2000}
}
@article{Mcgovern2001,
abstract = {An ability to adjust to changing environments and unforeseen circumstances is likely to be an important component of a successful autonomous space robot. This paper shows how to augment reinforcement learning algorithms with a method for automatically discovering certain types of subgoals online. By creating useful new subgoals while learning, the agent is able to accelerate learning on a current task and to transfer its expertise to related tasks through the reuse of its ability to attain subgoals. Subgoals are created based on commonalities across multiple paths to a solution. We cast the task of finding these commonalities as a multiple-instance learning problem and use the concept of diverse density to find solutions. We introduced this approach in [10] and here we present additional results for a simulated mobile robot task.},
author = {Mcgovern, Amy and Barto, Andrew},
file = {::},
journal = {6th International Symposium on Artificial Intelligence, Robotics, and Automation in Space},
keywords = {automatic discovering,reinforcement learning,subgoal discovering},
title = {{Accelerating reinforcement learning through the discovery of useful subgoals}},
url = {/home/obada/Desktop/MAS 1 - ISW/Robot Learning/Accelerating reinforcement learning through the discovery of useful subgoals.pdf},
year = {2001}
}
@article{Dietterich2018,
abstract = {Exogenous state variables and rewards can slow down reinforcement learning by injecting uncontrolled variation into the reward signal. We formalize exogenous state variables and rewards and identify conditions under which an MDP with exogenous state can be decomposed into an exogenous Markov Reward Process involving only the exogenous state+reward and an endogenous Markov Decision Process defined with respect to only the endogenous rewards. We also derive a variance-covariance condition under which Monte Carlo policy evaluation on the endogenous MDP is accelerated compared to using the full MDP. Similar speedups are likely to carry over to all RL algorithms. We develop two algorithms for discovering the exogenous variables and test them on several MDPs. Results show that the algorithms are practical and can significantly speed up reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1806.01584},
author = {Dietterich, Thomas G. and Trimponias, George and Chen, Zhitang},
eprint = {1806.01584},
file = {::},
issn = {1938-7228},
title = {{Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning}},
url = {http://arxiv.org/abs/1806.01584},
year = {2018}
}
@article{Fernandes2013,
author = {Fernandes, J F Ribas},
file = {::},
number = {November},
title = {{Hierarchical reinforcement learning in behavior and the brain}},
year = {2013}
}
@article{Tirinzoni2018,
abstract = {We consider the transfer of experience samples (i.e., tuples {\textless} s, a, s', r {\textgreater}) in reinforcement learning (RL), collected from a set of source tasks to improve the learning process in a given target task. Most of the related approaches focus on selecting the most relevant source samples for solving the target task, but then all the transferred samples are used without considering anymore the discrepancies between the task models. In this paper, we propose a model-based technique that automatically estimates the relevance (importance weight) of each source sample for solving the target task. In the proposed approach, all the samples are transferred and used by a batch RL algorithm to solve the target task, but their contribution to the learning process is proportional to their importance weight. By extending the results for importance weighting provided in supervised learning literature, we develop a finite-sample analysis of the proposed batch RL algorithm. Furthermore, we empirically compare the proposed algorithm to state-of-the-art approaches, showing that it achieves better learning performance and is very robust to negative transfer, even when some source tasks are significantly different from the target task.},
archivePrefix = {arXiv},
arxivId = {1805.10886},
author = {Tirinzoni, Andrea and Sessa, Andrea and Pirotta, Matteo and Restelli, Marcello},
eprint = {1805.10886},
file = {::},
title = {{Importance Weighted Transfer of Samples in Reinforcement Learning}},
url = {http://arxiv.org/abs/1805.10886},
year = {2018}
}
@article{Konidaris2016,
abstract = {We describe a framework for building abstraction hierarchies whereby an agent alternates skill- and representation-acquisition phases to construct a sequence of increasingly abstract Markov decision processes. Our formulation builds on recent results showing that the appropriate abstract representation of a problem is specified by the agent's skills. We describe how such a hierarchy can be used for fast planning, and illustrate the construction of an appropriate hierarchy for the Taxi domain.},
archivePrefix = {arXiv},
arxivId = {1509.07582},
author = {Konidaris, George},
doi = {10.1016/j.surg.2015.03.052.Rate},
eprint = {1509.07582},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IJCAI International Joint Conference on Artificial Intelligence/Konidaris/Konidaris - 2016 - Constructing abstraction hierarchies using a skill-symbol loop.pdf:pdf},
isbn = {6176366127},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning},
pages = {1648--1654},
pmid = {28579718},
title = {{Constructing abstraction hierarchies using a skill-symbol loop}},
volume = {2016-Janua},
year = {2016}
}
