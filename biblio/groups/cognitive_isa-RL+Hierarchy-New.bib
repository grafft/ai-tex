Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@phdthesis{Mehta2011,
abstract = {Acting intelligently to efficiently solve sequential decision problems requires the ability to extract hierarchical structure from the underlying domain dynamics, exploit it for optimal or near-optimal decision-making, and transfer it to related problems instead of solving every problem in isolation. This dissertation makes three contributions toward this goal. The first contribution is the introduction of two frameworks for the transfer of hi- erarchical structure in sequential decision problems. The MASH framework facilitates transfer among multiple agents coordinating within a domain. The VRHRL framework allows an agent to transfer its knowledge across a family of domains that share the same transition dynamics but have differing reward dynamics. Both MASH and VRHRL are validated empirically in large domains and the results demonstrate significant speedup in the solutions due to transfer. The second contribution is a new approach to the discovery of hierarchical structure in sequential decision problems. HI-MAT leverages action models to analyze the relevant dependencies in a hierarchically-generated trajectory and it discovers hierarchical struc- ture that transfers to all problems whose actions share the same relevant dependencies as the single source problem. HierGen advances HI-MAT by learning simple action models, leveraging these models to analyze non-hierarchically-generated trajectories from mul- tiple source problems in a robust causal fashion, and discovering hierarchical structure that transfers to all problems whose actions share the same causal dependencies as those in the source problems. Empirical evaluations in multiple domains demonstrate that the discovered hierarchical structures are comparable to manually-designed structures in quality and performance. Action models are essential to hierarchical structure discovery and other aspects of intelligent behavior. The third contribution of this dissertation is the introduction of two general frameworks for learning action models in sequential decision problems. In the MBP framework, learning is user-driven; in the PLEX framework, the learner generates its own problems. The frameworks are formally analyzed and reduced to concept learning with one-sided error. A general action-modeling language is shown to be efficiently learnable in both frameworks.},
author = {Mehta, Neville},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Mehta/Mehta - 2011 - Hierarchical Structure Discovery and Transfer in Sequential Decision Problems.pdf:pdf},
pages = {180},
title = {{Hierarchical Structure Discovery and Transfer in Sequential Decision Problems}},
year = {2011}
}
@article{Vien2016,
abstract = {Reinforcement learning (RL) is an area of machine learning that is concerned with how an agent learns to make decisions sequentially in order to optimize a par-ticular performance measure. For achieving such a goal, the agent has to choose either 1) exploiting previously known knowledge that might end up at local optimality or 2) exploring to gather new knowledge that expects to improve the current performance. Among other RL algo-rithms, Bayesian model-based RL (BRL) is well-known to be able to trade-off between exploitation and explo-ration optimally via belief planning, i.e. partially observ-able Markov decision process (POMDP). However, solving that POMDP often suffers from curse of dimensionality and curse of history. In this paper, we make two major contributions which are: 1) an integration framework of temporal abstraction into BRL that eventually results in a hierarchical POMDP formulation, which can be solved online using a hierarchical sample-based planning solver; 2) a subgoal discovery method for hierarchical BRL that automatically discovers useful macro actions to accelerate learning. In the experiment section, we demonstrate that the proposed approach can scale up to much larger prob-lems. On the other hand, the agent is able to discover useful subgoals for speeding up Bayesian reinforcement learning.},
author = {Vien, Ngo Anh and Lee, Seung Gwan and Chung, Tae Choong},
doi = {10.1007/s10489-015-0742-2},
file = {::},
issn = {15737497},
journal = {Applied Intelligence},
keywords = {Bayesian reinforcement learning,Hierarchical Monte-Carlo planning,Hierarchical reinforcement learning,MDP,Monte-Carlo tree search,POMDP,POSMDP,Reinforcement learning},
number = {1},
pages = {112--126},
publisher = {Applied Intelligence},
title = {{Bayes-adaptive hierarchical MDPs}},
url = {http://dx.doi.org/10.1007/s10489-015-0742-2},
volume = {45},
year = {2016}
}
@article{Vezhnevets2017a,
abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.},
archivePrefix = {arXiv},
arxivId = {1703.01161},
author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
eprint = {1703.01161},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Vezhnevets et al/Vezhnevets et al. - 2017 - FeUdal Networks for Hierarchical Reinforcement Learning.pdf:pdf},
issn = {1938-7228},
title = {{FeUdal Networks for Hierarchical Reinforcement Learning}},
url = {http://arxiv.org/abs/1703.01161},
year = {2017}
}
@article{Digney1998,
abstract = {While the need for hierarchies within control systems is apparent, it is also clear to many researchers that such hierarchies should be learned. Learning both the structure and the component behaviors is a difficult task. The benefit of learning the hierarchical structures of behaviors is that the decomposition of the control structure into smaller transportable chunks allows previously learned knowledge to be applied to new but related tasks. Presented in this paper are improvements to Nested Q-learning (NQL) that allow more realistic learning of control hierarchies in reinforcement environments. Also presented is a simulation of a simple robot performing a series of related tasks that is used to compare both hierarchical and non-hierarchal learning techniques.},
author = {Digney, Bruce L},
file = {::},
isbn = {0-262-66144-6},
journal = {From Animals to Animats 5: Proceedings of the Fifth International Conference on the Simulation of Adaptive Behavior},
keywords = {Options,hierarchical skills,sub-goal,subgoal},
pages = {321--330},
title = {{Learning Hierarchical Control Structures for Multiple Tasks and Changing Environments}},
volume = {5},
year = {1998}
}
@article{Jong2008,
abstract = {Hierarchical decomposition promises to help scale reinforcement learning algorithms naturally to real-world problems by exploiting their under- lying structure. Model-based algorithms, which provided the first finite-time convergence guaran- tees for reinforcement learning, may also play an important role in copingwith the relative scarcity of data in large environments. In this paper, we introduce an algorithm that fully integrates mod- ern hierarchical and model-learning methods in the standard reinforcement learning setting. Our algorithm, R-MAXQ, inherits the efficientmodel- based exploration of the R-MAX algorithm and the opportunities for abstraction provided by the MAXQframework. We analyze the sample com- plexity of our algorithm, and our experiments in a standard simulation environment illustrate the advantages of combining hierarchies andmodels},
author = {Jong, Nicholas K and Stone, Peter},
doi = {10.1145/1390156.1390211},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/25th International Conference on Machine Learning/Jong, Stone/Jong, Stone - 2008 - Hierarchical Model-Based Reinforcement Learning R- MAX MAXQ.pdf:pdf},
isbn = {9781605582054},
journal = {25th International Conference on Machine Learning},
number = {July},
title = {{Hierarchical Model-Based Reinforcement Learning : R- MAX + MAXQ}},
year = {2008}
}
@article{Nichol2018,
abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
archivePrefix = {arXiv},
arxivId = {1803.02999},
author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
eprint = {1803.02999},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Nichol, Achiam, Schulman/Nichol, Achiam, Schulman - 2018 - On First-Order Meta-Learning Algorithms.pdf:pdf},
pages = {1--15},
title = {{On First-Order Meta-Learning Algorithms}},
url = {http://arxiv.org/abs/1803.02999},
year = {2018}
}
@article{Fox2017,
abstract = {Augmenting an agent's control with useful higher-level behaviors called options can greatly reduce the sample complexity of reinforcement learning, but manually designing options is infeasible in high-dimensional and abstract state spaces. While recent work has proposed several techniques for automated option discovery, they do not scale to multi-level hierarchies and to expressive representations such as deep networks. We present Discovery of Deep Options (DDO), a policy-gradient algorithm that discovers parametrized options from a set of demonstration trajectories, and can be used recursively to discover additional levels of the hierarchy. The scalability of our approach to multi-level hierarchies stems from the decoupling of low-level option discovery from high-level meta-control policy learning, facilitated by under-parametrization of the high level. We demonstrate that using the discovered options to augment the action space of Deep Q-Network agents can accelerate learning by guiding exploration in tasks where random actions are unlikely to reach valuable states. We show that DDO is effective in adding options that accelerate learning in 4 out of 5 Atari RAM environments chosen in our experiments. We also show that DDO can discover structure in robot-assisted surgical videos and kinematics that match expert annotation with 72{\%} accuracy.},
archivePrefix = {arXiv},
arxivId = {1703.08294},
author = {Fox, Roy and Krishnan, Sanjay and Stoica, Ion and Goldberg, Ken},
eprint = {1703.08294},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Fox et al/Fox et al. - 2017 - Multi-Level Discovery of Deep Options.pdf:pdf},
isbn = {9781510827806},
title = {{Multi-Level Discovery of Deep Options}},
url = {http://arxiv.org/abs/1703.08294},
year = {2017}
}
@article{Mousavi2014,
abstract = {Reinforcement learning (RL) for solving large and complex problems faces the curse of dimensions problem. To overcome this problem, frameworks based on the temporal abstraction have been presented; each having their advantages and disadvantages. This paper proposes a new method like the strategies introduced in the hierarchical abstract machines (HAMs) to create a high-level controller layer of reinforcement learning which uses options. The proposed framework considers a non-deterministic automata as a controller to make a more effective use of temporally extended actions and state space clustering. This method can be viewed as a bridge between option and HAM frameworks, which tries to suggest a new framework to decrease the disadvantage of both by creating connection structures between them and at the same time takes advantages of them. Experimental results on different test environments show significant efficiency of the proposed method.},
author = {Mousavi, Seyed Sajad and Ghazanfari, Behzad and Mozayani, Nasser and Jahed-Motlagh, Mohammad Reza},
doi = {10.1016/j.asoc.2014.08.071},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Applied Soft Computing Journal/Mousavi et al/Mousavi et al. - 2014 - Automatic abstraction controller in reinforcement learning agent via automata.pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Cluster,Hierarchical reinforcement learning,Multi-agent learning,Reinforcement learning},
pages = {118--128},
publisher = {Elsevier B.V.},
title = {{Automatic abstraction controller in reinforcement learning agent via automata}},
url = {http://dx.doi.org/10.1016/j.asoc.2014.08.071},
volume = {25},
year = {2014}
}
