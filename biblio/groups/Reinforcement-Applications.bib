Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Chaplot2017,
abstract = {To perform tasks specified by natural language instructions, autonomous agents need to extract semantically meaningful representations of language and map it to visual elements and actions in the environment. This problem is called task-oriented language grounding. We propose an end-to-end trainable neural architecture for task-oriented language grounding in 3D environments which assumes no prior linguistic or perceptual knowledge and requires only raw pixels from the environment and the natural language instruction as input. The proposed model combines the image and text representations using a Gated-Attention mechanism and learns a policy to execute the natural language instruction using standard reinforcement and imitation learning methods. We show the effectiveness of the proposed model on unseen instructions as well as unseen maps, both quantitatively and qualitatively. We also introduce a novel environment based on a 3D game engine to simulate the challenges of task-oriented language grounding over a rich set of instructions and environment states.},
archivePrefix = {arXiv},
arxivId = {1706.07230},
author = {Chaplot, Devendra Singh and Sathyendra, Kanthashree Mysore and Pasumarthi, Rama Kumar and Rajagopal, Dheeraj and Salakhutdinov, Ruslan},
eprint = {1706.07230},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Chaplot et al/Chaplot et al. - 2017 - Gated-Attention Architectures for Task-Oriented Language Grounding.pdf:pdf},
title = {{Gated-Attention Architectures for Task-Oriented Language Grounding}},
url = {http://arxiv.org/abs/1706.07230},
year = {2017}
}
@article{Oh2016,
abstract = {In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.},
archivePrefix = {arXiv},
arxivId = {1605.09128},
author = {Oh, Junhyuk and Chockalingam, Valliappa and Singh, Satinder and Lee, Honglak},
doi = {10.1109/CVPR.2014.180},
eprint = {1605.09128},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Oh et al/Oh et al. - 2016 - Control of Memory, Active Perception, and Action in Minecraft.pdf:pdf},
isbn = {9781510829008},
issn = {10636919},
title = {{Control of Memory, Active Perception, and Action in Minecraft}},
url = {http://arxiv.org/abs/1605.09128},
year = {2016}
}
@inproceedings{Sutton2011a,
abstract = {Maintaining accurate world knowledge in a complex and changing environment is a perennial problem for robots and other arti cial intelligence systems. Our architecture for addressing this problem, called Horde, consists of a large number of independent reinforcement learning sub-agents, or demons. Each demon is responsible for answering a sin- gle predictive or goal-oriented question about the world, thereby contributing in a factored, modular way to the sys- tem's overall knowledge. The questions are in the form of a value function, but each demon has its own policy, re- ward function, termination function, and terminal-reward function unrelated to those of the base problem. Learning proceeds in parallel by all demons simultaneously so as to extract the maximal training information from whatever ac- tions are taken by the system as a whole. Gradient-based temporal-di erence learning methods are used to learn ef- ciently and reliably with function approximation in this o -policy setting. Horde runs in constant time and memory per time step, and is thus suitable for learning online in real- time applications such as robotics. We present results using Horde on a multi-sensored mobile robot to successfully learn goal-oriented behaviors and long-term predictions from o - policy experience. Horde is a signi cant incremental step towards a real-time architecture for ecient learning of gen-},
author = {Sutton, Richard S and Modayil, Joseph and Delp, Michael and Degris, Thomas and Pilarski, Patrick M and White, Adam},
booktitle = {Proc. of 10th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS 2011)},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proc. of 10th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS 2011)/Sutton et al/Sutton et al. - 2011 - Horde A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorim.pdf:pdf},
keywords = {artificial intelligence,difference learning,inforcement learning,knowledge representation,off policy learning,re,real time,robotics,temporal,value function approximation},
pages = {761--768},
title = {{Horde : A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction}},
url = {http://webdocs.cs.ualberta.ca/{~}sutton/papers/horde-aamas-11.pdf},
year = {2011}
}
@article{Thomas2017,
abstract = {In the artificial intelligence field, learning often corresponds to changing the parameters of a parameterized function. A learning rule is an algorithm or mathematical expression that specifies precisely how the parameters should be changed. When creating an artificial intelligence system, we must make two decisions: what representation should be used (i.e., what parameterized function should be used) and what learning rule should be used to search through the resulting set of representable functions. Using most learning rules, these two decisions are coupled in a subtle (and often unintentional) way. That is, using the same learning rule with two different representations that can represent the same sets of functions can result in two different outcomes. After arguing that this coupling is undesirable, particularly when using artificial neural networks, we present a method for partially decoupling these two decisions for a broad class of learning rules that span unsupervised learning, reinforcement learning, and supervised learning.},
archivePrefix = {arXiv},
arxivId = {1706.03100},
author = {Thomas, Philip S. and Dann, Christoph and Brunskill, Emma},
eprint = {1706.03100},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Thomas, Dann, Brunskill/Thomas, Dann, Brunskill - 2017 - Decoupling Learning Rules from Representations.pdf:pdf},
keywords = {TODO},
title = {{Decoupling Learning Rules from Representations}},
url = {http://arxiv.org/abs/1706.03100},
volume = {1},
year = {2017}
}
@article{Zoph2016,
abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
archivePrefix = {arXiv},
arxivId = {1611.01578},
author = {Zoph, Barret and Le, Quoc V.},
doi = {10.1016/j.knosys.2015.01.010},
eprint = {1611.01578},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Zoph, Le/Zoph, Le - 2016 - Neural Architecture Search with Reinforcement Learning.pdf:pdf},
isbn = {9781424425051},
issn = {1938-7228},
pages = {1--16},
title = {{Neural Architecture Search with Reinforcement Learning}},
url = {http://arxiv.org/abs/1611.01578},
year = {2016}
}
