Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Greff2017,
abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs ({\$}\backslashapprox 15{\$} years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
archivePrefix = {arXiv},
arxivId = {1503.04069},
author = {Greff, Klaus and Srivastava, Rupesh K. and Koutnik, Jan and Steunebrink, Bas R. and Schmidhuber, Jurgen},
doi = {10.1109/TNNLS.2016.2582924},
eprint = {1503.04069},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Greff et al/2017/LSTM A Search Space Odyssey.pdf:pdf},
isbn = {9788578110796},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Functional ANalysis Of VAriance (fANOVA),long short-term memory (LSTM),random search,recurrent neural networks,sequence learning},
number = {10},
pages = {2222--2232},
pmid = {25246403},
title = {{LSTM: A Search Space Odyssey}},
volume = {28},
year = {2017}
}
@article{Mehta2014,
archivePrefix = {arXiv},
arxivId = {1410.3831},
author = {Mehta, Pankaj and Schwab, David J.},
eprint = {1410.3831},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Mehta, Schwab/2014/An exact mapping between the Variational Renormalization Group and Deep Learning.pdf:pdf},
title = {{An exact mapping between the Variational Renormalization Group and Deep Learning}},
url = {http://arxiv.org/abs/1410.3831v1},
year = {2014}
}
@inproceedings{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
booktitle = {arXiv: 1312.5602},
eprint = {1312.5602},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Mnih et al/2013/Playing Atari with Deep Reinforcement Learning.pdf:pdf},
pages = {1--9},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@book{Haikin2006,
address = {М.},
author = {Хайкин, С.},
edition = {2-е},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Хайкин/2006/Нейронные сети полный курс.djvu:djvu},
language = {russian},
pages = {1104},
publisher = {Издательский дом "Вильямс"},
title = {{Нейронные сети: полный курс}},
year = {2006}
}
@inproceedings{Szegedy2016,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible. We benchmark our methods on the ILSVRC 2012 classification challenge validation set and demonstrate substantial gains over the state of the art via to carefully factorized convolutions and aggressive regularization: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1002/2014GB005021},
eprint = {1512.00567},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Szegedy et al/2016/Rethinking the Inception Architecture for Computer Vision.pdf:pdf},
isbn = {9781617796029},
issn = {08866236},
pages = {2818--2826},
pmid = {8190083},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567{\%}5Cnhttp://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2016/html/Szegedy{\_}Rethinking{\_}the{\_}Inception{\_}CVPR{\_}2016{\_}paper.html},
year = {2016}
}
@article{Yang2016,
abstract = {We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the word and sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
archivePrefix = {arXiv},
arxivId = {1606.02393},
author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
doi = {10.18653/v1/N16-1174},
eprint = {1606.02393},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Yang et al/2016/Hierarchical Attention Networks for Document Classification.pdf:pdf},
isbn = {9781941643914},
journal = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
pages = {1480--1489},
title = {{Hierarchical Attention Networks for Document Classification}},
url = {http://aclweb.org/anthology/N16-1174},
year = {2016}
}
@article{Xu2015a,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
archivePrefix = {arXiv},
arxivId = {1502.03044},
author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
eprint = {1502.03044},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Xu et al/2015/Show, Attend and Tell Neural Image Caption Generation with Visual Attention.pdf:pdf},
journal = {ArXIV},
month = {feb},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
url = {http://arxiv.org/abs/1502.03044},
year = {2015}
}
@techreport{Jaeger2014a,
archivePrefix = {arXiv},
arxivId = {arXiv:1403.3369v1},
author = {Jaeger, Herbert},
eprint = {arXiv:1403.3369v1},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Jaeger/2014/Controlling Recurrent Neural Networks by Conceptors.pdf:pdf},
institution = {Jacobs University Bremen},
title = {{Controlling Recurrent Neural Networks by Conceptors}},
year = {2014}
}
@article{Blundell2016,
abstract = {State of the art deep reinforcement learning algorithms take many millions of interactions to attain human-level performance. Humans, on the other hand, can very quickly exploit highly rewarding nuances of an environment upon first discovery. In the brain, such rapid learning is thought to depend on the hippocampus and its capacity for episodic memory. Here we investigate whether a simple model of hippocampal episodic control can learn to solve difficult sequential decision-making tasks. We demonstrate that it not only attains a highly rewarding strategy significantly faster than state-of-the-art deep reinforcement learning algorithms, but also achieves a higher overall reward on some of the more challenging domains.},
archivePrefix = {arXiv},
arxivId = {1606.04460},
author = {Blundell, Charles and Uria, Benigno and Pritzel, Alexander and Li, Yazhe and Ruderman, Avraham and Leibo, Joel Z and Rae, Jack and Wierstra, Daan and Hassabis, Demis},
eprint = {1606.04460},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Blundell et al/2016/Model-Free Episodic Control.pdf:pdf},
month = {jun},
title = {{Model-Free Episodic Control}},
url = {http://arxiv.org/abs/1606.04460},
year = {2016}
}
@incollection{Marek2008,
address = {Berlin},
author = {Marek, Rudolf and Skrbek, Miroslav},
booktitle = {Artificial Neural Networks - ICANN 2008},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Marek, Skrbek/2008/Efficient Implementation of the THSOM Neural Network.pdf:pdf},
pages = {159--168},
publisher = {Springer},
title = {{Efficient Implementation of the THSOM Neural Network}},
year = {2008}
}
@article{Fernando2017,
abstract = {For artificial general intelligence (AGI) it would be efficient if multiple users trained the same giant neural network, permitting parameter reuse, without catastrophic forgetting. PathNet is a first step in this direction. It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks. Agents are pathways (views) through the network which determine the subset of parameters that are used and updated by the forwards and backwards passes of the backpropogation algorithm. During learning, a tournament selection genetic algorithm is used to select pathways through the neural network for replication and mutation. Pathway fitness is the performance of that pathway measured according to a cost function. We demonstrate successful transfer learning; fixing the parameters along a path learned on task A and re-evolving a new population of paths for task B, allows task B to be learned faster than it could be learned from scratch or after fine-tuning. Paths evolved on task B re-use parts of the optimal path evolved on task A. Positive transfer was demonstrated for binary MNIST, CIFAR, and SVHN supervised learning classification tasks, and a set of Atari and Labyrinth reinforcement learning tasks, suggesting PathNets have general applicability for neural network training. Finally, PathNet also significantly improves the robustness to hyperparameter choices of a parallel asynchronous reinforcement learning algorithm (A3C).},
archivePrefix = {arXiv},
arxivId = {1701.08734},
author = {Fernando, Chrisantha and Banarse, Dylan and Blundell, Charles and Zwols, Yori and Ha, David and Rusu, Andrei A. and Pritzel, Alexander and Wierstra, Daan},
eprint = {1701.08734},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Fernando et al/2017/PathNet Evolution Channels Gradient Descent in Super Neural Networks.pdf:pdf},
issn = {1701.08734},
journal = {ArXiv},
keywords = {basal ganglia,continual learning,evolution and,giant networks,learning,multitask,path evolution algorithm,transfer learning},
title = {{PathNet: Evolution Channels Gradient Descent in Super Neural Networks}},
url = {http://arxiv.org/abs/1701.08734},
year = {2017}
}
@article{Scellier2016,
abstract = {We introduce Equilibrium Propagation (e-prop), a learning algorithm for energy-based models. This algorithm involves only one kind of neural computation both for the first phase (when the prediction is made) and the second phase (after the target is revealed) of training. Contrary to backpropagation in feedforward networks, there is no need for special computation in the second phase of our learning algorithm. Equilibrium Propagation combines features of Contrastive Hebbian Learning and Contrastive Divergence while solving the theoretical issues of both algorithms: the algorithm computes the exact gradient of a well defined objective function. Because the objective function is defined in terms of local perturbations, the second phase of e-prop corresponds to only nudging the first-phase fixed point towards a configuration that has lower cost value. In the case of a multi-layer supervised neural network, the output units are slightly nudged towards their target, and the perturbation introduced at the output layer propagates backward in the network. The theory developed in this paper shows that the signal 'back-propagated' during this second phase actually contains information about the error derivatives, which we use to implement a learning rule proved to perform gradient descent with respect to the objective function. Thus, this work makes it more plausible that a mechanism similar to backpropagation could be implemented by brains.},
archivePrefix = {arXiv},
arxivId = {1602.05179},
author = {Scellier, Benjamin and Bengio, Yoshua},
eprint = {1602.05179},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Scellier, Bengio/2016/Equilibrium Propagation Bridging the Gap Between Energy-Based Models and Backpropagation.pdf:pdf},
month = {feb},
title = {{Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation}},
url = {http://arxiv.org/abs/1602.05179},
year = {2016}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Silver et al/2016/Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}
@incollection{Koutn2008,
address = {Berlin},
author = {Koutnik, Jan and Snorek, Miroslav},
booktitle = {Artificial Neural Networks - ICANN 2008},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Koutnik, Snorek/2008/Temporal Hebbian Self-Organizing Map for Sequences.pdf:pdf},
pages = {632--641},
publisher = {Springer},
title = {{Temporal Hebbian Self-Organizing Map for Sequences}},
year = {2008}
}
@article{Vinyals2016,
abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6{\%} to 93.2{\%} and from 88.0{\%} to 93.8{\%} on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
archivePrefix = {arXiv},
arxivId = {1606.04080},
author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {1606.04080},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Vinyals et al/2016/Matching Networks for One Shot Learning.pdf:pdf},
journal = {arXiv},
title = {{Matching Networks for One Shot Learning}},
url = {http://arxiv.org/abs/1606.04080},
year = {2016}
}
@article{Fatahi2016,
author = {Fatahi, Mazdak and Ahmadi, Mahmood and Ahmadi, Arash and Shahsavari, Mahyar and Devienne, Philippe},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Fatahi et al/2016/Towards an Spiking Deep Belief Network for Face Recognition Application.pdf:pdf},
isbn = {9781509035861},
number = {Iccke},
pages = {153--158},
title = {{Towards an Spiking Deep Belief Network for Face Recognition Application}},
year = {2016}
}
@article{Deng2013a,
abstract = {This book is aimed to provide an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria: 1) expertise or knowledge of the authors; 2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and 3) the application areas that have the potential to be impacted significantly by deep learning and that have gained concentrated research efforts, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning. In Chapter 1, we provide the background of deep learning, as intrinsically connected to the use of multiple layers of nonlinear transformations to derive features from the sensory signals such as speech and visual images. In the most recent literature, deep learning is embodied also as representation learning, which involves a hierarchy of features or concepts where higher-level representations of them are defined from lower-level ones and where the same lower-level representations help to define higher-level ones. In Chapter 2, a brief historical account of deep learning is presented. In particular, selected chronological development of speech recognition is used to illustrate the recent impact of deep learning that has become a dominant technology in speech recognition industry within only a few years since the start of a collaboration between academic and industrial researchers in applying deep learning to speech recognition. In Chapter 3, a three-way classification scheme for a large body of work in deep learning is developed. We classify a growing number of deep learning techniques into unsupervised, supervised, and hybrid categories, and present qualitative descriptions and a literature survey for each category. From Chapter 4 to Chapter 6, we discuss in detail three popular deep networks and related learning methods, one in each category. Chapter 4 is devoted to deep autoencoders as a prominent example of the unsupervised deep learning techniques. Chapter 5 gives a major example in the hybrid deep network category, which is the discriminative feed-forward neural network for supervised learning with many layers initialized using layer-by-layer generative, unsupervised pre-training. In Chapter 6, deep stacking networks and several of the variants are discussed in detail, which exemplify the discriminative or supervised deep learning techniques in the three-way categorization scheme. In Chapters 7-11, we select a set of typical and successful applications of deep learning in diverse areas of signal and information processing and of applied artificial intelligence. In Chapter 7, we review the applications of deep learning to speech and audio processing, with emphasis on speech recognition organized according to several prominent themes. In Chapters 8, we present recent results of applying deep learning to language modeling and natural language processing. Chapter 9 is devoted to selected applications of deep learning to information retrieval including Web search. In Chapter 10, we cover selected applications of deep learning to image object recognition in computer vision. Selected applications of deep learning to multi-modal processing and multi-task learning are reviewed in Chapter 11. Finally, an epilogue is given in Chapter 12 to summarize what we presented in earlier chapters and to discuss future challenges and directions.},
author = {Deng, Li and Yu, Dong},
doi = {10.1136/bmj.319.7209.0a},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Deng, Yu/2013/Deep Learning Methods and Applications.pdf:pdf},
isbn = {9781405161251},
issn = {09598138},
journal = {Foundations and Trends in Signal Processing},
number = {3-4},
pages = {197----387},
pmid = {10463930},
title = {{Deep Learning: Methods and Applications}},
volume = {7},
year = {2013}
}
@article{Hinton2006,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Hinton, Osindero, Teh/2006/A fast learning algorithm for deep belief nets.pdf:pdf},
journal = {Neural Computation},
number = {7},
pages = {1527--1554},
title = {{A fast learning algorithm for deep belief nets}},
volume = {18},
year = {2006}
}
@article{Elman1990,
author = {Elman, J. L.},
doi = {10.1207/s15516709cog1402_1},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Elman/1990/Finding structure in time.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive science},
number = {2},
pages = {179--211},
pmid = {19563812},
title = {{Finding structure in time}},
volume = {14},
year = {1990}
}
@article{Tino2006,
abstract = {Recently there has been an outburst of interest in extending topographic maps of vectorial data to more general data structures, such as sequences or trees. However, there is no general consensus as to how best to process sequences using topographic maps, and this topic remains an active focus of neurocomputational research. The representational capabilities and internal representations of the models are not well understood. Here, we rigorously analyze a generalization of the self-organizing map (SOM) for processing sequential data, recursive SOM(RecSOM) (Voegtlin, 2002), as a nonautonomous dynamical system consisting of a set of fixed input maps. We argue that contractive fixed-input maps are likely to produce Markovian organizations of receptive fields on the RecSOM map. We derive bounds on parameter beta (weighting the importance of importing past information when processing sequences) under which contractiveness of the fixed-input maps is guaranteed. Some generalizations of SOM contain a dynamic module responsible for processing temporal contexts as an integral part of the model. We show that Markovian topographic maps of sequential data can be produced using a simple fixed (nonadaptable) dynamic module externally feeding a standard topographic model designed to process static vectorial data of fixed dimensionality (e.g., SOM). However, by allowing trainable feedback connections, one can obtain Markovian maps with superior memory depth and topography preservation. We elaborate on the importance of non-Markovian organizations in topographic maps of sequential data.},
author = {Tino, Peter and Farkas, Igor and van Mourik, Jort},
doi = {10.1162/neco.2006.18.10.2529},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Tino, Farkas, van Mourik/2006/Dynamics and topographic organization of recursive self-organizing maps.pdf:pdf},
isbn = {0899-7667 (Print)$\backslash$n0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural computation},
keywords = {review{\_}neuromorph},
mendeley-tags = {review{\_}neuromorph},
number = {10},
pages = {2529--2567},
pmid = {16907636},
title = {{Dynamics and topographic organization of recursive self-organizing maps}},
volume = {18},
year = {2006}
}
@inproceedings{Szegedy2014,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Szegedy et al/2014/Going Deeper with Convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
pages = {1--9},
pmid = {24920543},
title = {{Going Deeper with Convolutions}},
url = {http://arxiv.org/abs/1409.4842},
year = {2014}
}
@book{2018,
address = {СПб.},
author = {Николенко, С. and Кадурин, А. and Архангельская, Е.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Николенко, Кадурин, Архангельская/2018/Глубокое обучение.pdf:pdf},
isbn = {9785496025362},
pages = {480},
publisher = {Питер},
title = {{Глубокое обучение}},
year = {2018}
}
@inproceedings{Homenda2014,
author = {Homenda, Wladyslaw and Jastrzebska, Agnieszka and Pedrycz, Witold},
booktitle = {Fourth World Congress on Information and Communication Technologies (WICT), 2014},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Homenda, Jastrzebska, Pedrycz/2014/On Interpretation of Fuzzy Cognitive Maps Trained to Model Time Series.pdf:pdf},
keywords = {-fuzzy cognitive map,cognitive map,fcms has been studied,ii,in,interpretation of fuzzy,l iterature r eview,time series,time series modelling with},
pages = {152--157},
publisher = {IEEE},
title = {{On Interpretation of Fuzzy Cognitive Maps Trained to Model Time Series}},
year = {2014}
}
@article{Tokunaga2009,
abstract = {This study aims to develop a generalized framework of an SOM called a modular network SOM (mnSOM). The mnSOM has an array structure consisting of functional modules that are trainable neural networks, e.g., multi-layer perceptrons (MLPs), instead of the vector units of the conventional SOM. In the case of MLP-modules, an mnSOM learns a group of systems or functions in terms of the input-output relationships in parallel with generating a feature map of them. Thus an mnSOM with MLP modules is an SOM in function space rather than in vector space. In this paper, first, as an example, we focus on a class of mnSOM that consists of MLP modules and introduce the architecture and algorithm. Then, a more generalized framework is described. Finally, some simulation results of an MLP-module-mnSOM are presented. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
author = {Tokunaga, Kazuhiro and Furukawa, Tetsuo},
doi = {10.1016/j.neunet.2008.10.006},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Tokunaga, Furukawa/2009/Modular network SOM.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Function space,Modular network,SOM},
pages = {82--90},
pmid = {19103475},
title = {{Modular network SOM}},
volume = {22},
year = {2009}
}
@book{Bengio2009b,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua},
booktitle = {Foundations and Trends{\textregistered} in Machine Learning},
doi = {10.1561/2200000006},
eprint = {0500581},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Bengio/2009/Learning Deep Architectures for AI.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
number = {1},
pages = {1--127},
pmid = {17348934},
primaryClass = {submit},
title = {{Learning Deep Architectures for AI}},
volume = {2},
year = {2009}
}
@article{Rauber2002,
author = {Rauber, Andreas and Merkl, Dieter and Dittenbach, Michael},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Rauber, Merkl, Dittenbach/2002/The Growing Hierarchical Self-Organizing Map Exploratory Analysis of High-Dimensional Data.pdf:pdf},
journal = {IEEE Transactions on Neural Networks},
pages = {1331--1341},
title = {{The Growing Hierarchical Self-Organizing Map: Exploratory Analysis of High-Dimensional Data}},
volume = {13},
year = {2002}
}
@article{Graves2006,
abstract = {Many real-world sequence learning tasks re- quire the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their out- puts into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un- segmented sequences directly, thereby solv- ing both problems. An experiment on the TIMIT speech corpus demonstrates its ad- vantages over both a baseline HMM and a hybrid HMM-RNN.},
author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
doi = {10.1145/1143844.1143891},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Graves et al/2006/Connectionist Temporal Classification Labelling Unsegmented Sequence Data with Recurrent Neural Networks.pdf:pdf},
isbn = {1595933832},
issn = {10987576},
journal = {Proceedings of the 23rd international conference on Machine Learning},
pages = {369--376},
title = {{Connectionist Temporal Classification : Labelling Unsegmented Sequence Data with Recurrent Neural Networks}},
year = {2006}
}
@article{Lillicrap2015,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
doi = {10.1561/2200000006},
eprint = {1509.02971},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Lillicrap et al/2015/Continuous control with deep reinforcement learning.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
pmid = {24966830},
title = {{Continuous control with deep reinforcement learning}},
url = {http://arxiv.org/abs/1509.02971},
year = {2015}
}
@article{Cichy2016,
author = {Cichy, Radoslaw Martin and Khosla, Aditya and Pantazis, Dimitrios and Torralba, Antonio and Oliva, Aude},
doi = {10.1038/srep27755},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cichy et al/2016/Deep neural networks predict hierarchical spatio-temporal cortical dynamics of human visual object recognition.pdf:pdf},
issn = {2045-2322},
journal = {Scientific Reports},
number = {6},
pmid = {27282108},
title = {{Deep neural networks predict hierarchical spatio-temporal cortical dynamics of human visual object recognition}},
url = {http://dx.doi.org/10.1038/srep27755},
year = {2016}
}
@article{Guimaraes2002b,
author = {Guimar{\~{a}}es, Gabriela and Lobo, Victor Sousa and Moura-Pires, Fernando},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Guimar{\~{a}}es, Lobo, Moura-Pires/2002/A Taxonomy of Self-organizing Maps for Temporal Sequence Processing.pdf:pdf},
isbn = {1088-467X},
issn = {1088467X},
journal = {Intell Data Anal},
pages = {1--52},
title = {{A Taxonomy of Self-organizing Maps for Temporal Sequence Processing}},
year = {2002}
}
@article{Smith2016,
abstract = {Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications. Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet). Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.},
archivePrefix = {arXiv},
arxivId = {1611.00847},
author = {Smith, Leslie N. and Topin, Nicholay},
doi = {10.1051/0004-6361/201527329},
eprint = {1611.00847},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Smith, Topin/2016/Deep Convolutional Neural Network Design Patterns.pdf:pdf},
isbn = {9781611970685},
issn = {0004-6361},
journal = {arXiv},
month = {nov},
pages = {1--15},
title = {{Deep Convolutional Neural Network Design Patterns}},
url = {http://arxiv.org/abs/1511.06434 http://arxiv.org/abs/1611.00847},
year = {2016}
}
@article{Marblestone2016,
archivePrefix = {arXiv},
arxivId = {1606.03813},
author = {Marblestone, Adam H and Wayne, Greg and Kording, Konrad P},
doi = {10.1101/058545},
eprint = {1606.03813},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Marblestone, Wayne, Kording/2016/Towards an integration of deep learning and neuroscience.pdf:pdf},
keywords = {cognitive architecture,cost functions,neural networks,neuroscience},
title = {{Towards an integration of deep learning and neuroscience}},
year = {2016}
}
@article{Nøkland2016a,
abstract = {Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45{\%} error on the permutation invariant MNIST task.},
archivePrefix = {arXiv},
arxivId = {1609.01596},
author = {N{\o}kland, Arild},
eprint = {1609.01596},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/N{\o}kland/2016/Direct Feedback Alignment Provides Learning in Deep Neural Networks.pdf:pdf},
journal = {ArXiv},
month = {sep},
title = {{Direct Feedback Alignment Provides Learning in Deep Neural Networks}},
url = {http://arxiv.org/abs/1609.01596},
year = {2016}
}
@article{Schmidhuber2015,
abstract = {In recent years, deep neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.7828v1},
author = {Schmidhuber, J},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {arXiv:1404.7828v1},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Schmidhuber/2015/Deep Learning in Neural Networks An Overview.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
pages = {85--117},
publisher = {Elsevier Ltd},
title = {{Deep Learning in Neural Networks: An Overview}},
url = {http://arxiv.org/abs/1404.7828},
volume = {61},
year = {2015}
}
@article{Fukushima1980,
abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by “learning without a teacher”, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname “neocognitron”. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of “S-cells”, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of “C-cells” similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any “teacher” during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
author = {Fukushima, Kunihiko},
doi = {10.1007/BF00344251},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Fukushima/1980/Neocognitron A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position.pdf:pdf},
isbn = {0340-1200},
issn = {03401200},
journal = {Biological Cybernetics},
number = {4},
pages = {193--202},
pmid = {7370364},
title = {{Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position}},
volume = {36},
year = {1980}
}
@article{Koutnik2014a,
abstract = {Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when the long-term memory is required. This paper introduces a simple, yet powerful modification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of RNN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving two tasks: audio signal generation and TIMIT spoken word classification, where it outperforms both RNN and LSTM networks.},
archivePrefix = {arXiv},
arxivId = {1402.3511},
author = {Koutn{\'{i}}k, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, J{\"{u}}rgen},
eprint = {1402.3511},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Koutn{\'{i}}k et al/2014/A Clockwork RNN.pdf:pdf},
isbn = {9781634393973},
journal = {ArXIV},
month = {feb},
title = {{A Clockwork RNN}},
url = {http://arxiv.org/abs/1402.3511},
year = {2014}
}
@article{MacNeil2011,
abstract = {A central criticism of standard theoretical approaches to constructing stable, recurrent model networks is that the synaptic connection weights need to be finely-tuned. This criticism is severe because proposed rules for learning these weights have been shown to have various limitations to their biological plausibility. Hence it is unlikely that such rules are used to continuously fine-tune the network in vivo. We describe a learning rule that is able to tune synaptic weights in a biologically plausible manner. We demonstrate and test this rule in the context of the oculomotor integrator, showing that only known neural signals are needed to tune the weights. We demonstrate that the rule appropriately accounts for a wide variety of experimental results, and is robust under several kinds of perturbation. Furthermore, we show that the rule is able to achieve stability as good as or better than that provided by the linearly optimal weights often used in recurrent models of the integrator. Finally, we discuss how this rule can be generalized to tune a wide variety of recurrent attractor networks, such as those found in head direction and path integration systems, suggesting that it may be used to tune a wide variety of stable neural systems.},
author = {MacNeil, David and Eliasmith, Chris},
doi = {10.1371/journal.pone.0022885},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/MacNeil, Eliasmith/2011/Fine-tuning and the stability of recurrent neural networks.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {9},
pmid = {21980334},
title = {{Fine-tuning and the stability of recurrent neural networks}},
volume = {6},
year = {2011}
}
@article{VanderVelde2015,
author = {van der Velde, Frank},
doi = {10.1016/j.patrec.2015.02.008},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/van der Velde/2015/Computation and dissipative dynamical systems in neural networks for classification.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Classification,Classical cognitive science vs. dyn},
pages = {44--52},
publisher = {Elsevier Ltd.},
title = {{Computation and dissipative dynamical systems in neural networks for classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515000562},
volume = {64},
year = {2015}
}
@article{Miller2006,
abstract = {We present an approach for abstracting invariant classifications of spatiotemporal patterns presented in a highdimensionality input stream, and apply an early proof-of-concept to shift and scale invariant shape recognition. A model called Hierarchical Quilted Self-Organizing Map (HQSOM) is developed, using recurrent self-organizing maps (RSOM) arranged in a pyramidal hierarchy, attempting to mimic the parallel/hierarchical pattern of isocortical processing in the brain. The results of experiments are presented in which the algorithm learns to classify multiple shapes, invariant to shift and scale transformations, in a very small (7×7 pixel) field of view.},
author = {Miller, Jeffrey W and Lommel, Peter H},
doi = {10.1117/12.686183},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Miller, Lommel/2006/Biomimetic sensory abstraction using hierarchical quilted self-organizing maps.pdf:pdf},
isbn = {0819464821},
issn = {0277786X},
journal = {Proceedings of the International Society for Optics and Photonics (SPIE)},
keywords = {Biomimetic,Cognitive Learning,Computational Neuroscience,Image Interpretation,Pattern Recognition,Robot Vision,Self-Organizing Maps (SOM),Unsupervised Learning},
number = {617},
pages = {63840A--63840A--10},
title = {{Biomimetic sensory abstraction using hierarchical quilted self-organizing maps}},
url = {http://link.aip.org/link/PSISDG/v6384/i1/p63840A/s1{\&}Agg=doi},
volume = {6384},
year = {2006}
}
@misc{LeCun1989,
abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the US Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
booktitle = {Neural Computation},
doi = {10.1162/neco.1989.1.4.541},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/LeCun et al/1989/Backpropagation Applied to Handwritten Zip Code Recognition.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
number = {4},
pages = {541--551},
pmid = {1000111957},
title = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
volume = {1},
year = {1989}
}
@article{Bengio2016,
abstract = {We consider deep multi-layered generative models such as Boltzmann machines or Hopfield nets in which computation (which implements inference) is both recurrent and stochastic, but where the recurrence is not to model sequential structure, only to perform computation. We find conditions under which a simple feedforward computation is a very good initialization for inference, after the input units are clamped to observed values. It means that after the feedforward initialization, the recurrent network is very close to a fixed point of the network dynamics, where the energy gradient is 0. The main condition is that consecutive layers form a good auto-encoder, or more generally that different groups of inputs into the unit (in particular, bottom-up inputs on one hand, top-down inputs on the other hand) are consistent with each other, producing the same contribution into the total weighted sum of inputs. In biological terms, this would correspond to having each dendritic branch correctly predicting the aggregate input from all the dendritic branches, i.e., the soma potential. This is consistent with the prediction that the synaptic weights into dendritic branches such as those of the apical and basal dendrites of pyramidal cells are trained to minimize the prediction error made by the dendritic branch when the target is the somatic activity. Whereas previous work has shown how to achieve fast negative phase inference (when the model is unclamped) in a predictive recurrent model, this contribution helps to achieve fast positive phase inference (when the target output is clamped) in such recurrent neural models.},
archivePrefix = {arXiv},
arxivId = {1606.01651},
author = {Bengio, Yoshua and Scellier, Benjamin and Bilaniuk, Olexa and Sacramento, Joao and Senn, Walter},
eprint = {1606.01651},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Bengio et al/2016/Feedforward Initialization for Fast Inference of Deep Generative Networks is biologically plausible.pdf:pdf},
title = {{Feedforward Initialization for Fast Inference of Deep Generative Networks is biologically plausible}},
url = {http://arxiv.org/abs/1606.01651},
year = {2016}
}
@inproceedings{Novikov2015,
abstract = {Deep neural networks currently demonstrate state-of-the-art performance in sev- eral domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train [17] format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks [21] we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.06569v1},
author = {Novikov, Alexander and Vetrov, Dmitry and Podoprikhin, Dimitry and Osokin, Anton},
booktitle = {Advances in Neural Information Processing Systems 28 (NIPS 2015)},
eprint = {arXiv:1509.06569v1},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Novikov et al/2015/Tensorizing Neural Networks.pdf:pdf},
title = {{Tensorizing Neural Networks}},
url = {http://arxiv.org/pdf/1509.06569v1.pdf},
year = {2015}
}
@inproceedings{Koutnik,
abstract = {In this paper we present a new self-organizing neural network, which builds a spatiotemporal model of an input temporal sequence inductively. The network is an extension of Kohonen's Self-organizing Map with a modified Hebb's rule for update of temporal synapses. The model building behavior is shown on inductive learning of a transition matrix from a data generated by a Markov Chain.},
author = {Koutnik, Jan},
booktitle = {Proceeding of International Workshop on Inductive Modelling (IWIM 2007)},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Koutnik/2007/Inductive Modelling of Temporal Sequences by Means of Self-organization.pdf:pdf},
keywords = {inductive modelling,self-organization,temporal sequences},
pages = {269--277},
title = {{Inductive Modelling of Temporal Sequences by Means of Self-organization}},
year = {2007}
}
@article{Varsta2001,
abstract = {This paper compares two Self-Organizing Map (SOM) based$\backslash$nmodels for temporal sequence processing (TSP) both$\backslash$nanalytically and experimentally. These models, Temporal$\backslash$nKohonen Map (TKM) and Recurrent Self-Organizing Map (RSOM),$\backslash$nincorporate leaky integrator memory to preserve the$\backslash$ntemporal context of the input signals. The learning and the$\backslash$nconvergence properties of the TKM and RSOM are studied and$\backslash$nwe show analytically that the RSOM is a significant$\backslash$nimprovement over the TKM, because the RSOM allows simple$\backslash$nderivation of a consistent learning rule. The results of$\backslash$nthe analysis are demonstrated with experiments.},
author = {Varsta, Markus and Heikkonen, Jukka and Lampinen, Jouko and Mill{\'{a}}n, Jos{\'{e}} Del R},
doi = {10.1023/A:1011353011837},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Varsta et al/2001/Temporal Kohonen map and the recurrent self-organizing map Analytical and experimental comparison.pdf:pdf},
issn = {13704621},
journal = {Neural Processing Letters},
keywords = {Convergence analysis,Self-organizing maps,Temporal sequence processing},
number = {3},
pages = {237--251},
pmid = {21173165},
title = {{Temporal Kohonen map and the recurrent self-organizing map: Analytical and experimental comparison}},
volume = {13},
year = {2001}
}
@article{Szegedy2016b,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
eprint = {1602.07261},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Szegedy et al/2016/Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.pdf:pdf},
journal = {arXiv:1602.07261},
month = {feb},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
url = {http://arxiv.org/abs/1602.07261},
year = {2016}
}
@article{Guo2014,
author = {Guo, Xiaoxiao and Lee, Honglak and Wang, Xiaoshi and Lewis, Richard L},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Guo et al/2014/Deep learning for real-time Atari game play using offline Monte Carlo tree search planning.pdf:pdf},
journal = {Nips},
pages = {1--9},
title = {{Deep learning for real-time Atari game play using offline Monte Carlo tree search planning}},
volume = {2600},
year = {2014}
}
@article{Voegtlin2002a,
abstract = {This paper explores the combination of self-organizing map (SOM) and feedback, in order to represent sequences of inputs. In general, neural networks with time-delayed feedback represent time implicitly, by combining current inputs and past activities. It has been difficult to apply this approach to SOM, because feedback generates instability during learning. We demonstrate a solution to this problem, based on a nonlinearity. The result is a generalization of SOM that learns to represent sequences recursively. We demonstrate that the resulting representations are adapted to the temporal statistics of the input series. ?? 2002 Elsevier Science Ltd. All rights reserved.},
author = {Voegtlin, Thomas},
doi = {10.1016/S0893-6080(02)00072-2},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Voegtlin/2002/Recursive self-organizing maps.pdf:pdf},
isbn = {3343791121},
issn = {08936080},
journal = {Neural Networks},
keywords = {Kohonen map,Recurrent networks,Recursive self-organizing maps,Recursiveness,Time},
pages = {979--991},
pmid = {12416688},
title = {{Recursive self-organizing maps}},
volume = {15},
year = {2002}
}
@article{Scellier2017,
abstract = {We introduce Equilibrium Propagation, a learning framework for energy-based models. It involves only one kind of neural computation, performed in both the first phase (when the prediction is made) and the second phase of training (after the target or prediction error is revealed). Although this algorithm computes the gradient of an objective function just like Backpropagation, it does not need a special computation or circuit for the second phase, where errors are implicitly propagated. Equilibrium Propagation shares similarities with Contrastive Hebbian Learning and Contrastive Divergence while solving the theoretical issues of both algorithms: our algorithm computes the gradient of a well defined objective function. Because the objective function is defined in terms of local perturbations, the second phase of Equilibrium Propagation corresponds to only nudging the prediction (fixed point, or stationary distribution) towards a configuration that reduces prediction error. In the case of a recurrent multi-layer supervised network, the output units are slightly nudged towards their target in the second phase, and the perturbation introduced at the output layer propagates backward in the hidden layers. We show that the signal 'back-propagated' during this second phase corresponds to the propagation of error derivatives and encodes the gradient of the objective function, when the synaptic update corresponds to a standard form of spike-timing dependent plasticity. This work makes it more plausible that a mechanism similar to Backpropagation could be implemented by brains, since leaky integrator neural computation performs both inference and error back-propagation in our model. The only local difference between the two phases is whether synaptic changes are allowed or not.},
author = {Scellier, Benjamin and Bengio, Yoshua},
doi = {10.3389/fncom.2017.00024},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Scellier, Bengio/2017/Equilibrium Propagation Bridging the Gap Between Energy-Based Models and Backpropagation.pdf:pdf},
issn = {1662-5188},
journal = {Frontiers in Computational Neuroscience},
keywords = {01 december 2016,artificial neural network,backpropagation algorit,backpropagation algorithm,biologically plausible learning rule,contrastive,deep learning,fixed point,hebbian learning,hopfield networks,received,senior fellow of cifar,spike-timing dependent plasticity},
pages = {1--13},
title = {{Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation}},
volume = {11},
year = {2017}
}
@article{Young2014,
abstract = {Deep machine learning offers a comprehensive framework for extracting meaningful features from complex observations in an unsupervised manner. The majority of deep learning architectures described in the literature primarily focus on extracting spatial features. However, in real-world settings, capturing temporal dependencies in observations is critical for accurate inference. This paper introduces an enhancement to DeSTIN - a compositional deep learning architecture in which each layer consists of multiple instantiations of a common node - that learns to represent spatiotemporal patterns in data based on a novel recurrent clustering algorithm. Contrary to mainstream deep architectures, such as deep belief networks where layer-by-layer training is assumed, each of the nodes in the proposed architecture is trained independently and in parallel. Moreover, top-down and bottom-up information flows facilitate rich feature formation. A semi-supervised setting is demonstrated achieving state-of-the-art results on the MNIST classification benchmarks. A GPU implementation is discussed further accentuating the scalability properties of the proposed framework. ?? 2013 Elsevier Ltd. All rights reserved.},
author = {Young, S. R. and Davis, A. and Mishtal, A. and Arel, I.},
doi = {10.1016/j.patrec.2013.07.013},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Young et al/2014/Hierarchical spatiotemporal feature extraction using recurrent online clustering.pdf:pdf},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Deep machine learning,Online clustering,Pattern recognition,Recurrent clustering,Spatiotemporal signals,Unsupervised feature extraction},
number = {1},
pages = {115--123},
publisher = {Elsevier B.V.},
title = {{Hierarchical spatiotemporal feature extraction using recurrent online clustering}},
url = {http://dx.doi.org/10.1016/j.patrec.2013.07.013},
volume = {37},
year = {2014}
}
@article{Goodfellow2015,
abstract = {www.deeplearningbook.org},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
doi = {10.1038/nmeth.3707},
eprint = {arXiv:1312.6184v5},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Goodfellow, Bengio, Courville/2015/Deep Learning.pdf:pdf},
isbn = {9780521835688},
issn = {1548-7091},
journal = {Nature Methods},
number = {1},
pages = {35--35},
pmid = {10463930},
title = {{Deep Learning}},
url = {http://www.nature.com/doifinder/10.1038/nature14539{\%}5Cnhttp://www.nature.com/doifinder/10.1038/nmeth.3707},
volume = {13},
year = {2015}
}
@article{VanHasselt2015,
abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether this harms performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
archivePrefix = {arXiv},
arxivId = {1509.06461},
author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
eprint = {1509.06461},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/van Hasselt, Guez, Silver/2015/Deep Reinforcement Learning with Double Q-learning.pdf:pdf},
journal = {arXiv:1509.06461 [cs]},
title = {{Deep Reinforcement Learning with Double Q-learning}},
url = {http://arxiv.org/abs/1509.06461{\%}5Cnhttp://www.arxiv.org/pdf/1509.06461.pdf},
year = {2015}
}
@article{Pinto2009a,
abstract = {This paper introduces feedback connections into a previously proposed model of the simple and complex neurons of the neocortex. The original model considers only feedforward connections between a SOM (Self-Organizing Map) and a RSOM (Recurrent SOM). A variant of the SOM-RSOM pair is proposed, called LoopSOM. The RSOM sends predictions to the SOM, providing more robust pattern classification/recognition and solving ambiguities.},
author = {Pinto, R and Engel, P},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Pinto, Engel/2009/LoopSOM A Robust SOM Variant Using Self-Organizing Temporal Feedback Connections.pdf:pdf},
journal = {Proceedings of the VIII ENIA - Brazilian Meeting on Artificial Intelligence},
title = {{LoopSOM: A Robust SOM Variant Using Self-Organizing Temporal Feedback Connections}},
year = {2009}
}
@article{Kohonen1982,
author = {Kohonen, Teuvo},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Kohonen/1982/Self-Organized Formation of Topologically Correct Feature Maps.pdf:pdf},
journal = {Biological Cybernetics},
number = {43},
pages = {59--69},
title = {{Self-Organized Formation of Topologically Correct Feature Maps}},
year = {1982}
}
@article{Grant2017,
author = {Grant, W. Shane and Tanner, James and Itti, Laurent},
doi = {10.1016/j.neunet.2017.01.007},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Grant, Tanner, Itti/2017/Biologically plausible learning in neural networks with modulatory feedback.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
pages = {32--48},
publisher = {Elsevier Ltd},
title = {{Biologically plausible learning in neural networks with modulatory feedback}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608017300072},
volume = {88},
year = {2017}
}
@article{Beck2007,
abstract = {From first principles, we derive a quadratic nonlinear, first-order dynamical system capable of performing exact Bayes-Markov inferences for a wide class of biologically plausible stimulus-dependent patterns of activity while simultaneously providing an online estimate of model performance. This is accomplished by constructing a dynamical system that has solutions proportional to the probability distribution over the stimulus space, but with a constant of proportionality adjusted to provide a local estimate of the probability of the recent observations of stimulus-dependent activity-given model parameters. Next, we transform this exact equation to generate nonlinear equations for the exact evolution of log likelihood and log-likelihood ratios and show that when the input has low amplitude, linear rate models for both the likelihood and the log-likelihood functions follow naturally from these equations. We use these four explicit representations of the probability distribution to argue that, in contrast to the arguments of previous work, the dynamical system for the exact evolution of the likelihood (as opposed to the log likelihood or log-likelihood ratios) not only can be mapped onto a biologically plausible network but is also more consistent with physiological observations.},
author = {Beck, Jeffrey M and Pouget, Alexandre},
doi = {10.1162/neco.2007.19.5.1344},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Beck, Pouget/2007/Exact inferences in a neural implementation of a hidden Markov model.pdf:pdf},
isbn = {0899-7667 (Print)},
issn = {0899-7667},
journal = {Neural computation},
number = {5},
pages = {1344--1361},
pmid = {17381269},
title = {{Exact inferences in a neural implementation of a hidden Markov model}},
volume = {19},
year = {2007}
}
