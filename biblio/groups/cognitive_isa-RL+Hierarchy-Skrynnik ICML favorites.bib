Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{James2018a,
author = {James, Steven and Rosman, Benjamin and Africa, South and Konidaris, George},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/James et al/James et al. - 2018 - Learning to Plan with Portable Symbols.pdf:pdf},
number = {July},
title = {{Learning to Plan with Portable Symbols}},
year = {2018}
}
@article{Pardo2017,
abstract = {In reinforcement learning, it is common to let an agent interact for a fixed amount of time with its environment before resetting it and repeating the process in a series of episodes. The task that the agent has to learn can either be to maximize its performance over (i) that fixed period, or (ii) an indefinite period where time limits are only used during training to diversify experience. In this paper, we provide a formal account for how time limits could effectively be handled in each of the two cases and explain why not doing so can cause state-aliasing and invalidation of experience replay, leading to suboptimal policies and training instability. In case (i), we argue that the terminations due to time limits are in fact part of the environment, and thus a notion of the remaining time should be included as part of the agent's input to avoid violation of the Markov property. In case (ii), the time limits are not part of the environment and are only used to facilitate learning. We argue that this insight should be incorporated by bootstrapping from the value of the state at the end of each partial episode. For both cases, we illustrate empirically the significance of our considerations in improving the performance and stability of existing reinforcement learning algorithms, showing state-of-the-art results on several control tasks.},
archivePrefix = {arXiv},
arxivId = {1712.00378},
author = {Pardo, Fabio and Tavakoli, Arash and Levdik, Vitaly and Kormushev, Petar},
eprint = {1712.00378},
file = {::},
issn = {1938-7228},
title = {{Time Limits in Reinforcement Learning}},
url = {http://arxiv.org/abs/1712.00378},
year = {2017}
}
@article{Dubey2018,
abstract = {What makes humans so good at solving seemingly complex video games? Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors on human performance. We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors. We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play. Videos and the game manipulations are available at https://rach0012.github.io/humanRL{\_}website/},
archivePrefix = {arXiv},
arxivId = {1802.10217},
author = {Dubey, Rachit and Agrawal, Pulkit and Pathak, Deepak and Griffiths, Thomas L. and Efros, Alexei A.},
doi = {arXiv:1802.10217v1},
eprint = {1802.10217},
file = {::},
title = {{Investigating Human Priors for Playing Video Games}},
url = {http://arxiv.org/abs/1802.10217},
year = {2018}
}
@article{Co-Reyes2018a,
abstract = {In this work, we take a representation learning perspective on hierarchical reinforcement learning, where the problem of learning lower layers in a hierarchy is transformed into the problem of learning trajectory-level generative models. We show that we can learn continuous latent representations of trajectories, which are effective in solving temporally extended and multi-stage problems. Our proposed model, SeCTAR, draws inspiration from variational autoencoders, and learns latent representations of trajectories. A key component of this method is to learn both a latent-conditioned policy and a latent-conditioned model which are consistent with each other. Given the same latent, the policy generates a trajectory which should match the trajectory predicted by the model. This model provides a built-in prediction mechanism, by predicting the outcome of closed loop policy behavior. We propose a novel algorithm for performing hierarchical RL with this model, combining model-based planning in the learned latent space with an unsupervised exploration objective. We show that our model is effective at reasoning over long horizons with sparse rewards for several simulated tasks, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration.},
archivePrefix = {arXiv},
arxivId = {1806.02813},
author = {Co-Reyes, John D. and Liu, YuXuan and Gupta, Abhishek and Eysenbach, Benjamin and Abbeel, Pieter and Levine, Sergey},
eprint = {1806.02813},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Co-Reyes et al/Co-Reyes et al. - 2018 - Self-Consistent Trajectory Autoencoder Hierarchical Reinforcement Learning with Trajectory Embeddings.pdf:pdf},
issn = {1938-7228},
title = {{Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings}},
url = {http://arxiv.org/abs/1806.02813},
year = {2018}
}
@article{Lee2018a,
abstract = {Value Iteration Networks (VINs) are effective differentiable path planning modules that can be used by agents to perform navigation while still maintaining end-to-end differentiability of the entire architecture. Despite their effectiveness, they suffer from several disadvantages including training instability, random seed sensitivity, and other optimization problems. In this work, we reframe VINs as recurrent-convolutional networks which demonstrates that VINs couple recurrent convolutions with an unconventional max-pooling activation. From this perspective, we argue that standard gated recurrent update equations could potentially alleviate the optimization issues plaguing VIN. The resulting architecture, which we call the Gated Path Planning Network, is shown to empirically outperform VIN on a variety of metrics such as learning speed, hyperparameter sensitivity, iteration count, and even generalization. Furthermore, we show that this performance gap is consistent across different maze transition types, maze sizes and even show success on a challenging 3D environment, where the planner is only provided with first-person RGB images.},
archivePrefix = {arXiv},
arxivId = {1806.06408},
author = {Lee, Lisa and Parisotto, Emilio and Chaplot, Devendra Singh and Xing, Eric and Salakhutdinov, Ruslan},
eprint = {1806.06408},
file = {::},
issn = {1938-7228},
title = {{Gated Path Planning Networks}},
url = {http://arxiv.org/abs/1806.06408},
year = {2018}
}
@article{Jaderberg2018,
abstract = {Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.},
archivePrefix = {arXiv},
arxivId = {1807.01281},
author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
doi = {arXiv:1807.01281},
eprint = {1807.01281},
file = {:C$\backslash$:/Users/panov/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaderberg et al. - 2018 - Human-level performance in first-person multiplayer games with population-based deep reinforcement learning(2).pdf:pdf},
title = {{Human-level performance in first-person multiplayer games with population-based deep reinforcement learning}},
url = {http://arxiv.org/abs/1807.01281},
year = {2018}
}
@article{Abel2018b,
abstract = {We consider the problem of how best to use prior experience to bootstrap lifelong learning, where an agent faces a series of task instances drawn from some task distribution. First, we identify the initial policy that optimizes expected performance over the distribution of tasks for increasingly complex classes of policy and task distributions. We empirically demonstrate the relative performance of each policy class' optimal element in a variety of simple task distributions. We then consider value-function initialization methods that preserve PAC guarantees while simultaneously minimizing the learning required in two learning algorithms, yielding MaxQInit, a practical new method for value-function-based transfer. We show that MaxQInit performs well in simple lifelong RL experiments.},
author = {Abel, David and Jinnai, Yuu and Guo, Sophie Yue and Konidaris, George and Littman, Michael},
file = {::},
journal = {Proceedings of the 35th International Conference on Machine Learning},
pages = {20--29},
title = {{Policy and Value Transfer in Lifelong Reinforcement Learning}},
url = {http://proceedings.mlr.press/v80/abel18b.html},
volume = {80},
year = {2018}
}
@article{Rabinowitz2018a,
abstract = {Theory of mind (ToM) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We design a Theory of Mind neural network {\{}–{\}} a ToMnet {\{}–{\}} which uses meta-learning to build such models of the agents it encounters. The ToMnet learns a strong prior model for agents' future behaviour, and, using only a small number of behavioural observations, can bootstrap to richer predictions about agents' characteristics and mental states. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep RL agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test of recognising that others can hold false beliefs about the world.},
archivePrefix = {arXiv},
arxivId = {1802.07740},
author = {Rabinowitz, Neil and Perbet, Frank and Song, Francis and Zhang, Chiyuan and Eslami, S M Ali and Botvinick, Matthew},
eprint = {1802.07740},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Rabinowitz et al/Rabinowitz et al. - 2018 - Machine Theory of Mind.pdf:pdf},
issn = {1938-7228},
journal = {Proceedings of the 35th International Conference on Machine Learning},
pages = {4215--4224},
title = {{Machine Theory of Mind}},
url = {http://proceedings.mlr.press/v80/rabinowitz18a.html},
volume = {80},
year = {2018}
}
@article{Verma2018a,
abstract = {We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural "oracle". We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.},
archivePrefix = {arXiv},
arxivId = {1804.02477},
author = {Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
eprint = {1804.02477},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Verma et al/Verma et al. - 2018 - Programmatically Interpretable Reinforcement Learning.pdf:pdf},
issn = {1938-7228},
title = {{Programmatically Interpretable Reinforcement Learning}},
url = {http://arxiv.org/abs/1804.02477},
year = {2018}
}
@article{Riedmiller2018,
abstract = {We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.},
archivePrefix = {arXiv},
arxivId = {1802.10567},
author = {Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and {Van de Wiele}, Tom and Mnih, Volodymyr and Heess, Nicolas and Springenberg, Jost Tobias},
eprint = {1802.10567},
file = {::},
issn = {1938-7228},
number = {table 1},
title = {{Learning by Playing - Solving Sparse Reward Tasks from Scratch}},
url = {http://arxiv.org/abs/1802.10567},
volume = {48},
year = {2018}
}
@article{Florensa2017a,
abstract = {Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.},
archivePrefix = {arXiv},
arxivId = {1705.06366},
author = {Florensa, Carlos and Held, David and Geng, Xinyang and Abbeel, Pieter},
doi = {arXiv:1705.06366v3},
eprint = {1705.06366},
file = {::},
isbn = {076453601X},
issn = {1938-7228},
title = {{Automatic Goal Generation for Reinforcement Learning Agents}},
url = {http://arxiv.org/abs/1705.06366},
year = {2017}
}
@article{Dimakopoulou2018a,
abstract = {We consider a team of reinforcement learning agents that concurrently operate in a common environment, and we develop an approach to efficient coordinated exploration that is suitable for problems of practical scale. Our approach builds on seed sampling (Dimakopoulou and Van Roy, 2018) and randomized value function learning (Osband et al., 2016). We demonstrate that, for simple tabular contexts, the approach is competitive with previously proposed tabular model learning methods (Dimakopoulou and Van Roy, 2018). With a higher-dimensional problem and a neural network value function representation, the approach learns quickly with far fewer agents than alternative exploration schemes.},
archivePrefix = {arXiv},
arxivId = {1805.08948},
author = {Dimakopoulou, Maria and Osband, Ian and {Van Roy}, Benjamin},
doi = {10.1.1.151.8250},
eprint = {1805.08948},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Dimakopoulou, Osband, Van Roy/Dimakopoulou, Osband, Van Roy - 2018 - Scalable Coordinated Exploration in Concurrent Reinforcement Learning.pdf:pdf},
isbn = {0262042088},
issn = {1049-5258},
title = {{Scalable Coordinated Exploration in Concurrent Reinforcement Learning}},
url = {http://arxiv.org/abs/1805.08948},
year = {2018}
}
@article{Zanette2018,
abstract = {In order to make good decision under uncertainty an agent must learn from observations. To do so, two of the most common frameworks are Contextual Bandits and Markov Decision Processes (MDPs). In this paper, we study whether there exist algorithms for the more general framework (MDP) which automatically provide the best performance bounds for the specific problem at hand without user intervention and without modifying the algorithm. In particular, it is found that a very minor variant of a recently proposed reinforcement learning algorithm for MDPs already matches the best possible regret bound˜O bound˜ bound˜O(√ SAT) in the dominant term if deployed on a tabular Contextual Bandit problem despite the agent being agnostic to such setting.},
author = {Zanette, Andrea},
file = {::},
issn = {1938-7228},
journal = {Icml},
title = {{Problem Dependent Reinforcement Learning Bounds Which Can Identify Bandit Structure in MDPs}},
url = {http://proceedings.mlr.press/v80/zanette18a/zanette18a.pdf},
year = {2018}
}
@article{Abel2018a,
abstract = {In lifelong reinforcement learning, agents must effectively transfer knowledge across tasks while simultaneously addressing exploration, credit assignment, and generalization. State abstraction can help overcome these hurdles by compressing the representation used by an agent, thereby reducing the computational and statistical burdens of learning. To this end, we here develop theory to compute and use state abstractions in lifelong reinforcement learning. We introduce two new classes of abstractions: (1) transitive state abstractions, whose optimal form can be computed efficiently, and (2) PAC state abstractions, which are guaranteed to hold with respect to a distribution of tasks. We show that the joint family of transitive PAC abstractions can be acquired efficiently, preserve near optimal-behavior, and experimentally reduce sample complexity in simple domains, thereby yielding a family of desirable abstractions for use in lifelong reinforcement learning. Along with these positive results, we show that there are pathological cases where state abstractions can negatively impact performance.},
author = {Abel, David and Arumugam, Dilip and Lehnert, Lucas and Littman, Michael},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 35th International Conference on Machine Learning/Abel et al/Abel et al. - 2018 - State Abstractions for Lifelong Reinforcement Learning.pdf:pdf},
journal = {Proceedings of the 35th International Conference on Machine Learning},
pages = {10--19},
title = {{State Abstractions for Lifelong Reinforcement Learning}},
url = {http://proceedings.mlr.press/v80/abel18a.html},
volume = {80},
year = {2018}
}
@article{Dietterich2018,
abstract = {Exogenous state variables and rewards can slow down reinforcement learning by injecting uncontrolled variation into the reward signal. We formalize exogenous state variables and rewards and identify conditions under which an MDP with exogenous state can be decomposed into an exogenous Markov Reward Process involving only the exogenous state+reward and an endogenous Markov Decision Process defined with respect to only the endogenous rewards. We also derive a variance-covariance condition under which Monte Carlo policy evaluation on the endogenous MDP is accelerated compared to using the full MDP. Similar speedups are likely to carry over to all RL algorithms. We develop two algorithms for discovering the exogenous variables and test them on several MDPs. Results show that the algorithms are practical and can significantly speed up reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1806.01584},
author = {Dietterich, Thomas G. and Trimponias, George and Chen, Zhitang},
eprint = {1806.01584},
file = {::},
issn = {1938-7228},
title = {{Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning}},
url = {http://arxiv.org/abs/1806.01584},
year = {2018}
}
@article{Tirinzoni2018,
abstract = {We consider the transfer of experience samples (i.e., tuples {\textless} s, a, s', r {\textgreater}) in reinforcement learning (RL), collected from a set of source tasks to improve the learning process in a given target task. Most of the related approaches focus on selecting the most relevant source samples for solving the target task, but then all the transferred samples are used without considering anymore the discrepancies between the task models. In this paper, we propose a model-based technique that automatically estimates the relevance (importance weight) of each source sample for solving the target task. In the proposed approach, all the samples are transferred and used by a batch RL algorithm to solve the target task, but their contribution to the learning process is proportional to their importance weight. By extending the results for importance weighting provided in supervised learning literature, we develop a finite-sample analysis of the proposed batch RL algorithm. Furthermore, we empirically compare the proposed algorithm to state-of-the-art approaches, showing that it achieves better learning performance and is very robust to negative transfer, even when some source tasks are significantly different from the target task.},
archivePrefix = {arXiv},
arxivId = {1805.10886},
author = {Tirinzoni, Andrea and Sessa, Andrea and Pirotta, Matteo and Restelli, Marcello},
eprint = {1805.10886},
file = {::},
title = {{Importance Weighted Transfer of Samples in Reinforcement Learning}},
url = {http://arxiv.org/abs/1805.10886},
year = {2018}
}
