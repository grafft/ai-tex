Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@book{MacKay2005,
abstract = {This book is aimed at senior undergraduates and graduate students in Engineering, Science, Mathematics, and Computing. It expects familiarity with calculus, probability theory, and linear algebra as taught in a rst- or secondyear undergraduate course on mathematics for scientists and engineers. Conventional courses on information theory cover not only the beautiful theoretical ideas of Shannon, but also practical solutions to communication problems. This book goes further, bringing in Bayesian data modelling, Monte Carlo methods, variational methods, clustering algorithms, and neural networks. Why unify information theory and machine learning? Because they are two sides of the same coin. In the 1960s, a single eld, cybernetics, was populated by information theorists, computer scientists, and neuroscientists, all studying common problems. Information theory and machine learning still belong together. Brains are the ultimate compression and communication systems. And the state-of-the-art algorithms for both data compression and error-correcting codes use the same tools as machine learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {MacKay, David J C},
doi = {10.1198/jasa.2005.s54},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/MacKay/MacKay - 2005 - Information Theory, Inference, and Learning Algorithms.pdf:pdf},
isbn = {9780521642989},
issn = {01621459},
pages = {640},
pmid = {13217055},
title = {{Information Theory, Inference, and Learning Algorithms}},
url = {http://pubs.amstat.org/doi/abs/10.1198/jasa.2005.s54{\%}5Cnhttp://www.cambridge.org/0521642981},
year = {2005}
}
@book{Bratko1977,
address = {Новосибирск},
author = {Братко, А.А. and Кочергин, А. Н.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Братко, Кочергин/Братко, Кочергин - 1977 - Информация и психика.pdf:pdf},
language = {russian},
pages = {183},
publisher = {Наука},
title = {{Информация и психика}},
year = {1977}
}
@unpublished{Fomin2011,
author = {Фомин, С. А. and Кузюрин, Н. Н.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Фомин, Кузюрин/Фомин, Кузюрин - 2011 - Эффективные алгоритмы и сложность вычислений.pdf:pdf},
language = {russian},
pages = {363},
title = {{Эффективные алгоритмы и сложность вычислений}},
year = {2011}
}
@article{Hoel2013,
abstract = {Causal interactions within complex systems can be analyzed at multiple spatial and temporal scales. For example, the brain can be analyzed at the level of neurons, neuronal groups, and areas, over tens, hundreds, or thousands of milliseconds. It is widely assumed that, once a micro level is fixed, macro levels are fixed too, a relation called supervenience. It is also assumed that, although macro descriptions may be convenient, only the micro level is causally complete, because it includes every detail, thus leaving no room for causation at themacro level. However, this assumption can only be evaluated under a proper measure of causation. Here, we use ameasure[effectiveinformation (EI)] that depends on both the effectiveness of a system's mechanisms and the size of its state space: EI is higher the more the mechanisms constrain the system's possible past and future states. Bymeasuring EI at micro andmacro levels in simple systems whosemicro mechanisms are fixed,we show that for certain causal architectures EI can peak at a macro level in space and/or time. This happens when coarse-grained macro mechanisms are more effective (more deterministic and/or less degenerate) than the underlying micro mechanisms, to an extent that overcomes the smaller state space. Thus, although the macro level supervenes upon themicro, it can supersede it causally, leading to genuine causal emergence—the gain in EI when moving from a micro to a macro level of analysis.},
author = {Hoel, E. P. and Albantakis, L. and Tononi, G.},
doi = {10.1073/pnas.1314922110},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the National Academy of Sciences/Hoel, Albantakis, Tononi/Hoel, Albantakis, Tononi - 2013 - Quantifying causal emergence shows that macro can beat micro.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {49},
pages = {19790--19795},
title = {{Quantifying causal emergence shows that macro can beat micro}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1314922110},
volume = {110},
year = {2013}
}
@article{Kolmogorov1965,
author = {Колмогоров, А. Н.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Проблемы передачи информации/Колмогоров/Колмогоров - 1965 - Три подхода к определению понятия ``количества информации''.pdf:pdf},
journal = {Проблемы передачи информации},
language = {russian},
number = {1},
pages = {3--11},
title = {{Три подхода к определению понятия ``количества информации''}},
volume = {1},
year = {1965}
}
@book{2008,
address = {М.},
author = {Ивин, А.},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Unknown/Ивин/Ивин - 2008 - Логика Учеб. пособие для студентов вузов.pdf:pdf},
language = {russian},
pages = {336},
publisher = {ООО "Издательство Оникс"},
title = {{Логика: Учеб. пособие для студентов вузов}},
year = {2008}
}
