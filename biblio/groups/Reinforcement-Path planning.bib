Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{YangLuShujuanYiYurongLiu2016,
author = {Lu, Yang and Yi, Shujuan and Liu, Yurong and Ji, Yuling},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Assembly Automation/Lu et al/Lu et al. - 2016 - A Novel Path Planning Method for Biomimetic Robot Based on Deep Learning.pdf:pdf},
journal = {Assembly Automation},
number = {2},
title = {{A Novel Path Planning Method for Biomimetic Robot Based on Deep Learning}},
volume = {36},
year = {2016}
}
@inproceedings{Sharma2017,
author = {Sharma, Avinash and Gupta, Kanika and Kumar, Anirudha and Sharma, Aishwarya and Kumar, Rajesh and Member, Senior},
booktitle = {2017 IEEE International Conference on Industrial Technology (ICIT)},
doi = {10.1109/ICIT.2017.7915468},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/2017 IEEE International Conference on Industrial Technology (ICIT)/Sharma et al/Sharma et al. - 2017 - Model based path planning using Q-Learning.pdf:pdf},
keywords = {Control System,Robotics and Automation},
pages = {837--842},
publisher = {IEEE},
title = {{Model based path planning using Q-Learning}},
year = {2017}
}
@incollection{Xia2015,
address = {Paris},
author = {Xia, Chen and {El Kamel}, A.},
booktitle = {Proceedings of the 21st International Conference on Industrial Engineering and Engineering Management 2014},
doi = {10.2991/978-94-6239-102-4_136},
editor = {Qi, Ershi and Shen, Jiang and Dou, Runliang},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Proceedings of the 21st International Conference on Industrial Engineering and Engineering Management 2014/Xia, El Kamel/Xia, El Kamel - 2015 - A Reinforcement Learning Method of Obstacle Avoidance for Indust.pdf:pdf},
isbn = {978-94-6239-101-7},
keywords = {automation,capacity building,f rxqghu,ngo,npo,resource mismanagement,syndrome,v},
pages = {671--675},
publisher = {Atlantis Press},
series = {Proceedings of the International Conference on Industrial Engineering and Engineering Management},
title = {{A Reinforcement Learning Method of Obstacle Avoidance for Industrial Mobile Vehicles in Unknown Environments Using Neural Network}},
url = {http://link.springer.com/10.2991/978-94-6239-102-4 http://link.springer.com/10.2991/978-94-6239-102-4{\_}136},
volume = {2014},
year = {2015}
}
@article{Qu2009,
author = {Qu, Hong and Yang, Simon X and Willms, Allan R and Yi, Zhang},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IEEE Transactions on Neural Networks/Qu et al/Qu et al. - 2009 - Real-time robot path planning based on a modified pulse-coupled neural network model.pdf:pdf},
isbn = {1045-9227},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
number = {11},
pages = {1724--1739},
title = {{Real-time robot path planning based on a modified pulse-coupled neural network model}},
volume = {20},
year = {2009}
}
@article{Liu2015,
abstract = {The K Shortest Paths (KSPs) problem with non-numerable applications has been researched widely, which aims to compute KSPs between two nodes in a non-decreasing order. However, less effort has been devoted to single-source KSP problem than to single-pair KSP computation, especially by using parallel methods. This paper proposes a Modified Continued Pulse Coupled Neural Network (MCPCNN) model to solve the two kinds of KSP problems. Theoretical analysis of MCPCNN and two algorithms for KSPs computation are presented. By using the parallel pulse transmission characteristic of pulse coupled neural networks, the method is able to find k shortest paths quickly. The computational complexity is only related to the length of the longest shortest path. Simulative results for route planning show that the proposed MCPCNN method for KSPs computation outperforms many other current efficient algorithms.},
author = {Liu, Guisong and Qiu, Zhao and Qu, Hong and Ji, Luping},
doi = {10.1016/j.neucom.2014.09.012},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Neurocomputing/Liu et al/Liu et al. - 2015 - Computing k shortest paths using modified pulse-coupled neural network.pdf:pdf},
isbn = {9780874216561},
issn = {18728286},
journal = {Neurocomputing},
keywords = {K Shortest paths,Pulse coupled neural network,Single-pair KSP,Single-source KSP},
number = {PC},
pages = {1162--1176},
publisher = {Elsevier},
title = {{Computing k shortest paths using modified pulse-coupled neural network}},
url = {http://dx.doi.org/10.1016/j.neucom.2014.09.012},
volume = {149},
year = {2015}
}
@article{Xia2016,
abstract = {Designing intelligent and robust autonomous navigation systems remains a great challenge in mobile robotics. Inverse reinforcement learning (IRL) offers an efficient learning technique from expert demonstrations to teach robots how to perform specific tasks without manually specifying the reward function. Most of existing IRL algorithms assume the expert policy to be optimal and deterministic, and are applied to experiments with relatively small-size state spaces. However, in autonomous navigation tasks, the state spaces are frequently large and demonstrations can hardly visit all the states. Meanwhile the expert policy may be non-optimal and stochastic. In this paper, we focus on IRL with large-scale and high-dimensional state spaces by introducing the neural network to generalize the expert's behaviors to unvisited regions of the state space and an explicit policy representation is easily expressed by neural network, even for the stochastic expert policy. An efficient and convenient algorithm, Neural Inverse Reinforcement Learning (NIRL), is proposed. Experimental results on simulated autonomous navigation tasks show that a mobile robot using our approach can successfully navigate to the target position without colliding with unpredicted obstacles, largely reduce the learning time, and has a good generalization performance on undemonstrated states. Hence prove the robot intelligence of autonomous navigation transplanted from limited demonstrations to completely unknown tasks.},
author = {Xia, Chen and {El Kamel}, Abdelkader},
doi = {10.1016/j.robot.2016.06.003},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Robotics and Autonomous Systems/Xia, El Kamel/Xia, El Kamel - 2016 - Neural inverse reinforcement learning in autonomous navigation.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Autonomous navigation,Dynamic environments,Inverse reinforcement learning,Learning from demonstration,Markov decision processes,Neural network},
pages = {1--14},
publisher = {Elsevier B.V.},
title = {{Neural inverse reinforcement learning in autonomous navigation}},
url = {http://dx.doi.org/10.1016/j.robot.2016.06.003},
volume = {84},
year = {2016}
}
@article{Paxton2017,
abstract = {We consider task and motion planning in complex dynamic environments for problems expressed in terms of a set of Linear Temporal Logic (LTL) constraints, and a reward function. We propose a methodology based on reinforcement learning that employs deep neural networks to learn low-level control policies as well as task-level option policies. A major challenge in this setting, both for neural network approaches and classical planning, is the need to explore future worlds of a complex and interactive environment. To this end, we integrate Monte Carlo Tree Search with hierarchical neural net control policies trained on expressive LTL specifications. This paper investigates the ability of neural networks to learn both LTL constraints and control policies in order to generate task plans in complex environments. We demonstrate our approach in a simulated autonomous driving setting, where a vehicle must drive down a road in traffic, avoid collisions, and navigate an intersection, all while obeying given rules of the road.},
archivePrefix = {arXiv},
arxivId = {1703.07887},
author = {Paxton, Chris and Raman, Vasumathi and Hager, Gregory D. and Kobilarov, Marin},
eprint = {1703.07887},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/ArXiv 1703.07887/Paxton et al/Paxton et al. - 2017 - Combining Neural Networks and Tree Search for Task and Motion Planning in Challenging Environments.pdf:pdf},
journal = {ArXiv: 1703.07887},
title = {{Combining Neural Networks and Tree Search for Task and Motion Planning in Challenging Environments}},
url = {http://arxiv.org/abs/1703.07887},
year = {2017}
}
@article{Li2013,
abstract = {Pulse Coupled Neural Network (PCNN) is suitable for dealing with the classical shortest path problem, because of its autowave characteristic. However, most methods suggest that the autowave of PCNN models should keep a constant speed in finding the shortest paths. This paper proposes a novel self-adaptive autowave pulse-coupled neural network (SAPCNN) model for the shortest path problem. The autowave generated by SAPCNN propagates adaptively according to the current network state, which guarantees it spreads more effectively in finding the shortest paths. Our experiments, which have been carried out for both the shortest paths problem and K shortest paths problem, show that our proposed algorithm outperforms classical algorithms. ?? 2013 Elsevier B.V.},
author = {Li, Xiaojun and Ma, Yide and Feng, Xiaowen},
doi = {10.1016/j.neucom.2012.12.030},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/Neurocomputing/Li, Ma, Feng/Li, Ma, Feng - 2013 - Self-adaptive autowave pulse-coupled neural network for shortest-path problem.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Autowave,Pulse-coupled neural network,Self-adaptive autowave PCNN,Shortest path},
pages = {63--71},
publisher = {Elsevier},
title = {{Self-adaptive autowave pulse-coupled neural network for shortest-path problem}},
url = {http://dx.doi.org/10.1016/j.neucom.2012.12.030},
volume = {115},
year = {2013}
}
@article{Yang2004,
author = {Yang, S.X. and Luo, Chaomin},
doi = {10.1109/TSMCB.2003.811769},
file = {:C$\backslash$:/Users/panov/Documents/Mendeley Desktop/IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics)/Yang, Luo/Yang, Luo - 2004 - A Neural Network Approach to Complete Coverage Path Planning.pdf:pdf},
issn = {1083-4419},
journal = {IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics)},
month = {feb},
number = {1},
pages = {718--724},
title = {{A Neural Network Approach to Complete Coverage Path Planning}},
url = {http://ieeexplore.ieee.org/document/1262545/},
volume = {34},
year = {2004}
}
